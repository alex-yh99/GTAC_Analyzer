{"google_test_blog_": {"posts": ["By Siddartha Janga on behalf of Google iOS Developers\u00a0\n\nBrewing for quite some time, we are excited to announce EarlGrey, a functional UI testing framework for iOS. Several Google apps like YouTube, Google Calendar, Google Photos, Google Translate, Google Play Music and many more have successfully adopted the framework for their functional testing needs.\n\nThe key features offered by EarlGrey include:\n\n\nPowerful built-in synchronization : Tests will automatically wait for events such as animations, network requests, etc. before interacting with the UI. This will result in tests that are easier to write (no sleeps or waits) and simple to maintain (straight up procedural description of test steps).\u00a0\nVisibility checking : All interactions occur on elements that users can see. For example, attempting to tap a button that is behind an image will lead to test failure immediately.\u00a0\nFlexible design : The components that determine element selection, interaction, assertion and synchronization have been designed to be extensible.\u00a0\n\n\n\nAre you in need for a cup of refreshing EarlGrey? EarlGrey has been open sourced under the Apache license. Check out the getting started guide and add EarlGrey to your project using CocoaPods or manually add it to your Xcode project file.", "by Michael Klepikov and Lesley Katzen on behalf of the GTAC Committee\n\nThe ninth GTAC (Google Test Automation Conference) was held on November 10-11 at the Google Cambridge office, the \u201cHub\u201d of innovation. The conference was completely packed with presenters and attendees from all over the world, from industry and academia, discussing advances in test automation and the test engineering computer science field, bringing with them a huge diversity of experiences. Speakers from numerous companies and universities (Applitools, Automattic, Bitbar, Georgia Tech, Google, Indian Institute of Science, Intel, LinkedIn, Lockheed Martin, MIT, Nest, Netflix, OptoFidelity, Splunk, Supersonic, Twitter, Uber, University of Waterloo) spoke on a variety of interesting and cutting edge test automation topics.\n\n\n\n\n\n\n\nAll presentation videos and slides are posted on the Video Recordings and Presentations pages. All videos have professionally transcribed closed captions, and the YouTube descriptions have the slides links. Enjoy and share!\n\n\nWe had over 1,300 applicants and over 200 of those for speaking. Over 250 people filled our venue to capacity, and the live stream had a peak of about 400 concurrent viewers, with about 3,300 total viewing hours.\n\n\nOur goal in hosting GTAC is to make the conference highly relevant and useful for both attendees and the larger test engineering community as a whole. Our post-conference survey shows that we are close to achieving that goal; thanks to everyone who completed the feedback survey!\n\n\nOur 82 survey respondents were mostly (81%) test focused professionals with a wide range of 1 to 40 years of experience.\u00a0\nAnother 76% of respondents rated the conference as a whole as above average, with marked satisfaction for the venue, the food (those Diwali treats!), and the breadth and coverage of the talks themselves.\n\n\n\n\n\nThe top five most popular talks were:\n\n\nThe Uber Challenge of Cross-Application/Cross-Device Testing (Apple Chow and Bian Jiang)\u00a0\nYour Tests Aren't Flaky (Alister Scott)\u00a0\nStatistical Data Sampling (Celal Ziftci and Ben Greenberg)\u00a0\nCoverage is Not Strongly Correlated with Test Suite Effectiveness (Laura Inozemtseva)\u00a0\nChrome OS Test Automation Lab (Simran Basi and Chris Sosa).\n\n\n\n\n\nOur social events also proved  to be crowd pleasers.  The social events were a direct response to feedback from GTAC 2014 for organized opportunities for socialization among the GTAC attendees.\n\n\n\n\n\nThis isn\u2019t to say there isn\u2019t room for improvement. We had 11% of respondents express frustration with event communications and provided some long, thoughtful suggestions for what we could do to improve next year. Also, many of the long form comments asked for a better mix of technologies, noting that mobile had a big presence in the talks this year. \n\n\nIf you have any suggestions on how we can improve, please comment on this post, or better yet \u2013 fill out the survey, which remains open.  Based on feedback from last year urging more transparency in speaker selection, we included an individual outside of Google in the speaker evaluation.  Feedback is precious, we take it very seriously, and we will use it to improve next time around.\n\n\nThank you to all the speakers, attendees, and online viewers who made this a special event once again. To receive announcements about the next GTAC, currently planned for early 2017, subscribe to the Google Testing Blog.", "by Anthony Vallone on behalf of the GTAC Committee\n\nThe ninth GTAC (Google Test Automation Conference) commences on Tuesday, November 10th, at the Google Cambridge office. You can find the latest details on the conference site, including schedule, speaker profiles, and travel tips.\n\nIf you have not been invited to attend in person, you can watch the event live. And if you miss the livestream, we will post slides and videos later.\n\nWe have an outstanding speaker lineup this year, and we look forward to seeing you all there or online!", "by Anthony Vallone on behalf of the GTAC Committee\u00a0\n\nWe have completed the selection and confirmation of all speakers and attendees for GTAC 2015. You can find the detailed agenda at: developers.google.com/gtac/2015/schedule.\n\nThank you to all who submitted proposals!\n\nThere is a lot of interest in GTAC once again this year with about 1400 applicants and about 200 of those for speaking. Unfortunately, our venue only seats 250. We will livestream the event as usual, so fret not if you were not selected to attend. Information about the livestream and other details will be posted on the GTAC site soon and announced here.", "By: Patrik H\u00f6glund\n\n\n\nWhat is Automatic Gain Control?\u00a0\nIt\u2019s time to talk about advanced media quality tests again! As experienced Google testing blog readers know, when I write an article it\u2019s usually about WebRTC, and the unusual testing solutions we build to test it. This article is no exception. Today we\u2019re going to talk about Automatic Gain Control, or AGC. This is a feature that\u2019s on by default for WebRTC applications, such as http://apprtc.appspot.com. It uses various means to adjust the microphone signal so your voice makes it loud and clear to the other side of the peer connection. For instance, it can attempt to adjust your microphone gain or try to amplify the signal digitally.\n\n\n\n\nFigure 1. How Auto Gain Control works [code here].\n\n\n\nThis is an example of automatic control engineering (another example would be the classic PID controller) and happens in real time. Therefore, if you move closer to the mic while speaking, the AGC will notice the output stream is too loud, and reduce mic volume and/or digital gain. When you move further away, it tries to adapt up again. The fancy voice activity detector is there so we only amplify speech, and not, say, the microwave oven your spouse just started in the other room.\n\nTesting the AGC\nNow, how do we make sure the AGC works? The first thing is obviously to write unit tests and integration tests. You didn\u2019t think about building that end-to-end test first, did you? Once we have the lower-level tests in place, we can start looking at a bigger test. While developing the WebRTC implementation in Chrome, we had several bugs where the AGC code was working by itself, but was misconfigured in Chrome. In one case, it was simply turned off for all users. In another, it was only turned off in Hangouts. \n\n\nOnly an end-to-end test can catch these integration issues, and we already had stable, low-maintenance audio quality tests with the ability to record Chrome\u2019s output sound for analysis. I encourage you to read that article, but the bottom line is that those tests can run a WebRTC call in two tabs and record the audio output to a file. Those tests run the PESQ algorithm on input and output to see how similar they are.\n\n\nThat\u2019s a good framework to have, but I needed to make two changes:\n\n\nAdd file support to Chrome\u2019s fake audio input device, so we can play a known file. The original audio test avoided this by using WebAudio, but AGC doesn\u2019t run in the WebAudio path, just the microphone capture path, so that won\u2019t work.\nInstead of running PESQ, run an analysis that compares the gain between input and output.\n\n\nAdding Fake File Support\nThis is always a big part of the work in media testing: controlling the input and output. It\u2019s unworkable to tape microphones to loudspeakers or point cameras to screens to capture the media, so the easiest solution is usually to add a debug flag. It is exactly what I did here. It was a lot of work, but I won\u2019t go into much detail since Chrome\u2019s audio pipeline is complex. The core is this:\n\n\nint FileSource::OnMoreData(AudioBus* audio_bus, uint32 total_bytes_delay) {\n  // Load the file if we haven't already. This load needs to happen on the\n  // audio thread, otherwise we'll run on the UI thread on Mac for instance.\n  // This will massively delay the first OnMoreData, but we'll catch up.\n  if (!wav_audio_handler_)\n    LoadWavFile(path_to_wav_file_);\n  if (load_failed_)\n    return 0;\n\n  DCHECK(wav_audio_handler_.get());\n\n  // Stop playing if we've played out the whole file.\n  if (wav_audio_handler_->AtEnd(wav_file_read_pos_))\n    return 0;\n\n  // This pulls data from ProvideInput.\n  file_audio_converter_->Convert(audio_bus);\n  return audio_bus->frames();\n}\n\n\nThis code runs every 10 ms and reads a small chunk from the file, converts it to Chrome\u2019s preferred audio format and sends it on through the audio pipeline. After implementing this, I could simply run:\n\n\nchrome --use-fake-device-for-media-stream \\\n       --use-file-for-fake-audio-capture=/tmp/file.wav\n\n\nand whenever I hit a webpage that used WebRTC, the above file would play instead of my microphone input. Sweet!\n\nThe Analysis Stage\nNext I had to get the analysis stage figured out. It turned out there was something called an AudioPowerMonitor in the Chrome code, which you feed audio data into and get the average audio power for the data you fed in. This is a measure of how \u201cloud\u201d the audio is. Since the whole point of the AGC is getting to the right audio power level, we\u2019re looking to compute\n\nAdiff = Aout - Ain\n\n\nOr, really, how much louder or weaker is the output compared to the input audio? Then we can construct different scenarios: Adiff should be 0 if the AGC is turned off and it should be > 0 dB if the AGC is on and we feed in a low power audio file. Computing the average energy of an audio file was straightforward to implement:\n\n\n  // ...\n  size_t bytes_written;\n  wav_audio_handler->CopyTo(audio_bus.get(), 0, &bytes_written);\n  CHECK_EQ(bytes_written, wav_audio_handler->data().size())\n      << \"Expected to write entire file into bus.\";\n\n  // Set the filter coefficient to the whole file's duration; this will make\n  // the power monitor take the entire file into account.\n  media::AudioPowerMonitor power_monitor(wav_audio_handler->sample_rate(),\n                                         file_duration);\n  power_monitor.Scan(*audio_bus, audio_bus->frames());\n  // ...\n  return power_monitor.ReadCurrentPowerAndClip().first;\n\nI wrote a new test, and hooked up the above logic instead of PESQ. I could compute\u00a0Ain\u00a0by running the above algorithm on the reference file (which I fed in using the flag I implemented above) and Aout on the recording of the output audio. At this point I pretty much thought I was done. I ran a WebRTC call with the AGC turned off, expecting to get zero\u2026 and got a huge number. Turns out I wasn\u2019t done.\n\nWhat Went Wrong?\nI needed more debugging information to figure out what went wrong. Since the AGC was off, I would expect the power curves for output and input to be identical. All I had was the average audio power over the entire file, so I started plotting the audio power for each 10 millisecond segment instead to understand where the curves diverged. I could then plot the detected audio power over the time of the test. I started by plotting Adiff\u00a0:\n\n\n\n\n\nFigure 2. Plot of Adiff.\n\nThe difference is quite small in the beginning, but grows in amplitude over time. Interesting. I then plotted Aout and Ain next to each other:\n\n\n\n\n\n\nFigure 3. Plot of\u00a0Aout\u00a0and\u00a0Ain.\n\nA-ha! The curves drift apart over time; the above shows about 10 seconds of time, and the drift is maybe 80 ms at the end. The more they drift apart, the bigger the diff becomes. Exasperated, I asked our audio engineers about the above. Had my fancy test found its first bug? No, as it turns out - it was by design.\n\nClock Drift and Packet Loss\nLet me explain. As a part of WebRTC audio processing, we run a complex module called NetEq on the received audio stream. When sending audio over the Internet, there will inevitably be packet loss and clock drift. Packet losses always happen on the Internet, depending on the network path between sender and receiver. Clock drift happens because the sample clocks on the sending and receiving sound cards are not perfectly synced.\n\n\nIn this particular case, the problem was not packet loss since we have ideal network conditions (one machine, packets go over the machine\u2019s loopback interface = zero packet loss). But how can we have clock drift? Well, recall the fake device I wrote earlier that reads a file? It never touches the sound card like when the sound comes from the mic, so it runs on the system clock. That clock will drift against the machine\u2019s sound card clock, even when we are on the same machine. \n\n\nNetEq uses clever algorithms to conceal clock drift and packet loss. Most commonly it applies time compression or stretching on the audio it plays out, which means it makes the audio a little shorter or longer when needed to compensate for the drift. We humans mostly don\u2019t even notice that, whereas a drift left uncompensated would result in a depleted or flooded receiver buffer \u2013 very noticeable. Anyway, I digress. This drift of the recording vs. the reference file was natural and I would just have to deal with it.\n\nSilence Splitting to the Rescue!\nI could probably have solved this with math and postprocessing of the results (least squares  maybe?), but I had another idea. The reference file happened to be comprised of five segments with small pauses between them. What if I made these pauses longer, split the files on the pauses and trimmed away all the silence? This would effectively align the start of each segment with its corresponding segment in the reference file.\n\n\n\n\n\n\nFigure 4. Before silence splitting.\n\n\n\n\nFigure 5. After silence splitting.\n\nWe would still have NetEQ drift, but as you can see its effects will not stack up towards the end, so if the segments are short enough we should be able to mitigate this problem.\n\nResult\nHere is the final test implementation:\n\n\n\n  base::FilePath reference_file = \n      test::GetReferenceFilesDir().Append(reference_filename);\n  base::FilePath recording = CreateTemporaryWaveFile();\n\n  ASSERT_NO_FATAL_FAILURE(SetupAndRecordAudioCall(\n      reference_file, recording, constraints,\n      base::TimeDelta::FromSeconds(30)));\n\n  base::ScopedTempDir split_ref_files;\n  ASSERT_TRUE(split_ref_files.CreateUniqueTempDir());\n  ASSERT_NO_FATAL_FAILURE(\n      SplitFileOnSilenceIntoDir(reference_file, split_ref_files.path()));\n  std::vector<base::FilePath> ref_segments =\n      ListWavFilesInDir(split_ref_files.path());\n\n  base::ScopedTempDir split_actual_files;\n  ASSERT_TRUE(split_actual_files.CreateUniqueTempDir());\n  ASSERT_NO_FATAL_FAILURE(\n      SplitFileOnSilenceIntoDir(recording, split_actual_files.path()));\n\n  // Keep the recording and split files if the analysis fails.\n  base::FilePath actual_files_dir = split_actual_files.Take();\n  std::vector<base::FilePath> actual_segments =\n      ListWavFilesInDir(actual_files_dir);\n\n  AnalyzeSegmentsAndPrintResult(\n      ref_segments, actual_segments, reference_file, perf_modifier);\n\n  DeleteFileUnlessTestFailed(recording, false);\n  DeleteFileUnlessTestFailed(actual_files_dir, true);\n\nWhere AnalyzeSegmentsAndPrintResult looks like this:\n\n\n\nvoid AnalyzeSegmentsAndPrintResult(\n    const std::vector<base::FilePath>& ref_segments,\n    const std::vector<base::FilePath>& actual_segments,\n    const base::FilePath& reference_file,\n    const std::string& perf_modifier) {\n  ASSERT_GT(ref_segments.size(), 0u)\n      << \"Failed to split reference file on silence; sox is likely broken.\";\n  ASSERT_EQ(ref_segments.size(), actual_segments.size())\n      << \"The recording did not result in the same number of audio segments \"\n      << \"after on splitting on silence; WebRTC must have deformed the audio \"\n      << \"too much.\";\n\n  for (size_t i = 0; i < ref_segments.size(); i++) {\n    float difference_in_decibel = AnalyzeOneSegment(ref_segments[i],\n                                                    actual_segments[i],\n                                                    i);\n    std::string trace_name = MakeTraceName(reference_file, i);\n    perf_test::PrintResult(\"agc_energy_diff\", perf_modifier, trace_name,\n                           difference_in_decibel, \"dB\", false);\n  }\n}\n\n\nThe results look like this:\n\n\n\n\n\n\nFigure 6. Average Adiff values for each segment on the y axis, Chromium revisions on the x axis.\n\nWe can clearly see the AGC applies about 6 dB of gain to the (relatively low-energy) audio file we feed in. The maximum amount of gain the digital AGC can apply is 12 dB, and 7 dB is the default, so in this case the AGC is pretty happy with the level of the input audio. If we run with the AGC turned off, we get the expected 0 dB of gain. The diff varies a bit per segment, since the segments are different in audio power.\n\n\nUsing this test, we can detect if the AGC accidentally gets turned off or malfunctions on windows, mac or linux. If that happens, the with_agc graph will drop from ~6 db to 0, and we\u2019ll know something is up. Same thing if the amount of digital gain changes.\n\n\nA more advanced version of this test would also look at the mic level the AGC sets. This mic level is currently ignored in the test, but it could take it into account by artificially amplifying the reference file when played through the fake device. We could also try throwing curveballs at the AGC, like abruptly raising the volume mid-test (as if the user leaned closer to the mic), and look at the gain for the segments to ensure it adapted correctly.", "Posted by Anthony Vallone on behalf of the GTAC Committee \n\n\n\n\n\nThe deadline to apply for GTAC 2015 is this Monday, August 10th, 2015. There is a great deal of interest to both attend and speak, and we\u2019ve received many outstanding proposals. However, it\u2019s not too late to submit your proposal for consideration. If you would like to speak or attend, be sure to complete the form by Monday. \n\n\nWe will be making regular updates to the GTAC site (developers.google.com/gtac/2015/) over the next several weeks, and you can find conference details there.\n\n\nFor those that have already signed up to attend or speak, we will contact you directly by mid-September.", "Posted by Anthony Vallone on behalf of the GTAC Committee\n\n\nThe GTAC (Google Test Automation Conference) 2015 application process is now open for presentation proposals and attendance. GTAC will be held at the Google Cambridge office (near Boston, Massachusetts, USA) on November 10th - 11th, 2015.\n\n\nGTAC will be streamed live on YouTube again this year, so even if you can\u2019t attend in person, you\u2019ll be able to watch the conference remotely. We will post the live stream information as we get closer to the event, and recordings will be posted afterward.\n\n\nSpeakers\nPresentations are targeted at student, academic, and experienced engineers working on test automation. Full presentations are 30 minutes and lightning talks are 10 minutes. Speakers should be prepared for a question and answer session following their presentation.\n\n\nApplication\nFor presentation proposals and/or attendance, complete this form. We will be selecting about 25 talks and 200 attendees for the event. The selection process is not first come first serve (no need to rush your application), and we select a diverse group of engineers from various locations, company sizes, and technical backgrounds (academic, industry expert, junior engineer, etc).\n\n\nDeadline\nThe due date for both presentation and attendance applications is August 10th, 2015.\n\n\nFees\nThere are no registration fees, but speakers and attendees must arrange and pay for their own travel and accommodations.\n\n\nMore information\nYou can find more details at developers.google.com/gtac.", "Posted by Anthony Vallone on behalf of the GTAC Committee\n\n\n\n\n\nWe are pleased to announce that the ninth GTAC (Google Test Automation Conference) will be held in Cambridge (Greatah Boston, USA) on November 10th and 11th (Toozdee and Wenzdee), 2015. So, tell everyone to save the date for this wicked good event.\n\n\nGTAC is an annual conference hosted by Google, bringing together engineers from industry and academia to discuss advances in test automation and the test engineering computer science field. It\u2019s a great opportunity to present, learn, and challenge modern testing technologies and strategies.\n\n\nYou can browse presentation abstracts, slides, and videos from previous years on the GTAC site. \n\n\nStay tuned to this blog and the GTAC website for application information and opportunities to present at GTAC. Subscribing to this blog is the best way to get notified. We're looking forward to seeing you there!", "Author: Patrik H\u00f6glund\n\n\nAs we all know, software development is a complicated activity where we develop features and applications to provide value to our users. Furthermore, any nontrivial modern software is composed out of other software. For instance, the Chrome web browser pulls roughly a hundred libraries into its third_party folder when you build the browser. The most significant of these libraries is Blink, the rendering engine, but there\u2019s also ffmpeg for image processing, skia for low-level 2D graphics, and WebRTC for real-time communication (to name a few).\n\n\n\n\n\nFigure 1. Holy dependencies, Batman!\n\nThere are many reasons to use software libraries. Why write your own phone number parser when you can use libphonenumber, which is battle-tested by real use in Android and Chrome and available under a permissive license? Using such software frees you up to focus on the core of your software so you can deliver a unique experience to your users. On the other hand, you need to keep your application up to date with changes in the library (you want that latest bug fix, right?), and you also run a risk of such a change breaking your application. This article will examine that integration problem and how you can reduce the risks associated with it.\n\nUpdating Dependencies is Hard\nThe simplest solution is to check in a copy of the library, build with it, and avoid touching it as much as possible. This solution, however, can be problematic because you miss out on bug fixes and new features in the library. What if you need a new feature or bug fix that just made it in? You have a few options:\n\nUpdate the library to its latest release. If it\u2019s been a long time since you did this, it can be quite risky and you may have to spend significant testing resources to ensure all the accumulated changes don\u2019t break your application. You may have to catch up to interface changes in the library as well.\u00a0\nCherry-pick the feature/bug fix you want into your copy of the library. This is even riskier because your cherry-picked patches may depend on other changes in the library in subtle ways. Also, you still are not up to date with the latest version.\u00a0\nFind some way to make do without the feature or bug fix.\n\nNone of the above options are very good. Using this ad-hoc updating model can work if there\u2019s a low volume of changes in the library and our requirements on the library don\u2019t change very often. Even if that is the case, what will you do if a critical zero-day exploit is discovered in your socket library? \n\n\nOne way to mitigate the update risk is to integrate more often with your dependencies. As an extreme example, let\u2019s look at Chrome.\n\n\nIn Chrome development, there\u2019s a massive amount of change going into its dependencies. The Blink rendering engine lives in a separate code repository from the browser. Blink sees hundreds of code changes per day, and Chrome must integrate with Blink often since it\u2019s an important part of the browser. Another example is the WebRTC implementation, where a large part of Chrome\u2019s implementation resides in the webrtc.org repository. This article will focus on the latter because it\u2019s the team I happen to work on.\n\nHow \u201cRolling\u201d Works\u00a0\nThe open-sourced WebRTC codebase is used by Chrome but also by a number of other companies working on WebRTC. Chrome uses a toolchain called depot_tools to manage dependencies, and there\u2019s a checked-in text file called DEPS where dependencies are managed. It looks roughly like this:\n{\n  # ... \n  'src/third_party/webrtc':\n      'https://chromium.googlesource.com/' +\n      'external/webrtc/trunk/webrtc.git' + \n      '@' + '5727038f572c517204e1642b8bc69b25381c4e9f',\n}\n\n\nThe above means we should pull WebRTC from the specified git repository at the 572703... hash, similar to other dependency-provisioning frameworks. To build Chrome with a new version, we change the hash and check in a new version of the DEPS file. If the library\u2019s API has changed, we must update Chrome to use the new API in the same patch. This process is known as rolling WebRTC to a new version.\n\n\nNow the problem is that we have changed the code going into Chrome. Maybe getUserMedia has started crashing on Android, or maybe the browser no longer boots on Windows. We don\u2019t know until we have built and run all the tests. Therefore a roll patch is subject to the same presubmit checks as any Chrome patch (i.e. many tests, on all platforms we ship on). However, roll patches can be considerably more painful and risky than other patches.\n\n\n\n\n\n\nFigure 2. Life of a Roll Patch.\n\nOn the WebRTC team we found ourselves in an uncomfortable position a couple years back. Developers would make changes to the webrtc.org code and there was a fair amount of churn in the interface, which meant we would have to update Chrome to adapt to those changes. Also we frequently broke tests and WebRTC functionality in Chrome because semantic changes had unexpected consequences in Chrome. Since rolls were so risky and painful to make, they started to happen less often, which made things even worse. There could be two weeks between rolls, which meant Chrome was hit by a large number of changes in one patch.\n\nBots That Can See the Future: \u201cFYI Bots\u201d\u00a0\nWe found a way to mitigate this which we called FYI (for your information) bots. A bot is Chrome lingo for a continuous build machine which builds Chrome and runs tests.\n\n\nAll the existing Chrome bots at that point would build Chrome as specified in the DEPS file, which meant they would build the WebRTC version we had rolled to up to that point. FYI bots replace that pinned version with WebRTC HEAD, but otherwise build and run Chrome-level tests as usual. Therefore: \n\n\n\nIf all the FYI bots were green, we knew a roll most likely would go smoothly.\u00a0\nIf the bots didn\u2019t compile, we knew we would have to adapt Chrome to an interface change in the next roll patch.\u00a0\nIf the bots were red, we knew we either had a bug in WebRTC or that Chrome would have to be adapted to some semantic change in WebRTC.\n\nThe FYI \u201cwaterfall\u201d (a set of bots that builds and runs tests) is a straight copy of the main waterfall, which is expensive in resources. We could have cheated and just set up FYI bots for one platform (say, Linux), but the most expensive regressions are platform-specific, so we reckoned the extra machines and maintenance were worth it.\n\nMaking Gradual Interface Changes\u00a0\nThis solution helped but wasn\u2019t quite satisfactory. We initially had the policy that it was fine to break the FYI bots since we could not update Chrome to use a new interface until the new interface had actually been rolled into Chrome. This, however, often caused the FYI bots to be compile-broken for days. We quickly started to suffer from red blindness [1] and had no idea if we would break tests on the roll, especially if an interface change was made early in the roll cycle.\n\n\nThe solution was to move to a more careful update policy for the WebRTC API. For the more technically inclined, \u201ccareful\u201d here means \u201cfollowing the API prime directive\u201d [2]. Consider this example:\nclass WebRtcAmplifier {\n  ...\n  int SetOutputVolume(float volume);\n}\n\nNormally we would just change the method\u2019s signature when we needed to:\nclass WebRtcAmplifier {\n  ...\n  int SetOutputVolume(float volume, bool allow_eleven1);\n}\n\n\u2026 but this would compile-break Chome until it could be updated. So we started doing it like this instead:\nclass WebRtcAmplifier {\n  ...\n  int SetOutputVolume(float volume);\n  int SetOutputVolume2(float volume, bool allow_eleven);\n}\n\nThen we could:\n\nRoll into Chrome\u00a0\nMake Chrome use SetOutputVolume2\u00a0\nUpdate SetOutputVolume\u2019s signature\u00a0\nRoll again and make Chrome use SetOutputVolume\u00a0\nDelete SetOutputVolume2\n\nThis approach requires several steps but we end up with the right interface and at no point do we break Chrome.\n\nResults\nWhen we implemented the above, we could fix problems as they came up rather than in big batches on each roll. We could institute the policy that the FYI bots should always be green, and that changes breaking them should be immediately rolled back. This made a huge difference. The team could work smoother and roll more often. This reduced our risk quite a bit, particularly when Chrome was about to cut a new version branch. Instead of doing panicked and risky rolls around a release, we could work out issues in good time and stay in control.\n\nAnother benefit of FYI bots is more granular performance tests. Before the FYI bots, it would frequently happen that a bunch of metrics regressed. However, it\u2019s not fun to find which of the 100 patches in the roll caused the regression! With the FYI bots, we can see precisely which WebRTC revision caused the problem.\n\nFuture Work: Optimistic Auto-rolling\nThe final step on this ladder (short of actually merging the repositories) is auto-rolling. The Blink team implemented this with their ARB (AutoRollBot). The bot wakes up periodically and tries to do a roll patch. If it fails on the trybots, it waits and tries again later (perhaps the trybots failed because of a flake or other temporary error, or perhaps the error was real but has been fixed).\n\nTo pull auto-rolling off, you are going to need very good tests. That goes for any roll patch (or any patch, really), but if you\u2019re edging closer to a release and an unstoppable flood of code changes keep breaking you, you\u2019re not in a good place.\n\n\n\nReferences\n[1] Martin Fowler (May 2006) \u201cContinuous Integration\u201d\n[2] Dani Megert, Remy Chi Jian Suen, et. al. (Oct 2014) \u201cEvolving Java-based APIs\u201d\n\nFootnotes\n\nWe actually did have a hilarious bug in WebRTC where it was possible to set the volume to 1.1, but only 0.0-1.0 was supposed to be allowed. No, really. Thus, our WebRTC implementation must be louder than the others since everybody knows 1.1 must be louder than 1.0.", "by Mike Wacker\n\n\nAt some point in your life, you can probably recall a movie that you and your friends all wanted to see, and that you and your friends all regretted watching afterwards. Or maybe you remember that time your team thought they\u2019d found the next \"killer feature\" for their product, only to see that feature bomb after it was released.\n\n\nGood ideas often fail in practice, and in the world of testing, one pervasive good idea that often fails in practice is a testing strategy built around end-to-end tests.\n\n\nTesters can invest their time in writing many types of automated tests, including unit tests, integration tests, and end-to-end tests, but this strategy invests mostly in end-to-end tests that verify the product or service as a whole. Typically, these tests simulate real user scenarios.\n\nEnd-to-End Tests in Theory\u00a0\nWhile relying primarily on end-to-end tests is a bad idea, one could certainly convince a reasonable person that the idea makes sense in theory.\n\n\nTo start, number one on Google's list of ten things we know to be true is: \"Focus on the user and all else will follow.\" Thus, end-to-end tests that focus on real user scenarios sound like a great idea. Additionally, this strategy broadly appeals to many constituencies:\n\nDevelopers like it because it offloads most, if not all, of the testing to others.\u00a0\nManagers and decision-makers like it because tests that simulate real user scenarios can help them easily determine how a failing test would impact the user.\u00a0\nTesters like it because they often worry about missing a bug or writing a test that does not verify real-world behavior; writing tests from the user's perspective often avoids both problems and gives the tester a greater sense of accomplishment.\u00a0\n\n\nEnd-to-End Tests in Practice\u00a0\nSo if this testing strategy sounds so good in theory, then where does it go wrong in practice? To demonstrate, I present the following composite sketch based on a collection of real experiences familiar to both myself and other testers. In this sketch, a team is building a service for editing documents online (e.g., Google Docs).\n\n\nLet's assume the team already has some fantastic test infrastructure in place. Every night:\n\nThe latest version of the service is built.\u00a0\nThis version is then deployed to the team's testing environment.\u00a0\nAll end-to-end tests then run against this testing environment.\u00a0\nAn email report summarizing the test results is sent to the team.\n\n\nThe deadline is approaching fast as our team codes new features for their next release. To maintain a high bar for product quality, they also require that at least 90% of their end-to-end tests pass before features are considered complete. Currently, that deadline is one day away:\n\n\n\n\n\nDays LeftPass %Notes\n15%Everything is broken! Signing in to the service is broken. Almost all tests sign in a user, so almost all tests failed.\n04%A partner team we rely on deployed a bad build to their testing environment yesterday.\n-154%A dev broke the save scenario yesterday (or the day before?). Half the tests save a document at some point in time. Devs spent most of the day determining if it's a frontend bug or a backend bug.\n-254%It's a frontend bug, devs spent half of today figuring out where.\n-354%A bad fix was checked in yesterday. The mistake was pretty easy to spot, though, and a correct fix was checked in today.\n-41%Hardware failures occurred in the lab for our testing environment.\n-584%Many small bugs hiding behind the big bugs (e.g., sign-in broken, save broken). Still working on the small bugs.\n-687%We should be above 90%, but are not for some reason.\n-789.54%(Rounds up to 90%, close enough.) No fixes were checked in yesterday, so the tests must have been flaky yesterday.\n\n\n\nAnalysis\u00a0\nDespite numerous problems, the tests ultimately did catch real bugs.\n\n\nWhat Went Well\u00a0\n\nCustomer-impacting bugs were identified and fixed before they reached the customer.\n\n\n\nWhat Went Wrong\u00a0\n\nThe team completed their coding milestone a week late (and worked a lot of overtime).\u00a0\nFinding the root cause for a failing end-to-end test is painful and can take a long time.\u00a0\nPartner failures and lab failures ruined the test results on multiple days.\u00a0\nMany smaller bugs were hidden behind bigger bugs.\u00a0\nEnd-to-end tests were flaky at times.\u00a0\nDevelopers had to wait until the following day to know if a fix worked or not.\u00a0\n\n\nSo now that we know what went wrong with the end-to-end strategy, we need to change our approach to testing to avoid many of these problems. But what is the right approach?\n\nThe True Value of Tests\u00a0\nTypically, a tester's job ends once they have a failing test. A bug is filed, and then it's the developer's job to fix the bug. To identify where the end-to-end strategy breaks down, however, we need to think outside this box and approach the problem from first principles. If we \"focus on the user (and all else will follow),\" we have to ask ourselves how a failing test benefits the user. Here is the answer:\n\nA failing test does not directly benefit the user.\u00a0\n\nWhile this statement seems shocking at first, it is true. If a product works, it works, whether a test says it works or not. If a product is broken, it is broken, whether a test says it is broken or not. So, if failing tests do not benefit the user, then what does benefit the user?\n\nA bug fix directly benefits the user.\n\nThe user will only be happy when that unintended behavior - the bug - goes away. Obviously, to fix a bug, you must know the bug exists. To know the bug exists, ideally you have a test that catches the bug (because the user will find the bug if the test does not). But in that entire process, from failing test to bug fix, value is only added at the very last step.\n\n\n\n\n\nStageFailing TestBug OpenedBug Fixed\nValue AddedNoNoYes\n\n\nThus, to evaluate any testing strategy, you cannot just evaluate how it finds bugs. You also must evaluate how it enables developers to fix (and even prevent) bugs.\n\nBuilding the Right Feedback Loop\nTests create a feedback loop that informs the developer whether the product is working or not. The ideal feedback loop has several properties:\n\nIt's fast. No developer wants to wait hours or days to find out if their change works. Sometimes the change does not work - nobody is perfect - and the feedback loop needs to run multiple times. A faster feedback loop leads to faster fixes. If the loop is fast enough, developers may even run tests before checking in a change.\u00a0\nIt's reliable. No developer wants to spend hours debugging a test, only to find out it was a flaky test. Flaky tests reduce the developer's trust in the test, and as a result flaky tests are often ignored, even when they find real product issues.\u00a0\nIt isolates failures. To fix a bug, developers need to find the specific lines of code causing the bug. When a product contains millions of lines of codes, and the bug could be anywhere, it's like trying to find a needle in a haystack.\u00a0\n\n\nThink Smaller, Not Larger\nSo how do we create that ideal feedback loop? By thinking smaller, not larger.\n\n\nUnit Tests\nUnit tests take a small piece of the product and test that piece in isolation. They tend to create that ideal feedback loop:\n\n\nUnit tests are fast. We only need to build a small unit to test it, and the tests also tend to be rather small. In fact, one tenth of a second is considered slow for unit tests.\u00a0\nUnit tests are reliable. Simple systems and small units in general tend to suffer much less from flakiness. Furthermore, best practices for unit testing - in particular practices related to hermetic tests - will remove flakiness entirely.\u00a0\nUnit tests isolate failures. Even if a product contains millions of lines of code, if a unit test fails, you only need to search that small unit under test to find the bug.\u00a0\n\n\nWriting effective unit tests requires skills in areas such as dependency management, mocking, and hermetic testing. I won't cover these skills here, but as a start, the typical example offered to new Googlers (or Nooglers) is how Google builds and tests a stopwatch.\n\n\nUnit Tests vs. End-to-End Tests\nWith end-to-end tests, you have to wait: first for the entire product to be built, then for it to be deployed, and finally for all end-to-end tests to run. When the tests do run, flaky tests tend to be a fact of life. And even if a test finds a bug, that bug could be anywhere in the product.\n\nAlthough end-to-end tests do a better job of simulating real user scenarios, this advantage quickly becomes outweighed by all the disadvantages of the end-to-end feedback loop:\n\n\n\n\n\nUnitEnd-toEnd\nFast\n\n\n\n\nReliable\n\n\n\n\n\n\nIsolates Failures\n\n\n\n\nSimulates a Real User\n\n\n\n\n\n\n\nIntegration Tests\nUnit tests do have one major disadvantage: even if the units work well in isolation, you do not know if they work well together. But even then, you do not necessarily need end-to-end tests. For that, you can use an integration test. An integration test takes a small group of units, often two units, and tests their behavior as a whole, verifying that they coherently work together.\n\nIf two units do not integrate properly, why write an end-to-end test when you can write a much smaller, more focused integration test that will detect the same bug? While you do need to think larger, you only need to think a little larger to verify that units work together.\n\nTesting Pyramid\nEven with both unit tests and integration tests, you probably still will want a small number of end-to-end tests to verify the system as a whole. To find the right balance between all three test types, the best visual aid to use is the testing pyramid. Here is a simplified version of the testing pyramid from the opening keynote of the 2014 Google Test Automation Conference:\n\n\n\n\n\n\n\nThe bulk of your tests are unit tests at the bottom of the pyramid. As you move up the pyramid, your tests gets larger, but at the same time the number of tests (the width of your pyramid) gets smaller.\n\nAs a good first guess, Google often suggests a 70/20/10 split: 70% unit tests, 20% integration tests, and 10% end-to-end tests. The exact mix will be different for each team, but in general, it should retain that pyramid shape. Try to avoid these anti-patterns:\n\nInverted pyramid/ice cream cone. The team relies primarily on end-to-end tests, using few integration tests and even fewer unit tests.\u00a0\nHourglass. The team starts with a lot of unit tests, then uses end-to-end tests where integration tests should be used. The hourglass has many unit tests at the bottom and many end-to-end tests at the top, but few integration tests in the middle.\u00a0\n\nJust like a regular pyramid tends to be the most stable structure in real life, the testing pyramid also tends to be the most stable testing strategy.", "UPDATE:\u00a0Hey, this was an April fool's joke but in fact we wished we could\n\nhave realized this idea and we are looking forward to the day this has\n\nbeen worked out and becomes a reality.\n\nby Kevin Graney\n\n\nHere at Google we have a long history of capitalizing on the latest research and technology to improve the quality of our software.  Over our past 16+ years as a company, what started with some humble unit tests has grown into a massive operation.  As our software complexity increased, ever larger and more complex tests were dreamed up by our Software Engineers in Test (SETs).\n\n\nWhat we have come to realize is that our love of testing is a double-edged sword.  On the one hand, large-scale testing keeps us honest and gives us confidence.  It ensures our products remain reliable, our users' data is kept safe, and our engineers are able to work productively without fear of breaking things.  On the other hand, it's expensive in both engineer and machine time.  Our SETs have been working tirelessly to reduce the expense and latency of software tests at Google, while continuing to increase their quality.\n\n\nToday, we're excited to reveal how Google is tackling this challenge.  In collaboration with the Quantum AI Lab, SETs at Google have been busy revolutionizing how software is tested.  The theory is relatively simple: bits in a traditional computer are either zero or one, but bits in a quantum computer can be both one and zero at the same time.  This is known as superposition, and the classic example is Schrodinger's cat.  Through some clever math and cutting edge electrical engineering, researchers at Google have figured out how to utilize superposition to vastly improve the quality of our software testing and the speed at which our tests run.\n\n\n\n\n\n\nFigure 1 Some qubits inside a Google quantum device.\n\n\nWith superposition, tests at Google are now able to simultaneously model every possible state of the application under test.  The state of the application can be thought of as an n bit sequential memory buffer, consistent with the traditional Von Neuman architecture of computing.  Because each bit under superposition is simultaneously a 0 and a 1, these tests can simulate 2n different application states at any given instant in time in O(n) space.  Each of these application states can be mutated by application logic to another state in constant time using quantum algorithms developed by Google researchers.  These two properties together allow us to build a state transition graph of the application under test that shows every possible application state and all possible transitions to other application states.  Using traditional computing methods this problem has intractable time complexity, but after leveraging superposition and our quantum algorithms it becomes relatively fast and cheap.\n\n\n\n\n\nFigure 2 The application state graph for a demonstrative 3-bit application.  If the start state is 001 then 000, 110, 111, and 011 are all unreachable states.  States 010 and 100 both result in deadlock.\n\n\n\nOnce we have the state transition graph for the application under test, testing it becomes almost trivial.  Given the initial startup state of the application, i.e. the executable bits of the application stored on disk, we can find from the application's state transition graph all reachable states.  Assertions that ensure proper behavior are then written against the reachable subset of the transition graph.  This paradigm of test writing allows both Google's security engineers and software engineers to work more productively.  A security engineer can write a test, for example, that asserts \"no executable memory regions become mutated in any reachable state\".  This one test effectively eliminates the potential for security flaws that result from memory safety violations.  A test engineer can write higher level assertions using graph traversal methods that ensure data integrity is maintained across a subset of application state transitions.  Tests of this nature can detect data corruption bugs.\n\n\nWe're excited about the work our team has done so far to push the envelope in the field of quantum software quality.  We're just getting started, but based on early dogfood results among Googlers we believe the potential of this work is huge.  Stay tuned!", "by Mona El Mahdy\n\n\nOverview\n\n\nThis post reviews four strategies for Android UI testing with the goal of creating UI tests that are fast, reliable, and easy to debug.\n\nBefore we begin, let\u2019s not forget an important rule: whatever can be unit tested should be unit tested. Robolectric and gradle unit tests support are great examples of unit test frameworks for Android. UI tests, on the other hand, are used to verify that your application returns the correct UI output in response to a sequence of user actions on a device. Espresso is a great framework for running UI actions and verifications in the same process. For more details on the Espresso and UI Automator tools, please see: test support libraries.\n\n\nThe Google+ team has performed many iterations of UI testing. Below we discuss the lessons learned during each strategy of UI testing. Stay tuned for more posts with more details and code samples.\n\n\nStrategy 1: Using an End-To-End Test as a UI Test\n\n\nLet\u2019s start with some definitions. A UI test ensures that your application returns the correct UI output in response to a sequence of user actions on a device. An end-to-end (E2E) test brings up the full system of your app including all backend servers and client app. E2E tests will guarantee that data is sent to the client app and that the entire system functions correctly.\n\n\nUsually, in order to make the application UI functional, you need data from backend servers, so UI tests need to simulate the data but not necessarily the backend servers. In many cases UI tests are confused with E2E tests because E2E is very similar to manual test scenarios. However, debugging and stabilizing E2E tests is very difficult due to many variables like network flakiness, authentication against real servers, size of your system, etc.\n\n\n\n\n\nWhen you use UI tests as E2E tests, you face the following problems:\n\nVery large and slow tests.\u00a0\nHigh flakiness rate due to timeouts and memory issues.\u00a0\nHard to debug/investigate failures.\u00a0\nAuthentication issues (ex: authentication from automated tests is very tricky).\n\n\nLet\u2019s see how these problems can be fixed using the following strategies.\n\nStrategy 2: Hermetic UI Testing using Fake Servers\n\nIn this strategy, you avoid network calls and external dependencies, but you need to provide your application with data that drives the UI. Update your application to communicate to a local server rather than external one, and create a fake local server that provides data to your application. You then need a mechanism to generate the data needed by your application. This can be done using various approaches depending on your system design. One approach is to record server responses and replay them in your fake server.\n\n\nOnce you have hermetic UI tests talking to a local fake server, you should also have server hermetic tests. This way you split your E2E test into a server side test, a client side test, and an integration test to verify that the server and client are in sync (for more details on integration tests, see the backend testing section of blog).\n\n\nNow, the client test flow looks like:\n\n\n\n\n\nWhile this approach drastically reduces the test size and flakiness rate, you still need to maintain a separate fake server as well as your test. Debugging is still not easy as you have two moving parts: the test and the local server. While test stability will be largely improved by this approach, the local server will cause some flakes.\n\n\nLet\u2019s see how this could this be improved...\n\n\nStrategy 3: Dependency Injection Design for Apps. \n\n\nTo remove the additional dependency of a fake server running on Android, you should use dependency injection in your application for swapping real module implementations with fake ones. One example is Dagger, or you can create your own dependency injection mechanism if needed.\n\n\nThis will improve the testability of your app for both unit testing and UI testing, providing your tests with the ability to mock dependencies. In instrumentation testing, the test apk and the app under test are loaded in the same process, so the test code has runtime access to the app code. Not only that, but you can also use classpath override (the fact that test classpath takes priority over app under test) to override a certain class and inject test fakes there. For example, To make your test hermetic, your app should support injection of the networking implementation. During testing, the test injects a fake networking implementation to your app, and this fake implementation will provide seeded data instead of communicating with backend servers.\n\n\n\n\nStrategy 4: Building Apps into Smaller Libraries\n\n\nIf you want to scale your app into many modules and views, and plan to add more features while maintaining stable and fast builds/tests, then you should build your app into small components/libraries. Each library should have its own UI resources and user dependency management. This strategy not only enables mocking dependencies of your libraries for hermetic testing, but  also serves as an experimentation platform for various components of your application.\n\n\nOnce you have small components with dependency injection support, you can build a test app for each component.\n\n\nThe test apps bring up the actual UI of your libraries, fake data needed, and mock dependencies. Espresso tests will run against these test apps. This enables testing of smaller libraries in isolation.\n\n\nFor example, let\u2019s consider building smaller libraries for login and settings of your app.\n\n\n\n\n\nThe settings component test now looks like:\n\n\n\n\n\nConclusion\n\n\nUI testing can be very challenging for rich apps on Android. Here are some UI testing lessons learned on the Google+ team:\n\nDon\u2019t write E2E tests instead of UI tests. Instead write unit tests and integration tests beside the UI tests.\u00a0\nHermetic tests are the way to go.\u00a0\nUse dependency injection while designing your app.\u00a0\nBuild your application into small libraries/modules, and test each one in isolation. You can then have a few integration tests to verify integration between components is correct .\u00a0\nComponentized UI tests have proven to be much faster than E2E and 99%+ stable. Fast and stable tests have proven to drastically improve developer productivity.", "By Andrew Trenk\n\n\nThe Testing on the Toilet (TotT) series was created in 2006 as a way to spread unit-testing knowledge across Google by posting flyers in bathroom stalls. It quickly became a part of Google culture and is still going strong today, with new episodes published every week and read in hundreds of bathrooms by thousands of engineers in Google offices across the world. Initially focused on content related to testing, TotT now covers a variety of technical topics, such as tips on writing cleaner code and ways to prevent security bugs.\n\n\nWhile TotT episodes often have a big impact on many engineers across Google, until now we never did anything to formally thank authors for their contributions. To fix that, we decided to honor the most popular TotT episodes of 2014 by establishing the Testing on the Toilet Awards. The winners were chosen through a vote that was open to all Google engineers. The Google Testing Blog is proud to present the winners that were posted on this blog (there were two additional winners that weren\u2019t posted on this blog since we only post testing-related TotT episodes).\n\n\nAnd the winners are ...\n\n\nErik Kuefler: Test Behaviors, Not Methods and Don't Put Logic in Tests\u00a0\n\nAlex Eagle: Change-Detector Tests Considered Harmful\n\nThe authors of these episodes received their very own Flushy trophy, which they can proudly display on their desks.\n\n\n\n\n\n\n(The logo on the trophy is the same one we put on the printed version of each TotT episode, which you can see by looking for the \u201cprinter-friendly version\u201d link in the TotT blog posts).\n\n\nCongratulations to the winners!", "by Alex Eagle\n\nThis article was adapted from a \nGoogle Testing on the Toilet (TotT) episode. You can download a \nprinter-friendly version of this TotT episode and post it in your office.\n\n\n\nYou have just finished refactoring some code without modifying its behavior. Then you run the tests before committing and\u2026 a bunch of unit tests are failing. While fixing the tests, you get a sense that you are wasting time by mechanically applying the same transformation to many tests. Maybe you introduced a parameter in a method, and now must update 100 callers of that method in tests to pass an empty string.\n\n\nWhat does it look like to write tests mechanically? Here is an absurd but obvious way:\n// Production code:\ndef abs(i: Int)\n  return (i < 0) ? i * -1 : i\n\n// Test code:\nfor (line: String in File(prod_source).read_lines())\n  switch (line.number)\n    1: assert line.content equals def abs(i: Int)\n    2: assert line.content equals   return (i < 0) ? i * -1 : i\n\nThat test is clearly not useful: it contains an exact copy of the code under test and acts like a checksum. A correct or incorrect program is equally likely to pass a test that is a derivative of the code under test. No one is really writing tests like that, but how different is it from this next example?\n// Production code:\ndef process(w: Work)\n  firstPart.process(w)\n  secondPart.process(w)\n\n// Test code:\npart1 = mock(FirstPart)\npart2 = mock(SecondPart)\nw = Work()\nProcessor(part1, part2).process(w)\nverify_in_order\n  was_called part1.process(w)\n  was_called part2.process(w)\n\nIt is tempting to write a test like this because it requires little thought and will run quickly. This is a change-detector test\u2014it is a transformation of the same information in the code under test\u2014and it breaks in response to any change to the production code, without verifying correct behavior of either the original or modified production code.\n\n\nChange detectors provide negative value, since the tests do not catch any defects, and the added maintenance cost slows down development. These tests should be re-written or deleted.", "by Andrew Trenk\n\nThis article was adapted from a \nGoogle Testing on the Toilet (TotT) episode. You can download a \nprinter-friendly version of this TotT episode and post it in your office.\n\n\n\nDoes this class need to have tests?\n\n\nclass UserInfoValidator {\n  public void validate(UserInfo info) {\n    if (info.getDateOfBirth().isInFuture()) { throw new ValidationException()); }\n  }\n}\nIts method has some logic, so it may be good idea to test it. But what if its only user looks like this?\n\n\n\npublic class UserInfoService {\n  private UserInfoValidator validator;\n  public void save(UserInfo info) {\n    validator.validate(info); // Throw an exception if the value is invalid.\n    writeToDatabase(info);   \n  }\n}\nThe answer is: it probably doesn\u2019t need tests, since all paths can be tested through UserInfoService. The key distinction is that the class is an implementation detail, not a public API.\n\nA public API can be called by any number of users, who can pass in any possible combination of inputs to its methods. You want to make sure these are well-tested, which ensures users won\u2019t see issues when they use the API. Examples of public APIs include classes that are used in a different part of a codebase (e.g., a server-side class that\u2019s used by the client-side) and common utility classes that are used throughout a codebase.\n\n\nAn implementation-detail class exists only to support public APIs and is called by a very limited number of users (often only one). These classes can sometimes be tested indirectly by testing the public APIs that use them. \n\n\nTesting implementation-detail classes is still useful in many cases, such as if the class is complex or if the tests would be difficult to write for the public API class. When you do test them, they often don\u2019t need to be tested in as much depth as a public API, since some inputs may never be passed into their methods (in the above code sample, if UserInfoService ensured that UserInfo were never null, then it wouldn\u2019t be useful to test what happens when null is passed as an argument to UserInfoValidator.validate, since it would never happen).\n\n\nImplementation-detail classes can sometimes be thought of as private methods that happen to be in a separate class, since you typically don\u2019t want to test private methods directly either. You should also try to restrict the visibility of implementation-detail classes, such as by making them package-private in Java.\n\n\nTesting implementation-detail classes too often leads to a couple problems:\n\n\n- Code is harder to maintain since you need to update tests more often, such as when changing a method signature of an implementation-detail class or even when doing a refactoring. If testing is done only through public APIs, these changes wouldn\u2019t affect the tests at all.\n\n\n- If you test a behavior only through an implementation-detail class, you may get false confidence in your code, since the same code path may not work properly when exercised through the public API. You also have to be more careful when refactoring, since it can be harder to ensure that all the behavior of the public API will be preserved if not all paths are tested through the public API.", "by Dori Reuveni and Kurt Alfred Kluever\n\nThis article was adapted from a \nGoogle Testing on the Toilet (TotT) episode. You can download a \nprinter-friendly version of this TotT episode and post it in your office.\n\n\n\nAs engineers, we spend most of our time reading existing code, rather than writing new code. Therefore, we must make sure we always write clean, readable code. The same goes for our tests; we need a way to clearly express our test assertions.\n\nTruth is an open source, fluent testing framework for Java designed to make your test assertions and failure messages more readable. The fluent API makes reading (and writing) test assertions much more natural, prose-like, and discoverable in your IDE via autocomplete. For example, compare how the following assertion reads with JUnit vs. Truth:\nassertEquals(\"March\", monthMap.get(3));          // JUnit\nassertThat(monthMap).containsEntry(3, \"March\");  // Truth\nBoth statements are asserting the same thing, but the assertion written with Truth can be easily read from left to right, while the JUnit example requires \"mental backtracking\".\n\nAnother benefit of Truth over JUnit is the addition of useful default failure messages. For example:\nImmutableSet<String> colors = ImmutableSet.of(\"red\", \"green\", \"blue\", \"yellow\");\nassertTrue(colors.contains(\"orange\"));  // JUnit\nassertThat(colors).contains(\"orange\");  // Truth\nIn this example, both assertions will fail, but JUnit will not provide a useful failure message. However, Truth will provide a clear and concise failure message:\n\nAssertionError: <[red, green, blue, yellow]> should have contained <orange>\n\nTruth already supports specialized assertions for most of the common JDK types (Objects, primitives, arrays, Strings, Classes, Comparables, Iterables, Collections, Lists, Sets, Maps, etc.), as well as some Guava types (Optionals). Additional support for other popular types is planned as well (Throwables, Iterators, Multimaps, UnsignedIntegers, UnsignedLongs, etc.).\n\n\nTruth is also user-extensible: you can easily write a Truth subject to make fluent assertions about your own custom types. By creating your own custom subject, both your assertion API and your failure messages can be domain-specific. \n\n\nTruth's goal is not to replace JUnit assertions, but to improve the readability of complex assertions and their failure messages. JUnit assertions and Truth assertions can (and often do) live side by side in tests.\n\n\nTo get started with Truth, check out http://google.github.io/truth/", "by Anthony Vallone on behalf of the GTAC Committee\n\n\nOn October 28th and 29th, GTAC 2014, the eighth GTAC (Google Test Automation Conference), was held at the beautiful Google Kirkland office. The conference was completely packed with presenters and attendees from all over the world (Argentina, Australia, Canada, China, many European countries, India, Israel, Korea, New Zealand, Puerto Rico, Russia, Taiwan, and many US states), bringing with them a huge diversity of experiences.\n\n\n\n\nSpeakers from numerous companies and universities (Adobe, American Express, Comcast, Dropbox, Facebook, FINRA, Google, HP, Medidata Solutions, Mozilla, Netflix, Orange, and University of Waterloo) spoke on a variety of interesting and cutting edge test automation topics.\n\nAll of the slides and video recordings are now available on the GTAC site. Photos will be available soon as well.\n\n\n\n\nThis was our most popular GTAC to date, with over 1,500 applicants and almost 200 of those for speaking. About 250 people filled our venue to capacity, and the live stream had a peak of about 400 concurrent viewers with 4,700 playbacks during the event. And, there was plenty of interesting Twitter and Google+ activity during the event.\n\n\n\n\nOur goal in hosting GTAC is to make the conference highly relevant and useful for, not only attendees, but the larger test engineering community as a whole. Our post-conference survey shows that we are close to achieving that goal:\n\n\n\n\n\n\nIf you have any suggestions on how we can improve, please comment on this post.\n\n\nThank you to all the speakers, attendees, and online viewers who made this a special event once again. To receive announcements about the next GTAC, subscribe to the Google Testing Blog.", "By Hank Duan, Julie Ralph, and Arif Sukoco in Seattle\n\n\nHave you worked with WebDriver but been frustrated with all the waits needed for WebDriver to sync with the website, causing flakes and prolonged test times? If you are working with AngularJS apps, then Protractor is the right tool for you.\n\n\nProtractor (protractortest.org) is an end-to-end test framework specifically for AngularJS apps. It was built by a team in Google and released to open source. Protractor is built on top of WebDriverJS and includes important improvements tailored for AngularJS apps. Here are some of Protractor\u2019s key benefits:\n\n\nYou don\u2019t need to add waits or sleeps to your test. Protractor can communicate with your AngularJS app automatically and execute the next step in your test the moment the webpage finishes pending tasks, so you don\u2019t have to worry about waiting for your test and webpage to sync.\u00a0\nIt supports Angular-specific locator strategies (e.g., binding, model, repeater) as well as native WebDriver locator strategies (e.g., ID, CSS selector, XPath). This allows you to test Angular-specific elements without any setup effort on your part.\u00a0\nIt is easy to set up page objects. Protractor does not execute WebDriver commands until an action is needed (e.g., get, sendKeys, click). This way you can set up page objects so tests can manipulate page elements without touching the HTML.\u00a0\nIt uses Jasmine, the framework you use to write AngularJS unit tests, and Javascript, the same language you use to write AngularJS apps.\n\n\nFollow these simple steps, and in minutes, you will have you first Protractor test running: \n\n\n1) Set up environment\n\n\nInstall the command line tools \u2018protractor\u2019 and \u2018webdriver-manager\u2019 using npm:\nnpm install -g protractor\n\nStart up an instance of a selenium server:\nwebdriver-manager update & webdriver-manager start\n\nThis downloads the necessary binary, and starts a new webdriver session listening on http://localhost:4444.\n\n\n2) Write your test\n// It is a good idea to use page objects to modularize your testing logic\nvar angularHomepage = {\n  nameInput : element(by.model('yourName')),\n  greeting : element(by.binding('yourName')),\n  get : function() {\n    browser.get('index.html');\n  },\n  setName : function(name) {\n    this.nameInput.sendKeys(name);\n  }\n};\n\n// Here we are using the Jasmine test framework \n// See http://jasmine.github.io/2.0/introduction.html for more details\ndescribe('angularjs homepage', function() {\n  it('should greet the named user', function(){\n    angularHomepage.get();\n    angularHomepage.setName('Julie');\n    expect(angularHomepage.greeting.getText()).\n        toEqual('Hello Julie!');\n  });\n});\n\n3) Write a Protractor configuration file to specify the environment under which you want your test to run:\nexports.config = {\n  seleniumAddress: 'http://localhost:4444/wd/hub',\n  \n  specs: ['testFolder/*'],\n\n  multiCapabilities: [{\n    'browserName': 'chrome',\n    // browser-specific tests\n    specs: 'chromeTests/*' \n  }, {\n    'browserName': 'firefox',\n    // run tests in parallel\n    shardTestFiles: true \n  }],\n\n  baseUrl: 'http://www.angularjs.org',\n};\n\n4) Run the test:\n\n\nStart the test with the command:\n\nprotractor conf.js\n\nThe test output should be:\n\n1 test, 1 assertions, 0 failures\n\n\nIf you want to learn more, here\u2019s a full tutorial that highlights all of Protractor\u2019s features: http://angular.github.io/protractor/#/tutorial", "by Anthony Vallone on behalf of the GTAC Committee \n\n\nThe eighth GTAC commences on Tuesday at the Google Kirkland office. You can find the latest details on the conference at our site, including speaker profiles.\n\n\nIf you are watching remotely, we'll soon be updating the live stream page with the stream link and a Google Moderator link for remote Q&A.\n\n\nIf you have been selected to attend or speak, be sure to note the updated parking information. Google visitors will use off-site parking and shuttles.\n\n\nWe look forward to connecting with the greater testing community and sharing new advances and ideas.", "by Andrew Trenk\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nHow long does it take you to figure out what behavior is being tested in the following code?\n\n\n@Test public void isUserLockedOut_invalidLogin() {\n  authenticator.authenticate(username, invalidPassword);\n  assertFalse(authenticator.isUserLockedOut(username));\n\n  authenticator.authenticate(username, invalidPassword);\n  assertFalse(authenticator.isUserLockedOut(username));\n\n  authenticator.authenticate(username, invalidPassword);\n  assertTrue(authenticator.isUserLockedOut(username));\n}\n\nYou probably had to read through every line of code (maybe more than once) and understand what each line is doing. But how long would it take you to figure out what behavior is being tested if the test had this name?\n\nisUserLockedOut_lockOutUserAfterThreeInvalidLoginAttempts\n\n\nYou should now be able to understand what behavior is being tested by reading just the test name, and you don\u2019t even need to read through the test body. The test name in the above code sample hints at the scenario being tested (\u201cinvalidLogin\u201d), but it doesn\u2019t actually say what the expected outcome is supposed to be, so you had to read through the code to figure it out.\n\n\nPutting both the scenario and the expected outcome in the test name has several other benefits:\n\n\n- If you want to know all the possible behaviors a class has, all you need to do is read through the test names in its test class, compared to spending minutes or hours digging through the test code or even the class itself trying to figure out its behavior. This can also be useful during code reviews since you can quickly tell if the tests cover all expected cases.\n\n\n- By giving tests more explicit names, it forces you to split up testing different behaviors into separate tests. Otherwise you may be tempted to dump assertions for different behaviors into one test, which over time can lead to tests that keep growing and become difficult to understand and maintain.\n\n\n- The exact behavior being tested might not always be clear from the test code. If the test name isn\u2019t explicit about this, sometimes you might have to guess what the test is actually testing.\n\n\n- You can easily tell if some functionality isn\u2019t being tested. If you don\u2019t see a test name that describes the behavior you\u2019re looking for, then you know the test doesn\u2019t exist.\n\n\n\n- When a test fails, you can immediately see what functionality is broken without looking at the test\u2019s source code.\n\n\nThere are several common patterns for structuring the name of a test (one example is to name tests like an English sentence with \u201cshould\u201d in the name, e.g., shouldLockOutUserAfterThreeInvalidLoginAttempts). Whichever pattern you use, the same advice still applies: Make sure test names contain both the scenario being tested and the expected outcome.\n\n\n\nSometimes just specifying the name of the method under test may be enough, especially if the method is simple and has only a single behavior that is obvious from its name.", "by Anthony Vallone on behalf of the GTAC Committee \n\n\nWe have completed selection and confirmation of all speakers and attendees for GTAC 2014. You can find the detailed agenda at:\n\n\u00a0\u00a0developers.google.com/gtac/2014/schedule\n\nThank you to all who submitted proposals! It was very hard to make selections from so many fantastic submissions.\n\n\nThere was a tremendous amount of interest in GTAC this year with over 1,500 applicants (up from 533 last year) and 194 of those for speaking (up from 88 last year). Unfortunately, our venue only seats 250. However, don\u2019t despair if you did not receive an invitation. Just like last year, anyone can join us via YouTube live streaming. We\u2019ll also be setting up Google Moderator, so remote attendees can get involved in Q&A after each talk. Information about live streaming, Moderator, and other details will be posted on the GTAC site soon and announced here.", "by Patrik H\u00f6glund\n\n\nThis is the second in a series of articles about Chrome\u2019s WebRTC Interop Test. See the first.\n\nIn the previous blog post we managed to write an automated test which got a WebRTC call between Firefox and Chrome to run. But how do we verify that the call actually worked?\n\n\nVerifying the Call\nNow we can launch the two browsers, but how do we figure out the whether the call actually worked? If you try opening two apprtc.appspot.com tabs in the same room, you will notice the video feeds flip over using a CSS transform, your local video is relegated to a small frame and a new big video feed with the remote video shows up. For the first version of the test, I just looked at the page in the Chrome debugger and looked for some reliable signal. As it turns out, the remoteVideo.style.opacity property will go from 0 to 1 when the call goes up and from 1 to 0 when it goes down. Since we can execute arbitrary JavaScript in the Chrome tab from the test, we can simply implement the check like this:\n\n\nbool WaitForCallToComeUp(content::WebContents* tab_contents) {\n  // Apprtc will set remoteVideo.style.opacity to 1 when the call comes up.\n  std::string javascript =\n      \"window.domAutomationController.send(remoteVideo.style.opacity)\";\n  return test::PollingWaitUntil(javascript, \"1\", tab_contents);\n}\n\n\n\nVerifying Video is Playing\nSo getting a call up is good, but what if there is a bug where Firefox and Chrome cannot send correct video streams to each other? To check that, we needed to step up our game a bit. We decided to use our existing video detector, which looks at a video element and determines if the pixels are changing. This is a very basic check, but it\u2019s better than nothing. To do this, we simply evaluate the .js file\u2019s JavaScript in the context of the Chrome tab, making the functions in the file available to us. The implementation then becomes\n\n\nbool DetectRemoteVideoPlaying(content::WebContents* tab_contents) {\n  if (!EvalInJavascriptFile(tab_contents, GetSourceDir().Append(\n      FILE_PATH_LITERAL(\n          \"chrome/test/data/webrtc/test_functions.js\"))))\n    return false;\n  if (!EvalInJavascriptFile(tab_contents, GetSourceDir().Append(\n      FILE_PATH_LITERAL(\n          \"chrome/test/data/webrtc/video_detector.js\"))))\n    return false;\n\n  // The remote video tag is called remoteVideo in the AppRTC code.\n  StartDetectingVideo(tab_contents, \"remoteVideo\");\n  WaitForVideoToPlay(tab_contents);\n  return true;\n}\n\nwhere StartDetectingVideo and WaitForVideoToPlay call the corresponding JavaScript methods in video_detector.js. If the video feed is frozen and unchanging, the test will time out and fail.\n\n\nWhat to Send in the Call\nNow we can get a call up between the browsers and detect if video is playing. But what video should we send? For chrome, we have a convenient --use-fake-device-for-media-stream flag that will make Chrome pretend there\u2019s a webcam and present a generated video feed (which is a spinning green ball with a timestamp). This turned out to be useful since Firefox and Chrome cannot acquire the same camera at the same time, so if we didn\u2019t use the fake device we would have two webcams plugged into the bots executing the tests!\n\n\nBots running in Chrome\u2019s regular test infrastructure do not have either software or hardware webcams plugged into them, so this test must run on bots with webcams for Firefox to be able to acquire a camera. Fortunately, we have that in the WebRTC waterfalls in order to test that we can actually acquire hardware webcams on all platforms. We also added a check to just succeed the test when there\u2019s no real webcam on the system since we don\u2019t want it to fail when a dev runs it on a machine without a webcam:\n\n\n\nif (!HasWebcamOnSystem())\n  return;\n\n\nIt would of course be better if Firefox had a similar fake device, but to my knowledge it doesn\u2019t.\n\n\nDownloading all Code and Components\u00a0\nNow we have all we need to run the test and have it verify something useful. We just have the hard part left: how do we actually download all the resources we need to run this test? Recall that this is actually a three-way integration test between Chrome, Firefox and AppRTC, which require the following:\n\n\nThe AppEngine SDK in order to bring up the local AppRTC instance,\u00a0\nThe AppRTC code itself,\u00a0\nChrome (already present in the checkout), and\u00a0\nFirefox nightly.\n\n\nWhile developing the test, I initially just hand-downloaded these and installed and hard-coded the paths. This is a very bad idea in the long run. Recall that the Chromium infrastructure is comprised of thousands and thousands of machines, and while this test will only run on perhaps 5 at a time due to its webcam requirements, we don\u2019t want manual maintenance work whenever we replace a machine. And for that matter, we definitely don\u2019t want to download a new Firefox by hand every night and put it on the right location on the bots! So how do we automate this?\n\n\nDownloading the AppEngine SDK\nFirst, let\u2019s start with the easy part. We don\u2019t really care if the AppEngine SDK is up-to-date, so a relatively stale version is fine. We could have the test download it from the authoritative source, but that\u2019s a bad idea for a couple reasons. First, it updates outside our control. Second, there could be anti-robot measures on the page. Third, the download will likely be unreliable and fail the test occasionally.\n\n\nThe way we solved this was to upload a copy of the SDK to a Google storage bucket under our control and download it using the depot_tools script download_from_google_storage.py. This is a lot more reliable than an external website and will not download the SDK if we already have the right version on the bot.\n\n\nDownloading the AppRTC Code\nThis code is on GitHub. Experience has shown that git clone commands run against GitHub will fail every now and then, and fail the test. We could either write some retry mechanism, but we have found it\u2019s better to simply mirror the git repository in Chromium\u2019s internal mirrors, which are closer to our bots and thereby more reliable from our perspective. The pull is done by a Chromium DEPS file (which is Chromium\u2019s dependency provisioning framework).\n\n\nDownloading Firefox\nIt turns out that Firefox supplies handy libraries for this task. We\u2019re using mozdownload in this script in order to download the Firefox nightly build. Unfortunately this fails every now and then so we would like to have some retry mechanism, or we could write some mechanism to \u201cmirror\u201d the Firefox nightly build in some location we control.\n\n\nPutting it Together\nWith that, we have everything we need to deploy the test. You can see the final code here. \n\n\nThe provisioning code above was put into a separate \u201c.gclient solution\u201d so that regular Chrome devs and bots are not burdened with downloading hundreds of megs of SDKs and code that they will not use. When this test runs, you will first see a Chrome browser pop up, which will ensure the local apprtc instance is up. Then a Firefox browser will pop up. They will each acquire the fake device and real camera, respectively, and after a short delay the AppRTC call will come up, proving that video interop is working.\n\n\nThis is a complicated and expensive test, but we believe it is worth it to keep the main interop case under automation this way, especially as the spec evolves and the browsers are in varying states of implementation.\n\n\nFuture Work\n\n\nAlso run on Windows/Mac.\u00a0\nAlso test Opera.\u00a0\nInterop between Chrome/Firefox mobile and desktop browsers.\u00a0\nAlso ensure audio is playing.\u00a0\nMeasure bandwidth stats, video quality, etc.", "by Patrik H\u00f6glund\n\n\nWebRTC enables real time peer-to-peer video and voice transfer in the browser, making it possible to build, among other things, a working video chat with a small amount of Python and JavaScript. As a web standard, it has several unusual properties which makes it hard to test. A regular web standard generally accepts HTML text and yields a bitmap as output (what you see in the browser). For WebRTC, we have real-time RTP media streams on one side being sent to another WebRTC-enabled endpoint. These RTP packets have been jumping across NAT, through firewalls and perhaps through TURN servers to deliver hopefully stutter-free and low latency media.\n\n\nWebRTC is probably the only web standard in which we need to test direct communication between Chrome and other browsers. Remember, WebRTC builds on peer-to-peer technology, which means we talk directly between browsers rather than through a server. Chrome, Firefox and Opera have announced support for WebRTC so far. To test interoperability, we set out to build an automated test to ensure that Chrome and Firefox can get a call up. This article describes how we implemented such a test and the tradeoffs we made along the way.\n\n\nCalling in WebRTC\n\nSetting up a WebRTC call requires passing  SDP blobs over a signaling connection. These blobs contain information on the capabilities of the endpoint, such as what media formats it supports and what preferences it has (for instance, perhaps the endpoint has VP8 decoding hardware, which means the endpoint will handle VP8 more efficiently than, say, H.264). By sending these blobs the endpoints can agree on what media format they will be sending between themselves and how to traverse the network between them. Once that is done, the browsers will talk directly to each other, and nothing gets sent over the signaling connection.\n\n\n\n\nFigure 1. Signaling and media connections.\n\nHow these blobs are sent is up to the application. Usually the browsers connect to some server which mediates the connection between the browsers, for instance by using a contact list or a room number. The AppRTC reference application uses room numbers to pair up browsers and sends the SDP blobs from the browsers through the AppRTC server.\n\n\nTest Design\nInstead of designing a new signaling solution from scratch, we chose to use the AppRTC application we already had. This has the additional benefit of testing the AppRTC code, which we are also maintaining. We could also have used the small peerconnection_server binary and some JavaScript, which would give us additional flexibility in what to test. We chose to go with AppRTC since it effectively implements the signaling for us, leading to much less test code.\n\n\nWe assumed we would be able to get hold of the latest nightly Firefox and be able to launch that with a given URL. For the Chrome side, we assumed we would be running in a browser test, i.e. on a complete Chrome with some test scaffolding around it. For the first sketch of the test, we imagined just connecting the browsers to the live apprtc.appspot.com with some random room number. If the call got established, we would be able to look at the remote video feed on the Chrome side and verify that video was playing (for instance using the video+canvas grab trick). Furthermore, we could verify that audio was playing, for instance by using WebRTC getStats to measure the audio track energy level.\n\n\n\n\n\nFigure 2. Basic test design.\n\nHowever, since we like tests to be hermetic, this isn\u2019t a good design. I can see several problems. For example, if the network between us and AppRTC is unreliable. Also, what if someone has occupied myroomid? If that were the case, the test would fail and we would be none the wiser. So to make this thing work, we would have to find some way to bring up the AppRTC instance on localhost to make our test hermetic.\n\n\nBringing up AppRTC on localhost\nAppRTC is a Google App Engine application. As this hello world example demonstrates, one can test applications locally with\ngoogle_appengine/dev_appserver.py apprtc_code/\n\nSo why not just call this from our test? It turns out we need to solve some complicated problems first, like how to ensure the AppEngine SDK and the AppRTC code is actually available on the executing machine, but we\u2019ll get to that later. Let\u2019s assume for now that stuff is just available. We can now write the browser test code to launch the local instance:\nbool LaunchApprtcInstanceOnLocalhost() \n  // ... Figure out locations of SDK and apprtc code ...\n  CommandLine command_line(CommandLine::NO_PROGRAM);\n  EXPECT_TRUE(GetPythonCommand(&command_line));\n\n  command_line.AppendArgPath(appengine_dev_appserver);\n  command_line.AppendArgPath(apprtc_dir);\n  command_line.AppendArg(\"--port=9999\");\n  command_line.AppendArg(\"--admin_port=9998\");\n  command_line.AppendArg(\"--skip_sdk_update_check\");\n\n  VLOG(1) << \"Running \" << command_line.GetCommandLineString();\n  return base::LaunchProcess(command_line, base::LaunchOptions(),\n                             &dev_appserver_);\n}\n\n\nThat\u2019s pretty straightforward [1].\n\n\nFiguring out Whether the Local Server is Up\u00a0\nThen we ran into a very typical test problem. So we have the code to get the server up, and launching the two browsers to connect to http://localhost:9999?r=some_room is easy. But how do we know when to connect? When I first ran the test, it would work sometimes and sometimes not depending on if the server had time to get up.\n\n\nIt\u2019s tempting in these situations to just add a sleep to give the server time to get up. Don\u2019t do that. That will result in a test that is flaky and/or slow. In these situations we need to identify what we\u2019re really waiting for. We could probably monitor the stdout of the dev_appserver.py and look for some message that says \u201cServer is up!\u201d or equivalent. However, we\u2019re really waiting for the server to be able to serve web pages, and since we have two browsers that are really good at connecting to servers, why not use them? Consider this code.\nbool LocalApprtcInstanceIsUp() {\n  // Load the admin page and see if we manage to load it right.\n  ui_test_utils::NavigateToURL(browser(), GURL(\"localhost:9998\"));\n  content::WebContents* tab_contents =\n      browser()->tab_strip_model()->GetActiveWebContents();\n  std::string javascript =\n      \"window.domAutomationController.send(document.title)\";\n  std::string result;\n  if (!content::ExecuteScriptAndExtractString(tab_contents, \n                                              javascript,\n                                              &result))\n    return false;\n\n  return result == kTitlePageOfAppEngineAdminPage;\n}\n\n\nHere we ask Chrome to load the AppEngine admin page for the local server (we set the admin port to 9998 earlier, remember?) and ask it what its title is. If that title is \u201cInstances\u201d, the admin page has been displayed, and the server must be up. If the server isn\u2019t up, Chrome will fail to load the page and the title will be something like \u201clocalhost:9999 is not available\u201d.\n\n\nThen, we can just do this from the test:\nwhile (!LocalApprtcInstanceIsUp())\n  VLOG(1) << \"Waiting for AppRTC to come up...\";\n\n\nIf the server never comes up, for whatever reason, the test will just time out in that loop. If it comes up we can safely proceed with the rest of test.\n\n\nLaunching the Browsers\u00a0\nA browser window launches itself as a part of every Chromium browser test. It\u2019s also easy for the test to control the command line switches the browser will run under.\n\n\nWe have less control over the Firefox browser since it is the \u201cforeign\u201d browser in this test, but we can still pass command-line options to it when we invoke the Firefox process. To make this easier, Mozilla provides a Python library called mozrunner. Using that we can set up a launcher python script we can invoke from the test:\nfrom mozprofile import profile\nfrom mozrunner import runner\n\nWEBRTC_PREFERENCES = {\n    'media.navigator.permission.disabled': True,\n}\n\ndef main():\n  # Set up flags, handle SIGTERM, etc\n  # ...\n  firefox_profile = \n      profile.FirefoxProfile(preferences=WEBRTC_PREFERENCES)\n  firefox_runner = runner.FirefoxRunner(\n      profile=firefox_profile, binary=options.binary, \n      cmdargs=[options.webpage])\n\n  firefox_runner.start()\n\nNotice that we need to pass special preferences to make Firefox accept the getUserMedia prompt. Otherwise, the test would get stuck on the prompt and we would be unable to set up a call. Alternatively, we could employ some kind of clickbot to click \u201cAllow\u201d on the prompt when it pops up, but that is way harder to set up.\n\n\nWithout going into too much detail, the code for launching the browsers becomes\nGURL room_url = \n    GURL(base::StringPrintf(\"http://localhost:9999?r=room_%d\",\n                            base::RandInt(0, 65536)));\ncontent::WebContents* chrome_tab = \n    OpenPageAndAcceptUserMedia(room_url);\nASSERT_TRUE(LaunchFirefoxWithUrl(room_url));\n\nWhere LaunchFirefoxWithUrl essentially runs this:\nrun_firefox_webrtc.py --binary /path/to/firefox --webpage http://localhost::9999?r=my_room\n\nNow we can launch the two browsers. Next time we will look at how we actually verify that the call worked, and how we actually download all resources needed by the test in a maintainable and automated manner. Stay tuned!\n\n\n1\nThe explicit ports are because the default ports collided on the bots we were running on, and the --skip_sdk_update_check was because the SDK stopped and asked us something if there was an update.", "by Ruslan Khamitov\u00a0\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\nAdding ID attributes to elements can make it much easier to write tests that interact with the DOM (e.g., WebDriver tests). Consider the following DOM with two buttons that differ only by inner text:\n\n\n\nSave button\nEdit button\n\n\n<div class=\"button\">Save</div>\n<div class=\"button\">Edit</div>\n\n\n\n\nHow would you tell WebDriver to interact with the \u201cSave\u201d button in this case?  You have several options.  One option is to interact with the button using a CSS selector:\ndiv.button\n\nHowever, this approach is not sufficient to identify a particular button, and there is no mechanism to filter by text in CSS. Another option would be to write an XPath, which is generally fragile and discouraged:\n//div[@class='button' and text()='Save']\n\nYour best option is to add unique hierarchical IDs where each widget is passed a base ID that it prepends to the ID of each of its children. The IDs for each button will be:\ncontact-form.save-button\ncontact-form.edit-button\n\nIn GWT you can accomplish this by overriding onEnsureDebugId()on your widgets. Doing so allows you to create custom logic for applying debug IDs to the sub-elements that make up a custom widget:\n@Override protected void onEnsureDebugId(String baseId) {\n  super.onEnsureDebugId(baseId);\n  saveButton.ensureDebugId(baseId + \".save-button\");\n  editButton.ensureDebugId(baseId + \".edit-button\");\n}\n\nConsider another example. Let\u2019s set IDs for repeated UI elements in Angular using ng-repeat. \nSetting an index can help differentiate between repeated instances of each element:\n<tr id=\"feedback-{{$index}}\" class=\"feedback\" ng-repeat=\"feedback in ctrl.feedbacks\" >\n\nIn GWT you can do this with ensureDebugId(). Let\u2019s set an ID for each of the table cells:\n@UiField FlexTable table;\nUIObject.ensureDebugId(table.getCellFormatter().getElement(rowIndex, columnIndex),\n    baseID + colIndex + \"-\" + rowIndex);\n\nTake-away: Debug IDs are easy to set and make a huge difference for testing. Please add them early.", "by Erik Kuefler\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office. \n\n\nProgramming languages give us a lot of expressive power. Concepts like operators and conditionals are important tools that allow us to write programs that handle a wide range of inputs. But this flexibility comes at the cost of increased complexity, which makes our programs harder to understand.\n\n\nUnlike production code, simplicity is more important than flexibility in tests. Most unit tests verify that a single, known input produces a single, known output. Tests can avoid complexity by stating their inputs and outputs directly rather than computing them. Otherwise it's easy for tests to develop their own bugs.\n\nLet's take a look at a simple example. Does this test look correct to you?\n\n\n@Test public void shouldNavigateToPhotosPage() {\n  String baseUrl = \"http://plus.google.com/\";\n  Navigator nav = new Navigator(baseUrl);\n  nav.goToPhotosPage();\n  assertEquals(baseUrl + \"/u/0/photos\", nav.getCurrentUrl());\n}\n\nThe author is trying to avoid duplication by storing a shared prefix in a variable. Performing a single string concatenation doesn't seem too bad, but what happens if we simplify the test by inlining the variable?\n\n\n@Test public void shouldNavigateToPhotosPage() {\n  Navigator nav = new Navigator(\"http://plus.google.com/\");\n  nav.goToPhotosPage();\n  assertEquals(\"http://plus.google.com//u/0/photos\", nav.getCurrentUrl()); // Oops!\n}\n\nAfter eliminating the unnecessary computation from the test, the bug is obvious\u2014we're expecting two slashes in the URL! This test will either fail or (even worse) incorrectly pass if the production code has the same bug. We never would have written this if we stated our inputs and outputs directly instead of trying to compute them. And this is a very simple example\u2014when a test adds more operators or includes loops and conditionals, it becomes increasingly difficult to be confident that it is correct.\n\n\nAnother way of saying this is that, whereas production code describes a general strategy for computing outputs given inputs, tests are concrete examples of input/output pairs (where output might include side effects like verifying interactions with other classes). It's usually easy to tell whether an input/output pair is correct or not, even if the logic required to compute it is very complex. For instance, it's hard to picture the exact DOM that would be created by a Javascript function for a given server response. So the ideal test for such a function would just compare against a string containing the expected output HTML.\n\n\nWhen tests do need their own logic, such logic should often be moved out of the test bodies and into utilities and helper functions. Since such helpers can get quite complex, it's usually a good idea for any nontrivial test utility to have its own tests.", "Posted by Anthony Vallone on behalf of the GTAC Committee\n\n\nThe deadline to sign up for GTAC 2014 is next Monday, July 28th, 2014. There is a great deal of interest to both attend and speak, and we\u2019ve received many outstanding proposals. However, it\u2019s not too late to add yours for consideration. If you would like to speak or attend, be sure to complete the form by Monday. \n\n\nWe will be making regular updates to our site over the next several weeks, and you can find conference details there:\n\u00a0 developers.google.com/gtac\n\nFor those that have already signed up to attend or speak, we will contact you directly in mid August.", "By Marko Ivankovi\u0107, Google Z\u00fcrich\n\nIntroduction\n\nCode coverage is a very interesting metric, covered by a large body of research that reaches somewhat contradictory results. Some people think it is an extremely useful metric and that a certain percentage of coverage should be enforced on all code. Some think it is a useful tool to identify areas that need more testing but don\u2019t necessarily trust that covered code is truly well tested. Others yet think that measuring coverage is actively harmful because it provides a false sense of security.\n\n\nOur team\u2019s mission was to collect coverage related data then develop and champion code coverage practices across Google. We designed an opt-in system where engineers could enable two different types of coverage measurements for their projects: daily and per-commit. With daily coverage, we run all tests for their project, where as with per-commit coverage we run only the tests affected by the commit. The two measurements are independent and many projects opted into both.\n\n\nWhile we did experiment with branch, function and statement coverage, we ended up focusing mostly on statement coverage because of its relative simplicity and ease of visualization.\n\n\nHow we measured\n\n\nOur job was made significantly easier by the wonderful Google build system whose parallelism and flexibility allowed us to simply scale our measurements to Google scale. The build system had integrated various language-specific open source coverage measurement tools like Gcov (C++), Emma / JaCoCo (Java) and Coverage.py (Python), and we provided a central system where teams could sign up for coverage measurement.\n\nFor daily whole project coverage measurements, each team was provided with a simple cronjob that would run all tests across the project\u2019s codebase. The results of these runs were available to the teams in a centralized dashboard that displays charts showing coverage over time and allows daily / weekly / quarterly / yearly aggregations and per-language slicing. On this dashboard teams can also compare their project (or projects) with any other project, or Google as a whole.\n\n\nFor per-commit measurement, we hook into the Google code review process (briefly explained in this article) and display the data visually to both the commit author and the reviewers. We display the data on two levels: color coded lines right next to the color coded diff and a total aggregate number for the entire commit. \n\n\n\n\n\nDisplayed above is a screenshot of the code review tool. The green line coloring is the standard diff coloring for added lines. The orange and lighter green coloring on the line numbers is the coverage information. We use light green for covered lines, orange for non-covered lines and white for non-instrumented lines.\n\n\nIt\u2019s important to note that we surface the coverage information before the commit is submitted to the codebase, because this is the time when engineers are most likely to be interested in improving it.\n\n\nResults\n\nOne of the main benefits of working at Google is the scale at which we operate. We have been running the coverage measurement system for some time now and we have collected data for more than 650 different projects, spanning 100,000+ commits. We have a significant amount of data for C++, Java, Python, Go and JavaScript code.\n\n\nI am happy to say that we can share some preliminary results with you today:\n\n\n\n\n\nThe chart above is the histogram of average values of measured absolute coverage across Google. The median (50th percentile) code coverage is 78%, the 75th percentile 85% and 90th percentile 90%. We believe that these numbers represent a very healthy codebase.\n\n\nWe have also found it very interesting that there are significant differences between languages: \n\n\n\n\n\nC++\nJava\nGo\nJavaScript\nPython\n\n\n56.6%\n61.2%\n63.0%\n76.9%\n84.2%\n\n\n\n\nThe table above shows the total coverage of all analyzed code for each language, averaged over the past quarter. We believe that the large difference is due to structural, paradigm and best practice differences between languages and the more precise ability to measure coverage in certain languages.\n\n\nNote that these numbers should not be interpreted as guidelines for a particular language, the aggregation method used is too simple for that. Instead this finding is simply a data point for any future research that analyzes samples from a single programming language.\n\n\nThe feedback from our fellow engineers was overwhelmingly positive. The most loved feature was surfacing the coverage information during code review time. This early surfacing of coverage had a statistically significant impact: our initial analysis suggests that it increased coverage by 10% (averaged across all commits).\n\n\nFuture work\n\nWe are aware that there are a few problems with the dataset we collected. In particular, the individual tools we use to measure coverage are not perfect. Large integration tests, end to end tests and UI tests are difficult to instrument, so large parts of code exercised by such tests can be misreported as non-covered.\n\n\nWe are working on improving the tools, but also analyzing the impact of unit tests, integration tests and other types of tests individually. \n\n\nIn addition to languages, we will also investigate other factors that might influence coverage, such as platforms and frameworks, to allow all future research to account for their effect.\n\n\nWe will be publishing more of our findings in the future, so stay tuned.\n\n\nAnd if this sounds like something you would like to work on, why not apply on our job site?", "by Dmitry Vyukov, Synchronization Lookout, Google, Moscow\n\nHello,\n\n\nI work in the Dynamic Testing Tools team at Google. Our team develops tools like AddressSanitizer, MemorySanitizer and ThreadSanitizer which find various kinds of bugs. In this blog post I want to tell you about ThreadSanitizer, a fast data race detector for C++ and Go programs.\n\n\nFirst of all, what is a data race? A data race occurs when two threads access the same variable concurrently, and at least one of the accesses attempts is a write. Most programming languages provide very weak guarantees, or no guarantees at all, for programs with data races. For example, in C++ absolutely any data race renders the behavior of the whole program as completely undefined (yes, it can suddenly format the hard drive). Data races are common in concurrent programs, and they are notoriously hard to debug and localize. A typical manifestation of a data race is when a program occasionally crashes with obscure symptoms, the symptoms are different each time and do not point to any particular place in the source code. Such bugs can take several months of debugging without particular success, since typical debugging techniques do not work. Fortunately, ThreadSanitizer can catch most data races in the blink of an eye. See Chromium issue 15577 for an example of such a data race and issue 18488 for the resolution.\n\n\nDue to the complex nature of bugs caught by ThreadSanitizer, we don't suggest waiting until product release validation to use the tool. For example, in Google, we've made our tools easily accessible to programmers during development, so that anyone can use the tool for testing if they suspect that new code might introduce a race. For both Chromium and Google internal server codebase, we run unit tests that use the tool continuously. This catches many regressions instantly. The Chromium project has recently started using ThreadSanitizer on ClusterFuzz, a large scale fuzzing system. Finally, some teams also set up periodic end-to-end testing with ThreadSanitizer under a realistic workload, which proves to be extremely valuable. When races are found by the tool, our team has zero tolerance for races and does not consider any race to be benign, as even the most benign races can lead to memory corruption.\n\n\nOur tools are dynamic (as opposed to static tools). This means that they do not merely \"look\" at the code and try to surmise where bugs can be; instead they they instrument the binary at build time and then analyze dynamic behavior of the program to catch it red-handed. This approach has its pros and cons. On one hand, the tool does not have any false positives, thus it does not bother a developer with something that is not a bug. On the other hand, in order to catch a bug, the test must expose a bug -- the racing data access attempts must be executed in different threads. This requires writing good multi-threaded tests and makes end-to-end testing especially effective.\n\n\nAs a bonus, ThreadSanitizer finds some other types of bugs: thread leaks, deadlocks, incorrect uses of mutexes, malloc calls in signal handlers, and more. It also natively understands atomic operations and thus can find bugs in lock-free algorithms (see e.g. this bug in the V8 concurrent garbage collector).\n\n\nThe tool is supported by both Clang and GCC compilers (only on Linux/Intel64). Using it is very simple: you just need to add a -fsanitize=thread flag during compilation and linking. For Go programs, you simply need to add a -race flag to the go tool (supported on Linux, Mac and Windows).\n\n\nInterestingly, after integrating the tool into compilers, we've found some bugs in the compilers themselves. For example, LLVM was illegally widening stores, which can introduce very harmful data races into otherwise correct programs. And GCC was injecting unsafe code for initialization of function static variables. Among our other trophies are more than a thousand bugs in Chromium, Firefox, the Go standard library, WebRTC, OpenSSL, and of course in our internal projects.\n\n\nSo what are you waiting for? You know what to do!", "Posted by Anthony Vallone on behalf of the GTAC Committee\n\nThe application process is now open for presentation proposals and attendance for GTAC (Google Test Automation Conference) (see initial announcement) to be held at the Google Kirkland office (near Seattle, WA) on October 28 - 29th, 2014.\n\nGTAC will be streamed live on YouTube again this year, so even if you can\u2019t attend, you\u2019ll be able to watch the conference from your computer.\n\nSpeakers\nPresentations are targeted at student, academic, and experienced engineers working on test automation. Full presentations and lightning talks are 45 minutes and 15 minutes respectively. Speakers should be prepared for a question and answer session following their presentation.\n\nApplication\nFor presentation proposals and/or attendance, complete this form. We will be selecting about 300 applicants for the event.\n\nDeadline\nThe due date for both presentation and attendance applications is July 28, 2014.\n\nFees\nThere are no registration fees, and we will send out detailed registration instructions to each invited applicant. Meals will be provided, but speakers and attendees must arrange and pay for their own travel and accommodations.\n\n\nUpdate : Our contact email was bouncing - this is now fixed.", "Posted by Anthony Vallone on behalf of the GTAC Committee\n\nIf you're looking for a place to discuss the latest innovations in test automation, then charge your tablets and pack your gumboots - the  eighth GTAC (Google Test Automation Conference) will be held on October 28-29, 2014 at Google Kirkland! The Kirkland office is part of the Seattle/Kirkland campus in beautiful Washington state. This campus forms our third largest engineering office in the USA.\n\n\n\n\n\nGTAC is a periodic conference hosted by Google, bringing together engineers from industry and academia to discuss advances in test automation and the test engineering computer science field. It\u2019s a great opportunity to present, learn, and challenge modern testing technologies and strategies. \n\n\nYou can browse the presentation abstracts, slides, and videos from last year on the GTAC 2013 page.\n\n\nStay tuned to this blog and the GTAC website for application information and opportunities to present at GTAC. Subscribing to this blog is the best way to get notified. We're looking forward to seeing you there!", "by Peter Arrenbrecht\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nWe are all conditioned to write tests as we code: unit, functional, UI\u2014the whole shebang. We are professionals, after all. Many of us like how small tests let us work quickly, and how larger tests inspire safety and closure. Or we may just anticipate flak during review. We are so used to these tests that often we no longer question why we write them. This can be wasteful and dangerous.\n\n\nTests are a means to an end: To reduce the key risks of a project, and to get the biggest bang for the buck. This bang may not always come from the tests that standard practice has you write, or not even from tests at all.\n\n\nTwo examples:\n\n\n\n\u201cWe built a new debugging aid. We wrote unit, integration, and UI tests. We were ready to launch.\u201d\n\nOutstanding practice. Missing the mark.\n\nOur key risks were that we'd corrupt our data or bring down our servers for the sake of a debugging aid. None of the tests addressed this, but they gave a false sense of safety and \u201cbeing done\u201d.\nWe stopped the launch.\n\n\n\n\u201cWe wanted to turn down a feature, so we needed to alert affected users. Again we had unit and  integration tests, and even one expensive end-to-end test.\u201d\n\nStandard practice. Wasted effort.\n\nThe alert was so critical it actually needed end-to-end coverage for all scenarios. But it would be live for only three releases. The cheapest effective test? Manual testing before each release.\n\n\nA Better Approach: Risks First\n\n\nFor every project or feature, think about testing. Brainstorm your key risks and your best options to reduce them. Do this at the start so you don't waste effort and can adapt your design. Write them down as a QA design so you can point to it in reviews and discussions.\n\n\nTo be sure, standard practice remains a good idea in most cases (hence it\u2019s standard). Small tests are cheap and speed up coding and maintenance, and larger tests safeguard core use-cases and integration.\n\n\nJust remember: Your tests are a means. The bang is what counts. It\u2019s your job to maximize it.", "by Rich Martin, Zurich\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\n\nWhether we are writing an individual unit test or designing a product\u2019s entire testing process, it is important to take a step back and think about how effective are our tests at detecting and reporting bugs in our code. To be effective, there are three important qualities that every test should try to maximize:\n\n\nFidelity\n\n\nWhen the code under test is broken, the test fails. A high\u00ad-fidelity test is one which is very sensitive to defects in the code under test, helping to prevent bugs from creeping into the code.\n\n\nMaximize fidelity by ensuring that your tests cover all the paths through your code and include all relevant assertions on the expected state.\n\n\nResilience\n\n\nA test shouldn\u2019t fail if the code under test isn\u2019t defective. A resilient test is one that only fails when a breaking change is made to the code under test. Refactorings and other non-\u00adbreaking changes to the code under test can be made without needing to modify the test, reducing the cost of maintaining the tests.\n\n\nMaximize resilience by only testing the exposed API of the code under test\u037e avoid reaching into internals. Favor stubs and fakes over mocks\u037e don't verify interactions with dependencies unless it is that interaction that you are explicitly validating. A flaky test obviously has very low resilience.\n\n\nPrecision\n\n\nWhen a test fails, a high\u00ad-precision test tells you exactly where the defect lies. A well\u00ad-written unit test can tell you exactly which line of code is at fault. Poorly written tests (especially large end-to-end tests) often exhibit very low precision, telling you that something is broken but not where.\n\n\nMaximize precision by keeping your tests small and tightly \u00adfocused. Choose descriptive method names that convey exactly what the test is validating. For system integration tests, validate state at every boundary.\n\n\nThese three qualities are often in tension with each other. It's easy to write a highly resilient test (the empty test, for example), but writing a test that is both highly resilient and high\u00ad-fidelity is hard. As you design and write tests, use these qualities as a framework to guide your implementation.", "by Erik Kuefler\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nAfter writing a method, it's easy to write just one test that verifies everything the method does. But it can be harmful to think that tests and public methods should have a 1:1 relationship. What we really want to test are behaviors, where a single method can exhibit many behaviors, and a single behavior sometimes spans across multiple methods.\n\n\nLet's take a look at a bad test that verifies an entire method:\n\n\n@Test public void testProcessTransaction() {\n  User user = newUserWithBalance(LOW_BALANCE_THRESHOLD.plus(dollars(2));\n  transactionProcessor.processTransaction(\n      user,\n      new Transaction(\"Pile of Beanie Babies\", dollars(3)));\n  assertContains(\"You bought a Pile of Beanie Babies\", ui.getText());\n  assertEquals(1, user.getEmails().size());\n  assertEquals(\"Your balance is low\", user.getEmails().get(0).getSubject());\n}\n\n\nDisplaying the name of the purchased item and sending an email about the balance being low are two separate behaviors, but this test looks at both of those behaviors together just because they happen to be triggered by the same method. Tests like this very often become massive and difficult to maintain over time as additional behaviors keep getting added in\u2014eventually it will be very hard to tell which parts of the input are responsible for which assertions. The fact that the test's name is a direct mirror of the method's name is a bad sign. \n\n\nIt's a much better idea to use separate tests to verify separate behaviors:\n\n\n@Test public void testProcessTransaction_displaysNotification() {\n  transactionProcessor.processTransaction(\n      new User(), new Transaction(\"Pile of Beanie Babies\"));\n  assertContains(\"You bought a Pile of Beanie Babies\", ui.getText());\n}\n@Test public void testProcessTransaction_sendsEmailWhenBalanceIsLow() {\n  User user = newUserWithBalance(LOW_BALANCE_THRESHOLD.plus(dollars(2));\n  transactionProcessor.processTransaction(\n      user,\n      new Transaction(dollars(3)));\n  assertEquals(1, user.getEmails().size());\n  assertEquals(\"Your balance is low\", user.getEmails().get(0).getSubject());\n}\n\n\nNow, when someone adds a new behavior, they will write a new test for that behavior. Each test will remain focused and easy to understand, no matter how many behaviors are added. This will make your tests more resilient since adding new behaviors is unlikely to break the existing tests, and clearer since each test contains code to exercise only one behavior.", "Update: APRIL FOOLS!\n\n\n\nby Kaue Silveira\n\n\nHere at Google, we invest heavily in development productivity research. In fact, our TDD research group now occupies nearly an entire building of the Googleplex. The group has been working hard to minimize the development cycle time, and we\u2019d like to share some of the amazing progress they\u2019ve made.\n\n\nThe Concept\n\n\nIn the ways of old, it used to be that people wrote tests for their existing code. This was changed by TDD (Test-driven Development), where one would write the test first and then write the code to satisfy it. The TDD research group didn\u2019t think this was enough and wanted to elevate the humble test to the next level. We are pleased to announce the Real TDD, our latest innovation in the Program Synthesis field, where you write only the tests and have the computer write the code for you!\n\nThe following graph shows how the number of tests created by a small feature team grew since they started using this tool towards the end of 2013. Over the last 2 quarters, more than 89% of this team\u2019s production code was written by the tool!\n\n\n\n\nSee it in action:\n\n\nTest written by a Software Engineer:\n\n\nclass LinkGeneratorTest(googletest.TestCase):\n\n  def setUp(self):\n    self.generator = link_generator.LinkGenerator()\n\n  def testGetLinkFromIDs(self):\n    expected = ('https://frontend.google.com/advancedSearchResults?'\n                's.op=ALL&s.r0.field=ID&s.r0.val=1288585+1310696+1346270+')\n    actual = self.generator.GetLinkFromIDs(set((1346270, 1310696, 1288585)))\n    self.assertEqual(expected, actual)\n\nCode created by our tool:\n\n\nimport urllib\n\nclass LinkGenerator(object):\n\n  _URL = (\n      'https://frontend.google.com/advancedSearchResults?'\n      's.op=ALL&s.r0.field=ID&s.r0.val=')\n\n  def GetLinkFromIDs(self, ids):\n    result = []\n    for id in sorted(ids):\n      result.append('%s ' % id)\n    return self._URL + urllib.quote_plus(''.join(result))\n\n\nNote that the tool is smart enough to not generate the obvious implementation of returning a constant string, but instead it correctly abstracts and generalizes the relation between inputs and outputs. It becomes smarter at every use and it\u2019s behaving more and more like a human programmer every day. We once saw a comment in the generated code that said \"I need some coffee\".\n\n\nHow does it work?\n\n\nWe\u2019ve trained the Google Brain with billions of lines of open-source software to learn about coding patterns and how product code correlates with test code. Its accuracy is further improved by using  Type Inference to infer types from code and the Girard-Reynolds Isomorphism to infer code from types.\n\n\nThe tool runs every time your unit test is saved, and it uses the learned model to guide a backtracking search for a code snippet that satisfies all assertions in the test. It provides sub-second responses for 99.5% of the cases (as shown in the following graph), thanks to millions of pre-computed assertion-snippet pairs stored in Spanner for global low-latency access.\n\n\n\n\n\n\nHow can I use it?\n\n\nWe will offer a free (rate-limited) service that everyone can use, once we have sorted out the legal issues regarding the possibility of mixing code snippets originating from open-source projects with different licenses (e.g., GPL-licensed tests will simply refuse to pass BSD-licensed code snippets). If you would like to try our alpha release before the public launch, leave us a comment!", "by Erik Kuefler\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nUnit tests are important tools for verifying that our code is correct. But writing good tests is about much more than just verifying correctness \u2014 a good unit test should exhibit several other properties in order to be readable and maintainable.\n\n\nOne property of a good test is clarity. Clarity means that a test should serve as readable documentation for humans, describing the code being tested in terms of its public APIs. Tests shouldn't refer directly to implementation details. The names of a class's tests should say everything the class does, and the tests themselves should serve as examples for how to use the class.\n\n\nTwo more important properties are completeness and conciseness. A test is complete when its body contains all of the information you need to understand it, and concise when it doesn't contain any other distracting information. This test fails on both counts:\n\n\n@Test public void shouldPerformAddition() {\n  Calculator calculator = new Calculator(new RoundingStrategy(), \n      \"unused\", ENABLE_COSIN_FEATURE, 0.01, calculusEngine, false);\n  int result = calculator.doComputation(makeTestComputation());\n  assertEquals(5, result); // Where did this number come from?\n}\n\nLots of distracting information is being passed to the constructor, and the important parts are hidden off in a helper method. The test can be made more complete by clarifying the purpose of the helper method, and more concise by using another helper to hide the irrelevant details of constructing the calculator:\n\n\n@Test public void shouldPerformAddition() {\n  Calculator calculator = newCalculator();\n  int result = calculator.doComputation(makeAdditionComputation(2, 3));\n  assertEquals(5, result);\n}\n\nOne final property of a good test is resilience. Once written, a resilient test doesn't have to change unless the purpose or behavior of the class being tested changes. Adding new behavior should only require adding new tests, not changing old ones. The original test above isn't resilient since you'll have to update it (and probably dozens of other tests!) whenever you add a new irrelevant constructor parameter. Moving these details into the helper method solved this problem.", "by Hongfei Ding, Software Engineer, Shanghai\n\n\nMockito is a popular open source Java testing framework that allows the creation of mock objects. For example, we have the below interface used in our SUT (System Under Test):\ninterface Service {\n  Data get();\n}\n\nIn our test, normally we want to fake the Service\u2019s behavior to return canned data, so that the unit test can focus on testing the code that interacts with the Service. We use when-return clause to stub a method.\nwhen(service.get()).thenReturn(cannedData);\n\nBut sometimes you need mock object behavior that's too complex for when-return. An Answer object can be a clean way to do this once you get the syntax right.\n\nA common usage of Answer is to stub asynchronous methods that have callbacks. For example, we have mocked the interface below:\ninterface Service {\n  void get(Callback callback);\n}\n\nHere you\u2019ll find that when-return is not that helpful anymore. Answer is the replacement. For example, we can emulate a success by calling the onSuccess function of the callback.\ndoAnswer(new Answer<Void>() {\n    public Void answer(InvocationOnMock invocation) {\n       Callback callback = (Callback) invocation.getArguments()[0];\n       callback.onSuccess(cannedData);\n       return null;\n    }\n}).when(service).get(any(Callback.class));\n\nAnswer can also be used to make smarter stubs for synchronous methods. Smarter here means the stub can return a value depending on the input, rather than canned data. It\u2019s sometimes quite useful. For example, we have mocked the Translator\u00a0interface below:\ninterface Translator {\n  String translate(String msg);\n}\n\nWe might choose to mock Translator to return a constant string and then assert the result. However, that test is not thorough, because the input to the translator\u00a0function has been ignored. To improve this, we might capture the input and do extra verification, but then we start to fall into the \u201ctesting interaction rather than testing state\u201d trap. \n\n\nA good usage of Answer is to reverse the input message as a fake translation. So that both things are assured by checking the result string: 1) translate has been invoked, 2) the msg being translated is correct. Notice that this time we\u2019ve used thenAnswer syntax, a twin of doAnswer, for stubbing a non-void method.\nwhen(translator.translate(any(String.class))).thenAnswer(reverseMsg())\n...\n// extracted a method to put a descriptive name\nprivate static Answer<String> reverseMsg() { \n  return new Answer<String>() {\n    public String answer(InvocationOnMock invocation) {\n       return reverseString((String) invocation.getArguments()[0]));\n    }\n  }\n}\n\nLast but not least, if you find yourself writing many nontrivial Answers, you should consider using a fake instead.", "by Anthony Vallone\n\nUnreproducible bugs are the bane of my existence. Far too often, I find a bug, report it, and hear back that it\u2019s not a bug because it can\u2019t be reproduced. Of course, the bug is still there, waiting to prey on its next victim. These types of bugs can be very expensive due to increased investigation time and overall lifetime. They can also have a damaging effect on product perception when users reporting these bugs are effectively ignored. We should be doing more to prevent them. In this article, I\u2019ll go over some obvious, and maybe not so obvious, development/testing guidelines that can reduce the likelihood of these bugs from occurring.\n\n\n\nAvoid and test for race conditions, deadlocks, timing issues, memory corruption, uninitialized memory access, memory leaks, and resource issues\n\n\nI am lumping together many bug types in this section, but they are all related somewhat by how we test for them and how disproportionately hard they are to reproduce and debug. The root cause and effect can be separated by milliseconds or hours, and stack traces might be nonexistent or misleading. A system may fail in strange ways when exposed to unusual traffic spikes or insufficient resources. Race conditions and deadlocks may only be discovered during unique traffic patterns or resource configurations. Timing issues may only be noticed when many components are integrated and their performance parameters and failure/retry/timeout delays create a chaotic system. Memory corruption or uninitialized memory access may go unnoticed for a large percentage of calls but become fatal for rare states. Memory leaks may be negligible unless the system is exposed to load for an extended period of time.\n\n\nGuidelines for development:\n\n\n\n\nSimplify your synchronization logic. If it\u2019s too hard to understand, it will be difficult to reproduce and debug complex concurrency problems.\n\n\nAlways obtain locks in the same order. This is a tried-and-true guideline to avoid deadlocks, but I still see code that breaks it periodically. Define an order for obtaining multiple locks and never change that order. \n\n\nDon\u2019t optimize by creating many fine-grained locks, unless you have verified that they are needed. Extra locks increase concurrency complexity.\n\n\nAvoid shared memory, unless you truly need it. Shared memory access is very easy to get wrong, and the bugs may be quite difficult to reproduce.\n\n\n\nGuidelines for testing:\n\n\n\nStress test your system regularly. You don't want to be surprised by unexpected failures when your system is under heavy load. \n\n\nTest timeouts. Create tests that mock/fake dependencies to test timeout code. If your timeout code does something bad, it may cause a bug that only occurs under certain system conditions.\n\n\nTest with debug and optimized builds. You may find that a well behaved debug build works fine, but the system fails in strange ways once optimized.\n\n\nTest under constrained resources. Try reducing the number of data centers, machines, processes, threads, available disk space, or available memory. Also try simulating reduced network bandwidth.\n\n\nTest for longevity. Some bugs require a long period of time to reveal themselves. For example, persistent data may become corrupt over time.\n\n\nUse dynamic analysis tools like memory debuggers, ASan, TSan, and MSan regularly. They can help identify many categories of unreproducible memory/threading issues.\n\n\n\n\nEnforce preconditions\n\n\nI\u2019ve seen many well-meaning functions with a high tolerance for bad input. For example, consider this function:\n\n\nvoid ScheduleEvent(int timeDurationMilliseconds) {\n  if (timeDurationMilliseconds <= 0) {\n    timeDurationMilliseconds = 1;\n  }\n  ...\n}\n\n\nThis function is trying to help the calling code by adjusting the input to an acceptable value, but it may be doing damage by masking a bug. The calling code may be experiencing any number of problems described in this article, and passing garbage to this function will always work fine. The more functions that are written with this level of tolerance, the harder it is to trace back to the root cause, and the more likely it becomes that the end user will see garbage. Enforcing preconditions, for instance by using asserts, may actually cause a higher number of failures for new systems, but as systems mature, and many minor/major problems are identified early on, these checks can help improve long-term reliability.\n\n\nGuidelines for development:\n\n\n\n\nEnforce preconditions in your functions unless you have a good reason not to.\n\n\n\n\nUse defensive programming\n\n\nDefensive programming is another tried-and-true technique that is great at minimizing unreproducible bugs. If your code calls a dependency to do something, and that dependency quietly fails or returns garbage, how does your code handle it? You could test for situations like this via mocking or faking, but it\u2019s even better to have your production code do sanity checking on its dependencies. For example:\n\n\ndouble GetMonthlyLoanPayment() {\n  double rate = GetTodaysInterestRateFromExternalSystem();\n  if (rate < 0.001 || rate > 0.5) {\n    throw BadInterestRate(rate);\n  }\n  ...\n}\n\n\nGuidelines for development:\n\n\n\n\nWhen possible, use defensive programming to verify the work of your dependencies with known risks of failure like user-provided data, I/O operations, and RPC calls.\n\n\n\nGuidelines for testing:\n\n\n\n\nUse fuzz testing to test your systems hardiness when enduring bad data.\n\n\n\n\nDon\u2019t hide all errors from the user\n\n\nThere has been a trend in recent years toward hiding failures from users at all costs. In many cases, it makes perfect sense, but in some, we have gone overboard. Code that is very quiet and permissive during minor failures will allow an uninformed user to continue working in a failed state. The software may ultimately reach a fatal tipping point, and all the error conditions that led to failure have been ignored. If the user doesn\u2019t know about the prior errors, they will not be able to report them, and you may not be able to reproduce them.\n\n\nGuidelines for development:\n\n\n\n\nOnly hide errors from the user when you are certain that there is no impact to system state or the user.\n\n\nAny error with impact to the user should be reported to the user with instructions for how to proceed. The information shown to the user, combined with data available to an engineer, should be enough to determine what went wrong.\n\n\n\n\nTest error handling\n\n\nThe most common sections of code to remain untested is error handling code. Don\u2019t skip test coverage here. Bad error handling code can cause unreproducible bugs and create great risk if it does not handle fatal errors well.\n\n\nGuidelines for testing:\n\n\n\n\nAlways test your error handling code. This is usually best accomplished by mocking or faking the component triggering the error.\n\n\nIt\u2019s also a good practice to examine your log quality for all types of error handling. \n\n\n\n\nCheck for duplicate keys\n\n\nIf unique identifiers or data access keys are generated using random data or are not guaranteed to be globally unique, duplicate keys may cause data corruption or concurrency issues. Key duplication bugs are very difficult to reproduce.\n\n\nGuidelines for development:\n\n\n\n\nTry to guarantee uniqueness of all keys.\n\n\nWhen not possible to guarantee unique keys, check if the recently generated key is already in use before using it.\n\n\nWatch out for potential race conditions here and avoid them with synchronization.\n\n\n\n\nTest for concurrent data access\n\n\nSome bugs only reveal themselves when multiple clients are reading/writing the same data. Your stress tests might be covering cases like these, but if they are not, you should have special tests for concurrent data access. Case like these are often unreproducible. For example, a user may have two instances of your app running against the same account, and they may not realize this when reporting a bug.\n\n\nGuidelines for testing:\n\n\n\n\nAlways test for concurrent data access if it\u2019s a feature of the system. Actually, even if it\u2019s not a feature, verify that the system rejects it. Testing concurrency can be challenging. An approach that usually works for me is to create many worker threads that simultaneously attempt access and a master thread that monitors and verifies that some number of attempts were indeed concurrent, blocked or allowed as expected, and all were successful. Programmatic post-analysis of all attempts and changing system state may also be necessary to ensure that the system behaved well.\n\n\n\n\nSteer clear of undefined behavior and non-deterministic access to data\n\n\nSome APIs and basic operations have warnings about undefined behavior when in certain states or provided with certain input. Similarly, some data structures do not guarantee an iteration order (example: Java\u2019s Set). Code that ignores these warnings may work fine most of the time but fail in unusual ways that are hard to reproduce.\n\n\n\nGuidelines for development:\n\n\n\n\nUnderstand when the APIs and operations you use might have undefined behavior and prevent those conditions.\n\n\nDo not depend on data structure iteration order unless it is guaranteed. It is a common mistake to depend on the ordering of sets or associative arrays.\n\n\n\n\nLog the details for errors or test failures\n\n\nIssues described in this article can be easier to reproduce and debug when the logs contain enough detail to understand the conditions that led to an error.\n\n\nGuidelines for development:\n\n\n\n\nFollow good logging practices, especially in your error handling code.\n\n\nIf logs are stored on a user\u2019s machine, create an easy way for them to provide you the logs.\n\n\n\nGuidelines for testing:\n\n\n\n\nSave your test logs for potential analysis later.\n\n\n\n\nAnything to add?\n\n\nHave I missed any important guidelines for minimizing these bugs? What is your favorite hard-to-reproduce bug that you discovered and resolved?", "by Anthony Vallone\n\nThis is the third in a series of articles about our work environment. See the first\u00a0and second.\n\nI will never forget the awe I felt when running my first load test on my first project at Google. At previous companies I\u2019ve worked, running a substantial load test took quite a bit of resource planning and preparation. At Google, I wrote less than 100 lines of code and was simulating tens of thousands of users after just minutes of prep work. The ease with which I was able to accomplish this is due to the impressive coding, building, and testing tools available at Google. In this article, I will discuss these tools and how they affect our test and development process.\n\n\nCoding and building\n\n\nThe tools and process for coding and building make it very easy to change production and test code. Even though we are a large company, we have managed to remain nimble. In a matter of minutes or hours, you can edit, test, review, and submit code to head. We have achieved this without sacrificing code quality by heavily investing in tools, testing, and infrastructure, and by prioritizing code reviews.\n\n\nMost production and test code is in a single, company-wide source control repository (open source projects like Chromium and Android have their own). There is a great deal of code sharing in the codebase, and this provides an incredible suite of code to build on. Most code is also in a single branch, so the majority of development is done at head. All code is also navigable, searchable, and editable from the browser. You\u2019ll find code in numerous languages, but Java, C++, Python, Go, and JavaScript are the most common.\n\n\nHave a strong preference for editor? Engineers are free to choose from many IDEs and editors. The most common are Eclipse, Emacs, Vim, and IntelliJ, but many others are used as well. Engineers that are passionate about their prefered editors have built up and shared some truly impressive editor plugins/tooling over the years.\n\n\nCode reviews for all submissions are enforced via source control tooling. This also applies to test code, as our test code is held to the same standards as production code. The reviews are done via web-based code review tools that even include automatically generated test results. The process is very streamlined and efficient. Engineers can change and submit code in any part of the repository, but it must get reviewed by owners of the code being changed. This is great, because you can easily change code that your team depends on, rather than merely request a change to code you do not own.\n\n\nThe Google build system is used for building most code, and it is designed to work across many languages and platforms. It is remarkably simple to define and build targets. You won\u2019t be needing that old Makefile book.\n\n\nRunning jobs and tests\n\n\nWe have some pretty amazing machine and job management tools at Google. There is a generally available pool of machines in many data centers around the globe. The job management service makes it very easy to start jobs on arbitrary machines in any of these data centers. Failing machines are automatically removed from the pool, so tests rarely fail due to machine issues. With a little effort, you can also set up monitoring and pager alerting for your important jobs.\n\n\nFrom any machine you can spin up a massive number of tests and run them in parallel across many machines in the pool, via a single command. Each of these tests are run in a standard, isolated environment, so we rarely run into the \u201cit works on my machine!\u201d issue.\n\n\nBefore code is submitted, presubmit tests can be run that will find all tests that depend transitively on the change and run them. You can also define presubmit rules that run checks on a code change and verify that tests were run before allowing submission.\n\n\nOnce you\u2019ve submitted test code, the build and test system automatically registers the test, and starts building/testing continuously. If the test starts failing, your team will get notification emails. You can also visit a test dashboard for your team and get details about test runs and test data. Monitoring the build/test status is made even easier with our build orbs designed and built by Googlers. These small devices will glow red if the build starts failing. Many teams have had fun customizing these orbs to various shapes, including a statue of liberty with a glowing torch.\n\n\n\n\n\nStatue of LORBerty\n\n\nRunning larger integration and end-to-end tests takes a little more work, but we have some excellent tools to help with these tests as well: Integration test runners, hermetic environment creation, virtual machine service, web test frameworks, etc.\n\n\nThe impact\n\n\nSo how do these tools actually affect our productivity? For starters, the code is easy to find, edit, review, and submit. Engineers are free to choose tools that make them most productive. Before and after submission, running small tests is trivial, and running large tests is relatively easy. Since tests are easy to create and run, it\u2019s fairly simple to maintain a green build, which most teams do most of the time. This allows us to spend more time on real problems and less on the things that shouldn\u2019t even be problems. It allows us to focus on creating rigorous tests. It dramatically accelerates the development process that can prototype Gmail in a day and code/test/release service features on a daily schedule. And, of course, it lets us focus on the fun stuff.\n\n\nThoughts?\n\n\nWe are interested to hear your thoughts on this topic. Google has the resources to build tools like this, but would small or medium size companies benefit from a similar investment in its infrastructure? Did Google create the infrastructure or did the infrastructure create Google?", "by Anthony Vallone\n\nThis is the second in a series of articles about our work environment. See the first.\n\n\nThere are few things as frustrating as getting hampered in your work by a bug in a product you depend on. What if it\u2019s a product developed by your company? Do you report/fix the issue or just work around it and hope it\u2019ll go away soon? In this article, I\u2019ll cover how and why Google dogfoods its own products.\n\n\nDogfooding\n\n\nGoogle makes heavy use of its own products. We have a large ecosystem of development/office tools and use them for nearly everything we do. Because we use them on a daily basis, we can dogfood releases company-wide before launching to the public. These dogfood versions often have features unavailable to the public but may be less stable. Instability is exactly what you want in your tools, right? Or, would you rather that frustration be passed on to your company\u2019s customers? Of course not!\n\n\nDogfooding is an important part of our test process. Test teams do their best to find problems before dogfooding, but we all know that testing is never perfect. We often get dogfood bug reports for edge and corner cases not initially covered by testing. We also get many comments about overall product quality and usability. This internal feedback has, on many occasions, changed product design.\n\n\nNot surprisingly, test-focused engineers often have a lot to say during the dogfood phase. I don\u2019t think there is a single public-facing product that I have not reported bugs on. I really appreciate the fact that I can provide feedback on so many products before release.\n\n\nInterested in helping to test Google products? Many of our products have feedback links built-in. Some also have Beta releases available. For example, you can start using Chrome Beta and help us file bugs.\n\n\nOffice software\n\n\nFrom system design documents, to test plans, to discussions about beer brewing techniques, our products are used internally. A company\u2019s choice of office tools can have a big impact on productivity, and it is fortunate for Google that we have such a comprehensive suite. The tools have a consistently simple UI (no manual required), perform very well, encourage collaboration, and auto-save in the cloud. Now that I am used to these tools, I would certainly have a hard time going back to the tools of previous companies I have worked. I\u2019m sure I would forget to click the save buttons for years to come.\n\n\nExamples of frequently used tools by engineers:\n\n\nGoogle Drive Apps (Docs, Sheets, Slides, etc.) are used for design documents, test plans, project data, data analysis, presentations, and more.\nGmail and Hangouts are used for email and chat.\nGoogle Calendar is used to schedule all meetings, reserve conference rooms, and setup video conferencing using Hangouts.\nGoogle Maps is used to map office floors.\nGoogle Groups are used for email lists.\nGoogle Sites are used to host team pages, engineering docs, and more.\nGoogle App Engine hosts many corporate, development, and test apps.\nChrome is our primary browser on all platforms.\nGoogle+ is used for organizing internal communities on topics such as food or C++, and for socializing.\n\n\nThoughts?\n\n\nWe are interested to hear your thoughts on this topic. Do you dogfood your company\u2019s products? Do your office tools help or hinder your productivity? What office software and tools do you find invaluable for your job? Could you use Google Docs/Sheets for large test plans?\n\n\n\n(Continue to part 3)", "by Anthony Vallone\n\nWhen conducting interviews, I often get questions about our workspace and engineering environment. What IDEs do you use? What programming languages are most common? What kind of tools do you have for testing? What does the workspace look like?\n\n\nGoogle is a company that is constantly pushing to improve itself. Just like software development itself, most environment improvements happen via a bottom-up approach. All engineers are responsible for fine-tuning, experimenting with, and improving our process, with a goal of eliminating barriers to creating products that amaze. \n\n\nOffice space and engineering equipment can have a considerable impact on productivity. I\u2019ll focus on these areas of our work environment in this first article of a series on the topic.\n\n\nOffice layout\n\n\nGoogle is a highly collaborative workplace, so the open floor plan suits our engineering process. Project teams composed of Software Engineers (SWEs), Software Engineers in Test (SETs), and Test Engineers (TEs) all sit near each other or in large rooms together. The test-focused engineers are involved in every step of the development process, so it\u2019s critical for them to sit with the product developers. This keeps the lines of communication open. \n\n\n\n\n\nGoogle Munich\n\n\nThe office space is far from rigid, and teams often rearrange desks to suit their preferences. The facilities team recently finished renovating a new floor in the New York City office, and after a day of engineering debates on optimal arrangements and white board diagrams, the floor was completely transformed.\n\n\nBesides the main office areas, there are lounge areas to which Googlers go for a change of scenery or a little peace and quiet. If you are trying to avoid becoming a casualty of The Great Foam Dart War, lounges are a great place to hide.\n\n\n\n\n\nGoogle Dublin\n\n\nWorking with remote teams\n\n\nGoogle\u2019s worldwide headquarters is in Mountain View, CA, but it\u2019s a very global company, and our project teams are often distributed across multiple sites. To help keep teams well connected, most of our conference rooms have video conferencing equipment. We make frequent use of this equipment for team meetings, presentations, and quick chats.\n\n\n\n\n\nGoogle Boston\n\n\nWhat\u2019s at your desk?\n\n\nAll engineers get high-end machines and have easy access to data center machines for running large tasks. A new member on my team recently mentioned that his Google machine has 16 times the memory of the machine at his previous company.\n\n\nMost Google code runs on Linux, so the majority of development is done on Linux workstations. However, those that work on client code for Windows, OS X, or mobile, develop on relevant OSes. For displays, each engineer has a choice of either two 24 inch monitors or one 30 inch monitor. We also get our choice of laptop, picking from various models of Chromebook, MacBook, or Linux. These come in handy when going to meetings, lounges, or working remotely. \n\n\n\n\n\nGoogle Zurich\n\n\nThoughts?\n\n\nWe are interested to hear your thoughts on this topic. Do you prefer an open-office layout, cubicles, or private offices? Should test teams be embedded with development teams, or should they operate separately? Do the benefits of offering engineers high-end equipment outweigh the costs?\n\n\n\n(Continue to part 2)", "by Patrik H\u00f6glund\n\n\nThe WebRTC project is all about enabling peer-to-peer video, voice and data transfer in the browser. To give our users the best possible experience we need to adapt the quality of the media to the bandwidth and processing power we have available. Our users encounter a wide variety of network conditions and run on a variety of devices, from powerful desktop machines with a wired broadband connection to laptops on WiFi to mobile phones on spotty 3G networks.\n\n\nWe want to ensure good quality for all these use cases in our implementation in Chrome. To some extent we can do this with manual testing, but the breakneck pace of Chrome development makes it very hard to keep up (several hundred patches land every day)! Therefore, we'd like to test the quality of our video and voice transfer with an automated test. Ideally, we\u2019d like to test for the most common network scenarios our users encounter, but to start we chose to implement a test where we have plenty of CPU and bandwidth. This article covers how we built such a test.\n\n\nQuality Metrics\n\nFirst, we must define what we want to measure. For instance, the WebRTC video quality test uses peak signal-to-noise ratio and structural similarity to measure the quality of the video (or to be more precise, how much the output video differs from the input video; see this GTAC 13 talk for more details). The quality of the user experience is a subjective thing though. Arguably, one probably needs dozens of different metrics to really ensure a good user experience. For video, we would have to (at the very least) have some measure for frame rate and resolution besides correctness. To have the system send somewhat correct video frames seemed the most important though, which is why we chose the above metrics.\n\n\nFor this test we wanted to start with a similar correctness metric, but for audio. It turns out there's an algorithm called Perceptual Evaluation of Speech Quality (PESQ) which analyzes two audio files and tell you how similar they are, while taking into account how the human ear works (so it ignores differences a normal person would not hear anyway). That's great, since we want our metrics to measure the user experience as much as possible.  There are many aspects of voice transfer you could measure, such as latency (which is really important for voice calls), but for now we'll focus on measuring how much a voice audio stream gets distorted by the transfer.\n\n\nFeeding Audio Into WebRTC\n\nIn the WebRTC case we already had a test which would launch a Chrome browser, open two tabs, get the tabs talking to each other through a signaling server and set up a call on a single machine. Then we just needed to figure out how to feed a reference audio file into a WebRTC call and record what comes out on the other end. This part was actually harder than it sounds. The main WebRTC use case is that the web page acquires the user's mic through getUserMedia, sets up a PeerConnection with some remote peer and sends the audio from the mic through the connection to the peer where it is played in the peer's audio output device. \n\n\n\n\n\n\n\nWebRTC calls transmit voice, video and data peer-to-peer, over the Internet.\n\n\nBut since this is an automated test, of course we could not have someone speak in a microphone every time the test runs; we had to feed in a known input file, so we had something to compare the recorded output audio against. \n\n\nCould we duct-tape a small stereo to the mic and play our audio file on the stereo? That's not very maintainable or reliable, not to mention annoying for anyone in the vicinity. What about some kind of fake device driver which makes a microphone-like device appear on the device level? The problem with that is that it's hard to control a driver from the userspace test program. Also, the test will be more complex and flaky, and the driver interaction will not be portable.[1]\n\n\nInstead, we chose to sidestep this problem. We used a solution where we load an audio file with WebAudio and play that straight into the peer connection through the WebAudio-PeerConnection integration. That way we start the playing of the file from the same renderer process as the call itself, which made it a lot easier to time the start and end of the file. We still needed to be careful to avoid playing the file too early or too late, so we don't clip the audio at the start or end - that would destroy our PESQ scores! - but it turned out to be a workable approach.[2]\n\n\nRecording the Output\n\nAlright, so now we could get a WebRTC call set up with a known audio file with decent control of when the file starts playing. Now we had to record the output. There are a number of possible solutions. The most end-to-end way is to straight up record what the system sends to default audio out (like speakers or headphones). Alternatively, we could write a hook in our application to dump our audio as late as possible, like when we're just about to send it to the sound card.\n\n\nWe went with the former. Our colleagues in the Chrome video stack team in Kirkland had already found  that it's possible to configure a Windows or Linux machine to send the system's audio output (i.e. what plays on the speakers) to a virtual recording device. If we make that virtual recording device the default one, simply invoking SoundRecorder.exe and arecord respectively will record what the system is playing out. \n\n\nThey found this works well if one also uses the sox utility to eliminate silence around the actual audio content (recall we had some safety margins at both ends to ensure we record the whole input file as playing through the WebRTC call). We adopted the same approach, since it records what the user would hear, and yet uses only standard tools. This means we don't have to install additional software on the myriad machines that will run this test.[3]\n\n\nAnalyzing Audio\n\nThe only remaining step was to compare the silence-eliminated recording with the input file. When we first did this, we got a really bad score (like 2.0 out of 5.0, which means PESQ thinks it\u2019s barely legible). This didn't seem to make sense, since both the input and recording sounded very similar. Turns out we didn\u2019t think about the following:\n\n\n\n\nWe were comparing a full-band (24 kHz) input file to a wide-band (8 kHz) result (although both files were sampled at 48 kHz). This essentially amounted to a low pass filtering of the result file.\n\n\nBoth files were in stereo, but PESQ is only mono-aware.\n\n\nThe files were 32-bit, but the PESQ implementation is designed for 16 bits.\n\n\n\nAs you can see, it\u2019s important to pay attention to what format arecord and SoundRecorder.exe records in, and make sure the input file is recorded in the same way. After correcting the input file and \u201crebasing\u201d, we got the score up to about 4.0.[4]\n\n\nThus, we ended up with an automated test that runs continously on the torrent of Chrome change lists and protects WebRTC's ability to transmit sound. You can see the finished code here. With automated tests and cleverly chosen metrics you can protect against most regressions a user would notice. If your product includes video and audio handling, such a test is a great addition to your testing mix. \n\n\n\n\n\n\nHow the components of the test fit together.\n\n\nFuture work\n\n\n\n\nIt might be possible to write a Chrome extension which dumps the audio from Chrome to a file. That way we get a simpler-to-maintain and portable solution. It would be less end-to-end but more than worth it due to the simplified maintenance and setup. Also, the recording tools we use are not perfect and add some distortion, which makes the score less accurate.\n\n\nThere are other algorithms than PESQ to consider - for instance, POLQA is the successor to PESQ and is better at analyzing high-bandwidth audio signals.\n\n\nWe are working on a solution which will run this test under simulated network conditions. Simulated networks combined with this test is a really powerful way to test our behavior under various packet loss and delay scenarios and ensure we deliver a good experience to all our users, not just those with great broadband connections. Stay tuned for future articles on that topic!\n\n\nInvestigate feasibility of running this set-up on mobile devices.\n\n\n\n\n\n\n1\nIt would be tolerable if the driver was just looping the input file, eliminating the need for the test to control the driver (i.e. the test doesn't have to tell the driver to start playing the file). This is actually what we do in the video quality test. It's a much better fit to take this approach on the video side since each recorded video frame is independent of the others. We can easily embed barcodes into each frame and evaluate them independently. \n\n\nThis seems much harder for audio. We could possibly do audio watermarking, or we could embed a kind of start marker (for instance, using DTMF tones) in the first two seconds of the input file and play the real content after that, and then do some fancy audio processing on the receiving end to figure out the start and end of the input audio. We chose not to pursue this approach due to its complexity.\n\n\n2\nUnfortunately, this also means we will not test the capturer path (which handles microphones, etc in WebRTC). This is an example of the frequent tradeoffs one has to do when designing an end-to-end test. Often we have to trade end-to-endness (how close the test is to the user experience) with robustness and simplicity of a test. It's not worth it to cover 5% more of the code if the test become unreliable or radically more expensive to maintain. Another example: A WebRTC call will generally involve two peers on different devices separated by the real-world internet. Writing such a test and making it reliable would be extremely difficult, so we make the test single-machine and hope we catch most of the bugs anyway.\n\n\n3\nIt's important to keep the continuous build setup simple and the build machines easy to configure - otherwise you will inevitably pay a heavy price in maintenance when you try to scale your testing up.\n\n\n4\nWhen sending audio over the internet, we have to compress it since lossless audio consumes way too much bandwidth. WebRTC audio generally sounds great, but there's still compression artifacts if you listen closely (and, in fact, the recording tools are not perfect and add some distorsion as well). Given that this test is more about detecting regressions than measuring some absolute notion of quality, we'd like to downplay those artifacts. As our Kirkland colleagues found, one of the ways to do that is to \"rebase\" the input file. That means we start with a pristine recording, feed that through the WebRTC call and record what comes out on the other end. After manually verifying the quality, we use that as our input file for the actual test. In our case, it pushed our PESQ score up from 3 to about 4 (out of 5), which gives us a bit more sensitivity to regressions.", "Cross-posted from the Android Developers Google+ Page\n\nEarlier this year, we presented Espresso at GTAC as a solution to the UI testing problem. Today we are announcing the launch of the developer preview for Espresso!\n\nThe compelling thing about developing Espresso was making it easy and fun for developers to write reliable UI tests. Espresso has a small, predictable, and easy to learn API, which is still open for customization. But most importantly - Espresso removes the need to think about the complexity of multi-threaded testing. With Espresso, you can think procedurally and write concise, beautiful, and reliable Android UI tests quickly.\n\nEspresso is now being used by over 30 applications within Google (Drive, Maps and G+, just to name a few). Starting from today, Espresso will also be available to our great developer community. We hope you will also enjoy testing your applications with Espresso and looking forward to your feedback and contributions!\n\n\n\n\n\nAndroid Test Kit: https://code.google.com/p/android-test-kit/", "by Eduardo Bravo Ortiz\n\n\n\u201cMobile first\u201d is the motto these days for many companies. However, being able to test a mobile app in a meaningful way is very challenging. On the Google+ team we have had our share of trial and error that has led us to successful strategies for testing mobile applications on both iOS and Android.\n\n\nGeneral\n\n\n\nUnderstand the platform. Testing on Android is not the same as testing on iOS. The testing tools and frameworks available for each platform are significantly different. (e.g., Android uses Java while iOS uses Objective-C, UI layouts are built differently on each platform, UI testing frameworks also work very differently in both platforms.)\n\nStabilize your test suite and test environments. Flaky tests are worse than having no tests, because a flaky test pollutes your build health and decreases the credibility of your suite.\n\n\nBreak down testing into manageable pieces. There are too many complex pieces when testing on mobile (e.g., emulator/device state, actions triggered by the OS). \n\nProvide a hermetic test environment for your tests. Mobile UI tests are flaky by nature; don\u2019t add more flakiness to them by having external dependencies.\n\nUnit tests are the backbone of your mobile test strategy. Try to separate the app code logic from the UI as much as possible. This separation will make unit tests more granular and faster.\n\n\n\n\nAndroid Testing\n\n\nUnit Tests\n\nSeparating UI code from code logic is especially hard in Android. For example, an Activity is expected to act as a controller and view at the same time; make sure you keep this in mind when writing unit tests. Another useful recommendation is to decouple unit tests from the Android emulator, this will remove the need to build an APK and install it and your tests will run much faster. Robolectric is a perfect tool for this; it stubs the implementation of the Android platform while running tests.\n\n\nHermetic UI Tests\n\nA hermetic UI test typically runs as a test without network calls or external dependencies. Once the tests can run in a hermetic environment, a white box testing framework like Espresso can simulate user actions on the UI and is tightly coupled to the app code. Espresso will also synchronize your tests actions with events on the UI thread, reducing flakiness. More information on Espresso is coming in a future Google Testing Blog article.\n\n\n\nDiagram: Non-Hermetic Flow vs. Hermetic Flow\n\n\n\n\n\nMonkey Tests\n\nMonkey tests look for crashes and ANRs by stressing your Android application. They exercise pseudo-random events like clicks or gestures on the app under test. Monkey test results are reproducible to a certain extent; timing and latency are not completely under your control and can cause a test failure. Re-running the same monkey test against the same configuration will often reproduce these failures, though. If you run them daily against different SDKs, they are very effective at catching bugs earlier in the development cycle of a new release.\n\n\niOS Testing\n\n\nUnit Tests\n\nUnit test frameworks like OCUnit, which comes bundled with Xcode, or GTMSenTestcase are both good choices.\n\n\nHermetic UI Tests\n\nKIF has proven to be a powerful solution for writing Objective-C UI tests. It runs in-process which allows tests to be more tightly coupled with the app under test, making the tests inherently more stable. KIF allows iOS developers to write tests using the same language as their application.\n\n\nFollowing the same paradigm as Android UI tests, you want Objective-C tests to be hermetic. A good approach is to mock the server with pre-canned responses. Since KIF tests run in-process, responses can be built programmatically, making tests easier to maintain and more stable.\n\n\nMonkey Tests\n\niOS has no equivalent native tool for writing monkey tests as Android does, however this type of test still adds value in iOS (e.g. we found 16 crashes in one of our recent Google+ releases). The Google+ team developed their own custom monkey testing framework, but there are also many third-party options available.\n\n\nBackend Testing\n\n\nA mobile testing strategy is not complete without testing the integration between server backends and mobile clients. This is especially true when the release cycles of the mobile clients and backends are very different. A replay test strategy can be very effective at preventing backends from breaking mobile clients. The theory behind this strategy is to simulate mobile clients by having a set of golden request and response files that are known to be correct. The replay test suite should then send golden requests to the backend server and assert that the response returned by the server matches the expected golden response. Since client/server responses are often not completely deterministic, you will need to utilize a diffing tool that can ignore expected differences.\n\n\nTo make this strategy successful you need a way to seed a repeatable data set on the backend and make all dependencies that are not relevant to your backend hermetic. Using in-memory servers with fake data or an RPC replay to external dependencies are good ways of achieving repeatable data sets and hermetic environments. Google+ mobile backend uses Guice for dependency injection, which allows us to easily swap out dependencies with fake implementations during testing and seed data fixtures.\n\n\n\nDiagram: Normal flow vs Replay Tests flow\n\n\n\n\n\nConclusion\n\n\nMobile app testing can be very challenging, but building a comprehensive test strategy that understands the nature of different platforms and tools is the key to success. Providing a reliable and hermetic test environment is as important as the tests you write.\n\n\nFinally, make sure you prioritize your automation efforts according to your team needs. This is how we prioritize on the Google+ team:\n\n\n\nUnit tests: These should be your first priority in either Android or iOS. They run fast and are less flaky than any other type of tests.\n\n\nBackend tests: Make sure your backend doesn\u2019t break your mobile clients. Breakages are very likely to happen when the release cycle of mobile clients and backends are different.\n\n\nUI tests: These are slower by nature and flaky. They also take more time to write and maintain. Make sure you provide coverage for at least the critical paths of your app.\n\n\nMonkey tests: This is the final step to complete your mobile automation strategy.\n\n\n\n\nHappy mobile testing from the Google+ team.", "By Andrew Trenk\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nYour trusty Calculator class is one of your most popular open source projects, with many happy users:\n\n\npublic class Calculator {\n  public int add(int a, int b) {\n    return a + b;\n  }\n}\n\nYou also have tests to help ensure that it works properly:\n\n\npublic void testAdd() {\n  assertEquals(3, calculator.add(2, 1));\n  assertEquals(2, calculator.add(2, 0));\n  assertEquals(1, calculator.add(2, -1));\n}\n\nHowever, a fancy new library promises several orders of magnitude speedup in your code if you use it in place of the addition operator. You excitedly change your code to use this library:\n\n\npublic class Calculator {\n  private AdderFactory adderFactory;\n  public Calculator(AdderFactor adderFactory) { this.adderFactory = adderFactory; }\n  public int add(int a, int b) {\n    Adder adder = adderFactory.createAdder();\n    ReturnValue returnValue = adder.compute(new Number(a), new Number(b));\n    return returnValue.convertToInteger();\n  }\n}\n\nThat was easy, but what do you do about the tests for this code? None of the existing tests should need to change since you only changed the code's implementation, but its user-facing behavior didn't change. In most cases, tests should focus on testing your code's public API, and your code's implementation details shouldn't need to be exposed to tests.\n\n\nTests that are independent of implementation details are easier to maintain since they don't need to be changed each time you make a change to the implementation. They're also easier to understand since they basically act as code samples that show all the different ways your class's methods can be used, so even someone who's not familiar with the implementation should usually be able to read through the tests to understand how to use the class.\n\n\nThere are many cases where you do want to test implementation details (e.g. you want to ensure that your implementation reads from a cache instead of from a datastore), but this should be less common since in most cases your tests should be independent of your implementation.\n\n\nNote that test setup may need to change if the implementation changes (e.g. if you change your class to take a new dependency in its constructor, the test needs to pass in this dependency when it creates the class), but the actual test itself typically shouldn't need to change if the code's user-facing behavior doesn't change.", "By Andrew Trenk\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nA test double is an object that can stand in for a real object in a test, similar to how a stunt double stands in for an actor in a movie. These are sometimes all commonly referred to as \u201cmocks\u201d, but it's important to distinguish between the different types of test doubles since they all have different uses. The most common types of test doubles are stubs, mocks, and fakes.\n\n\nA stub has no logic, and only returns what you tell it to return. Stubs can be used when you need an object to return specific values in order to get your code under test into a certain state. While it's usually easy to write stubs by hand, using a mocking framework is often a convenient way to reduce boilerplate.\n\n\n// Pass in a stub that was created by a mocking framework.\nAccessManager accessManager = new AccessManager(stubAuthenticationService);\n// The user shouldn't have access when the authentication service returns false.\nwhen(stubAuthenticationService.isAuthenticated(USER_ID)).thenReturn(false);\nassertFalse(accessManager.userHasAccess(USER_ID));\n// The user should have access when the authentication service returns true. \nwhen(stubAuthenticationService.isAuthenticated(USER_ID)).thenReturn(true);\nassertTrue(accessManager.userHasAccess(USER_ID));\n\n\nA mock has expectations about the way it should be called, and a test should fail if it\u2019s not called that way. Mocks are used to test interactions between objects, and are useful in cases where there are no other visible state \nchanges or return results that you can verify (e.g. if your code reads from disk and you want to ensure that it doesn't do more than one disk read, you can use a mock to verify that the method that does the read is only called once).\n\n\n// Pass in a mock that was created by a mocking framework.\nAccessManager accessManager = new AccessManager(mockAuthenticationService);\naccessManager.userHasAccess(USER_ID);\n// The test should fail if accessManager.userHasAccess(USER_ID) didn't call\n// mockAuthenticationService.isAuthenticated(USER_ID) or if it called it more than once.\nverify(mockAuthenticationService).isAuthenticated(USER_ID);\n\nA fake doesn\u2019t use a mocking framework: it\u2019s a lightweight implementation of an API that behaves like the real implementation, but isn't suitable for production (e.g. an in-memory database). Fakes can be used when you can't use a real implementation in your test (e.g. if the real implementation is too slow or it talks over the network). You shouldn't need to write your own fakes often since fakes should usually be created and maintained by the person or team that owns the real implementation.\n\n\n// Creating the fake is fast and easy.\nAuthenticationService fakeAuthenticationService = new FakeAuthenticationService();\nAccessManager accessManager = new AccessManager(fakeAuthenticationService);\n// The user shouldn't have access since the authentication service doesn't\n// know about the user.\nassertFalse(accessManager.userHasAccess(USER_ID));\n// The user should have access after it's added to the authentication service.\nfakeAuthenticationService.addAuthenticatedUser(USER_ID);\nassertTrue(accessManager.userHasAccess(USER_ID));\n\nThe term \u201ctest double\u201d was coined by Gerard Meszaros in the book xUnit Test Patterns. You can find more information about test doubles in the book, or on the book\u2019s website. You can also find a discussion about the different types of test doubles in this article by Martin Fowler.", "By Jonathan Rockway and Andrew Trenk\n\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nAfter many years of blogging, you decide to try out your blog platform's API. You start playing around with it, but then you realize: how can you tell that your code works without having your test talk to a remote blog server?\n\n\npublic void deletePostsWithTag(Tag tag) {\n  for (Post post : blogService.getAllPosts()) {\n    if (post.getTags().contains(tag)) { blogService.deletePost(post.getId()); }\n  }\n}\n\n\nFakes to the rescue! A fake is a lightweight implementation of an API that behaves like the real implementation, but isn't suitable for production. In the case of the blog service, all you care about is the ability to retrieve and delete posts. While a real blog service would need a database and support multiple frontend servers, you don\u2019t need any of that to test your code, all you need is any implementation of the blog service API. You can achieve this with a simple in-memory implementation:\n\n\npublic class FakeBlogService implements BlogService {  \n  private final Set<Post> posts = new HashSet<Post>(); // Store posts in memory\n  public void addPost(Post post) { posts.add(post); }\n  public void deletePost(int id) {\n    for (Post post : posts) {\n      if (post.getId() == id) { posts.remove(post); return; }\n    }\n    throw new PostNotFoundException(\"No post with ID \" + id);\n  }\n  public Set<Post> getAllPosts() { return posts; }\n}\n\n\nNow your tests can swap out the real blog service with the fake and the code under test won't even know the difference.\n\n\nFakes are useful for when you can't use the real implementation in a test, such as if the real implementation is too slow (e.g. it takes several minutes to start up) or if it's non-deterministic (e.g. it talks to an external machine that may not be available when your test runs).\n\n\nYou shouldn't need to write your own fakes often since each fake should be created and maintained by the person or team that owns the real implementation. If you\u2019re using an API that doesn't provide a fake, it\u2019s often easy to create one yourself: write a wrapper around the part of the code that you can't use in your tests, and create a fake for that wrapper. Remember to create the fake at the lowest level possible (e.g. if you can't use a database in your tests, fake out the database instead of faking out all of your classes that talk to the database), that way you'll have fewer fakes to maintain, and your tests will be executing more real code for important parts of your system.\n\n\nFakes should have their own tests to ensure that they behave like the real implementation (e.g. if the real implementation throws an exception when given certain input, the fake implementation should also throw an exception when given the same input). One way to do this is to write tests against the API's public interface, and run those tests against both the real and fake implementations.\n\n\nIf you still don't fully trust that your code will work in production if all your tests use a fake, you can write a small number of integration tests to ensure that your code will work with the real implementation.", "by Anthony Vallone\n\nHow long does it take to find the root cause of a failure in your system? Five minutes? Five days? If you answered close to five minutes, it\u2019s very likely that your production system and tests have great logging. All too often, seemingly unessential features like logging, exception handling, and (dare I say it) testing are an implementation afterthought. Like exception handling and testing, you really need to have a strategy for logging in both your systems and your tests. Never underestimate the power of logging. With optimal logging, you can even eliminate the necessity for debuggers. Below are some guidelines that have been useful to me over the years.\n\n\nChanneling Goldilocks\n\nNever log too much. Massive, disk-quota burning logs are a clear indicator that little thought was put in to logging. If you log too much, you\u2019ll need to devise complex approaches to minimize disk access, maintain log history, archive large quantities of data, and query these large sets of data. More importantly, you\u2019ll make it very difficult to find valuable information in all the chatter.\n\nThe only thing worse than logging too much is logging too little. There are normally two main goals of logging: help with bug investigation and event confirmation. If your log can\u2019t explain the cause of a bug or whether a certain transaction took place, you are logging too little.\n\nGood things to log:\n\n\nImportant startup configuration\nErrors\nWarnings\nChanges to persistent data\nRequests and responses between major system components\nSignificant state changes\nUser interactions\nCalls with a known risk of failure\nWaits on conditions that could take measurable time to satisfy\nPeriodic progress during long-running tasks\nSignificant branch points of logic and conditions that led to the branch\nSummaries of processing steps or events from high level functions - Avoid logging every step of a complex process in low-level functions.\n\n\nBad things to log:\n\n\nFunction entry - Don\u2019t log a function entry unless it is significant or logged at the debug level.\nData within a loop - Avoid logging from many iterations of a loop. It is OK to log from iterations of small loops or to log periodically from large loops.\nContent of large messages or files - Truncate or summarize the data in some way that will be useful to debugging.\nBenign errors - Errors that are not really errors can confuse the log reader. This sometimes happens when exception handling is part of successful execution flow.\nRepetitive errors - Do not repetitively log the same or similar error. This can quickly fill a log and hide the actual cause. Frequency of error types is best handled by monitoring. Logs only need to capture detail for some of those errors.\n\n\n\nThere is More Than One Level\n\nDon't log everything at the same log level. Most logging libraries offer several log levels, and you can enable certain levels at system startup. This provides a convenient control for log verbosity.\n\nThe classic levels are:\n\n\nDebug - verbose and only useful while developing and/or debugging.\nInfo - the most popular level.\nWarning - strange or unexpected states that are acceptable.\nError - something went wrong, but the process can recover.\nCritical - the process cannot recover, and it will shutdown or restart.\n\n\nPractically speaking, only two log configurations are needed:\n\n\nProduction - Every level is enabled except debug. If something goes wrong in production, the logs should reveal the cause.\nDevelopment & Debug - While developing new code or trying to reproduce a production issue, enable all levels.\n\n\n\nTest Logs Are Important Too\n\nLog quality is equally important in test and production code. When a test fails, the log should clearly show whether the failure was a problem with the test or production system. If it doesn't, then test logging is broken.\n\nTest logs should always contain:\n\n\nTest execution environment\nInitial state\nSetup steps\nTest case steps\nInteractions with the system\nExpected results\nActual results\nTeardown steps\n\n\n\nConditional Verbosity With Temporary Log Queues\n\nWhen errors occur, the log should contain a lot of detail. Unfortunately, detail that led to an error is often unavailable once the error is encountered. Also, if you\u2019ve followed advice about not logging too much, your log records prior to the error record may not provide adequate detail. A good way to solve this problem is to create temporary, in-memory log queues. Throughout processing of a transaction, append verbose details about each step to the queue. If the transaction completes successfully, discard the queue and log a summary. If an error is encountered, log the content of the entire queue and the error. This technique is especially useful for test logging of system interactions.\n\n\nFailures and Flakiness Are Opportunities\n\nWhen production problems occur, you\u2019ll obviously be focused on finding and correcting the problem, but you should also think about the logs. If you have a hard time determining the cause of an error, it's a great opportunity to improve your logging. Before fixing the problem, fix your logging so that the logs clearly show the cause. If this problem ever happens again, it\u2019ll be much easier to identify.\n\nIf you cannot reproduce the problem, or you have a flaky test, enhance the logs so that the problem can be tracked down when it happens again.\n\nUsing failures to improve logging should be used throughout the development process. While writing new code, try to refrain from using debuggers and only use the logs. Do the logs describe what is going on? If not, the logging is insufficient.\n\n\nMight As Well Log Performance Data\n\nLogged timing data can help debug performance issues. For example, it can be very difficult to determine the cause of a timeout in a large system, unless you can trace the time spent on every significant processing step. This can be easily accomplished by logging the start and finish times of calls that can take measurable time:\n\n\nSignificant system calls\nNetwork requests\nCPU intensive operations\nConnected device interactions\nTransactions\n\n\n\nFollowing the Trail Through Many Threads and Processes\n\nYou should create unique identifiers for transactions that involve processing across many threads and/or processes. The initiator of the transaction should create the ID, and it should be passed to every component that performs work for the transaction. This ID should be logged by each component when logging information about the transaction. This makes it much easier to trace a specific transaction when many transactions are being processed concurrently.\n\n\nMonitoring and Logging Complement Each Other\n\nA production service should have both logging and monitoring. Monitoring provides a real-time statistical summary of the system state. It can alert you if a percentage of certain request types are failing, it is experiencing unusual traffic patterns, performance is degrading, or other anomalies occur. In some cases, this information alone will clue you to the cause of a problem. However, in most cases, a monitoring alert is simply a trigger for you to start an investigation. Monitoring shows the symptoms of problems. Logs provide details and state on individual transactions, so you can fully understand the cause of problems.", "By Andrew Trenk\n\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\nWhen writing tests for your code, it can seem easy to ignore your code's dependencies by mocking them out.\n\npublic void testCreditCardIsCharged() {\n  paymentProcessor = new PaymentProcessor(mockCreditCardServer);\n  when(mockCreditCardServer.isServerAvailable()).thenReturn(true);\n  when(mockCreditCardServer.beginTransaction()).thenReturn(mockTransactionManager);\n  when(mockTransactionManager.getTransaction()).thenReturn(transaction);\n  when(mockCreditCardServer.pay(transaction, creditCard, 500)).thenReturn(mockPayment);\n  when(mockPayment.isOverMaxBalance()).thenReturn(false);\n  paymentProcessor.processPayment(creditCard, Money.dollars(500));\n  verify(mockCreditCardServer).pay(transaction, creditCard, 500);\n}\n\nHowever, not using mocks can sometimes result in tests that are simpler and more useful.\n\npublic void testCreditCardIsCharged() {\n  paymentProcessor = new PaymentProcessor(creditCardServer);\n  paymentProcessor.processPayment(creditCard, Money.dollars(500));\n  assertEquals(500, creditCardServer.getMostRecentCharge(creditCard));\n}\n\nOverusing mocks can cause several problems:\n\n- Tests can be harder to understand. Instead of just a straightforward usage of your code (e.g. pass in some values to the method under test and check the return result), you need to include extra code to tell the mocks how to behave. Having this extra code detracts from the actual intent of what you\u2019re trying to test, and very often this code is hard to understand if you're not familiar with the implementation of the production code.\n\n- Tests can be harder to maintain. When you tell a mock how to behave, you're leaking implementation details of your code into your test. When implementation details in your production code change, you'll need to update your tests to reflect these changes. Tests should typically know little about the code's implementation, and should focus on testing the code's public interface.\n\n- Tests can provide less assurance that your code is working properly. When you tell a mock how to behave, the only assurance you get with your tests is that your code will work if your mocks behave exactly like your real implementations. This can be very hard to guarantee, and the problem gets worse as your code changes over time, as the behavior of the real implementations is likely to get out of sync with your mocks.\n\nSome signs that you're overusing mocks are if you're mocking out more than one or two classes, or if one of your mocks specifies how more than one or two methods should behave. If you're trying to read a test that uses \nmocks and find yourself mentally stepping through the code being tested in order to understand the test, \nthen you're probably overusing mocks.\n\nSometimes you can't use a real dependency in a test (e.g. if it's too slow or talks over the network), but there may better options than using mocks, such as a hermetic local server (e.g. a credit card server that you start up on your machine specifically for the test) or a fake implementation (e.g. an in-memory credit card server).\n\nFor more information about using hermetic servers, see http://googletesting.blogspot.com/2012/10/hermetic-servers.html. Stay tuned for a future Testing on the Toilet episode about using fake implementations.", "by The GTAC Committee\n\n\n\n\nThe Google Test Automation Conference (GTAC) was held last week in NYC on April 23rd & 24th. The theme for this year's conference was focused on Mobile and Media. We were fortunate to have a cross section of attendees and presenters from industry and academia. This year\u2019s talks focused on trends we are seeing in industry combined with compelling talks on tools and infrastructure that can have a direct impact on our products. We believe we achieved a conference that was focused for engineers by engineers. GTAC 2013 demonstrated that there is a strong trend toward the emergence of test engineering as a computer science discipline across companies and academia alike.\n\n\n\n\nAll of the slides, video recordings, and photos are now available on the GTAC site. Thank you to all the speakers and attendees who made this event spectacular. We are already looking forward to the next GTAC. If you have suggestions for next year\u2019s location or theme, please comment on this post. To receive GTAC updates, subscribe to the Google Testing Blog. \n\n\n\n\nHere are some responses to GTAC 2013:\n\n\n\u201cMy first GTAC, and one of the best conferences of any kind I've ever been to. The talks were consistently great and the chance to interact with so many experts from all over the map was priceless.\u201d - Gareth Bowles, Netflix\n\n\n\u201cAdding my own thanks as a speaker (and consumer of the material, I learned a lot from the other speakers) -- this was amazingly well run, and had facilities that I've seen many larger conferences not provide. I got everything I wanted from attending and more!\u201d - James Waldrop, Twitter\n\n\n\u201cThis was a wonderful conference. I learned so much in two days and met some great people. Can't wait to get back to Denver and use all this newly acquired knowledge!\u201d - Crystal Preston-Watson, Ping Identity\n\n\n\u201cGTAC is hands down the smoothest conference/event I've attended. Well done to Google and all involved.\u201d - Alister Scott, ThoughtWorks\n\n\n\u201cThanks and compliments for an amazingly brain activity spurring event. I returned very inspired. First day back at work and the first thing I am doing is looking into improving our build automation and speed (1 min is too long. We are not building that much, groovy is dynamic).\u201d - Irina Muchnik, Zynx Health", "by The GTAC Committee\n\nGTAC is just around the corner, and we\u2019re all very busy and excited. I know we say this every year, but this is going to be the best GTAC ever! We have updated the GTAC site with important details:\n\n\nThe Schedule\u00a0\nSpeaker Profiles\u00a0\nCommittee Profiles\u00a0\n\n\nIf you are on the attendance list, we\u2019ll see you on April 23rd. If not, check out the Live Stream page where you can watch the conference live and can get involved in Q&A after each talk. Perhaps your team can gather in a conference room and attend remotely.", "by Anthony Vallone\n\nWe have two excellent, new videos to share about testing at Google. If you are curious about the work that our Test Engineers (TEs) and Software Engineers in Test (SETs) do, you\u2019ll find both of these videos very interesting.\n\nThe \nLife at Google \nteam produced a video series called \nDo Cool Things That Matter. \nThis series includes a video from an SET and TE on the Maps team (Sean Jordan and Yvette Nameth) discussing their work on the Google Maps team.\n\nMeet Yvette and Sean from the Google Maps Test Team\n\n\n\n\n\n\nThe \nGoogle Students \nteam hosted a \nHangouts On Air \nevent with several Google SETs (Diego Salas, Karin Lundberg, Jonathan Velasquez, Chaitali Narla, and Dave Chen) \ndiscussing the SET role.\n\nSoftware Engineers in Test at Google - Covering your (Code)Bases\n\n\n\n\n\n\nInterested in joining the ranks of TEs or SETs at Google? Search for Google test jobs.", "By Andrew Trenk\nThis article was adapted from a Google Testing on the Toilet (TotT) episode. You can download a printer-friendly version of this TotT episode and post it in your office.\n\n\nThere are typically two ways a unit test can verify that the code under test is working properly: by testing state or by testing interactions. What\u2019s the difference between these?\n\nTesting state means you're verifying that the code under test returns the right results.\n\npublic void testSortNumbers() {\n  NumberSorter numberSorter = new NumberSorter(quicksort, bubbleSort);\n  // Verify that the returned list is sorted. It doesn't matter which sorting\n  // algorithm is used, as long as the right result is returned.\n  assertEquals(\n      new ArrayList(1, 2, 3),\n      numberSorter.sortNumbers(new ArrayList(3, 1, 2)));\n}\nTesting interactions means you're verifying that the code under test calls certain methods properly.\n\npublic void testSortNumbers_quicksortIsUsed() {\n  // Pass in mocks to the class and call the method under test.\n  NumberSorter numberSorter = new NumberSorter(mockQuicksort, mockBubbleSort);\n  numberSorter.sortNumbers(new ArrayList(3, 1, 2));\n  // Verify that numberSorter.sortNumbers() used quicksort. The test should\n  // fail if mockQuicksort.sort() is never called or if it's called with the\n  // wrong arguments (e.g. if mockBubbleSort is used to sort the numbers).\n  verify(mockQuicksort).sort(new ArrayList(3, 1, 2));\n}\nThe second test may result in good code coverage, but it doesn't tell you whether sorting works properly, only that quicksort.sort() was called. Just because a test that uses interactions is passing doesn't mean the code is working properly. This is why in most cases, you want to test state, not interactions.\n\nIn general, interactions should be tested when correctness doesn't just depend on what the code's output is, but also how the output is determined. In the above example, you would only want to test interactions in addition to testing state if it's important that quicksort is used (e.g. the method would run too slowly with a different sorting algorithm), otherwise the test using interactions is unnecessary.\n\nWhat are some other examples of cases where you want to test interactions?\n\n- The code under test calls a method where differences in the number or order of calls would cause undesired behavior, such as side effects (e.g. you only want one email to be sent), latency (e.g. you only want a certain number of disk reads to occur) or multithreading issues (e.g. your code will deadlock if it calls some methods in the wrong order). Testing interactions ensures that your tests will fail if these methods aren't called properly.\n\n- You're testing a UI where the rendering details of the UI are abstracted away from the UI logic (e.g. using MVC or MVP). In tests for your controller/presenter, you only care that a certain method of the view was called, not what was actually rendered, so you can test interactions with the view. Similarly, when testing the view, you can test interactions with the controller/presenter.", "by The GTAC Committee\n\nWe have completed selection and confirmation of all speakers for GTAC 2013. You can find the detailed agenda at:\n\u00a0 developers.google.com/gtac/2013/schedule\n\nThank you to all who submitted proposals! It was very hard to make selections from so many fantastic submissions.\n\nIf you were not extended an invitation, don\u2019t forget that you can join us via YouTube live streaming. We\u2019ll be setting up Google Moderator, so remote attendees can get involved in Q&A after each talk. Information about live streaming, Moderator, and other details will be posted on the GTAC site soon and announced here.", "By The GTAC Committee\n\nIf you would like to attend or speak at GTAC 2013, the deadline to sign-up is January 23rd, 2013.\n\nWe are really excited about hosting this event at our fabulous New York City office. We\u2019ve received many interesting presentation proposals so far, and this year\u2019s GTAC will certainly be a fascinating, important event for test automation professionals. We are still accepting proposals, so it\u2019s not too late to add yours for consideration.\n\nYou can find details about the conference at our new site:\n\u00a0 developers.google.com/gtac\nWe will be making regular updates to this site over the next several weeks.\n\nFor those that have already signed up to attend or speak, we will contact you directly in early February.", "By Yvette Nameth\n\nAt Google, we\u2019re very big into highlighting individuals\u2019 strengths and using them to make teams and products better. However, we frequently get asked \u201cWhat do Test Engineers (aka TEs) do?\u201d I pause when I get this question since it\u2019s hard to speak for my peers - I test Google Maps rendering, which is just one small portion of what Google\u2019s Test Engineers test.\n\nIn order to get a clearer picture of what Test Engineers are responsible for, I chatted with three of my colleagues. We were able to identify the underlying Test Engineers\u2019 similarities, while highlighting the differences.\n\nSo what common themes do Test Engineers specialize in at Google?\n\n\nWe\u2019re product experts:\nTest Engineers need to become a \u201cgo-to\u201d person for how their product works and integrates with other Google products. (You aren\u2019t expected to have this before working with a product, but you need to figure out how to become one on any product you work on!) TEs need to understand use cases and contracts with other services, products, and features. We aren\u2019t expected to write unit tests for other engineer\u2019s code; instead we ensure product quality on the functional and integration aspects of the product.\n\nWe\u2019re flexible:\nTest Engineers are required to switch tasks and re-prioritize frequently. From unplanned catastrophes, to shifting launch calendars, to people asking us questions, our work is filled with interrupts. We determine how to ensure quality in the face of the interrupts.\n\nWe also modify our tests based on the pace of the development and understand that there is no one right way to test a product. Test Engineers adapt tools to meet their needs and understand when a tool just can\u2019t get the job done.\n\nWe\u2019re clear communicators:\nWe have to be able to communicate via test plans, design docs, bugs, email and code. Every day we work with a wide variety of people in different roles: Software Engineers, Software Engineers in Test, Product Managers, Usability Researchers, Designers, Legal Counsel, etc. We need to address these different audiences to make sure we\u2019re either gathering the information that will help us build better strategies or presenting feedback that will help influence the product.\n\nWe\u2019re good at coordination:\nWe are people who use our \u201cin between the product and user\u201d status to coordinate integration testing efforts between products. We may coordinate manual testing efforts by our manual testers; or we may make sure that test gaps are being addressed by \u201csomeone\u201d (Test Engineer, Software Engineer in Test, or Software Engineer). We put our product knowledge and communication together with a bit of coordination and make sure that bugs are looked at and the product is getting tested hourly / daily.\n\nWe have impact:\nGoogle Test Engineers have big impact. We hold responsibility thinking of ways that our products could fail in \u201creal scenarios\u201d; and then we add tests to make sure that the worst won\u2019t come to pass.\n\nHow big is this? Well, in my case, I\u2019m responsible for making sure Google Maps represents a map that is useful to my relatives in the middle of rural Montana as well as my friends living in London, Paris or Sydney. When you add to that the billions of other users in different regions, speaking different languages and using the map for different reasons, I know that my testing is impacting their ability to get around and find out information about the physical world around them safely.\n\nWe code:\nThe other most common question is \u201cDo you write code?\u201d The answer is yes; Test Engineers at Google do code.\n\n\n\nThe three aspects that generally differentiate what a Test Engineer does day-to-day depend on the following:\n\n\nIndividual\u2019s Strengths & Interests:\nEveryone is different and every TE has different passions, strengths and areas of expertise. Thankfully, Google\u2019s a big enough company that many different areas of testing are available, and we gravitate to testing products we like. All TEs start with core competencies in testing, coding, and algorithms. How a TE applies this knowledge varies.\n\nThe Type of Product:\nDesktop, web app or mobile? Frontend or backend? The technologies that our products use and run on create a lot of variation in what and how we test.\n\nThe Product\u2019s History / Lifecycle:\nEarly concept products don\u2019t resemble those that exist in production. And the amount of testing that a product already has will determine what testing the TE is focused on. We work creating a test roadmap that parallels the product\u2019s development cycle and addresses any testing gaps.\n\n\n\nIf you still want to know what the day in the life of a Test Engineer entails, we\u2019ll never be able to give you a general answer for that. Instead I suggest that you check out what Alan Faulkner is doing or ask the next Google Test Engineer you meet.\n\nInterested in joining the ranks of Test Engineers (or Software Engineers in Test)? Check out http://goo.gl/2RDKj\n\nAbout the Contributors:\nAlbert Drona has been at Google for 5 years and is currently working on Google Maps for Mobile.\nJatin Shah has been at Google for 9 months on Google+.\nMohammad Khan has been at Google for 7 years and is currently working on Google+ releases.\nAbout the Author:\nYvette Nameth has been at Google for 5 years and is currently working on Google Maps rendering.", "By The GTAC Committee\n\nWe are happy to announce that the application process is now open for presentation proposals and attendance for the seventh GTAC (Google Test Automation Conference) to be held at the Google New York office on April 23 - 24th, 2013.\n\nGTAC brings together engineers from many organizations to discuss test automation. It is a great opportunity to present, learn, and challenge modern testing technologies and strategies. GTAC will be streamed live on YouTube this year, so even if you can\u2019t attend, you\u2019ll be able to watch the conference from your computer.\n\nSpeakers\nPresentations are targeted at student, academic, and experienced engineers working on test automation. Full presentations and lightning talks are 45 minutes and 15 minutes respectively. Speakers should be prepared for a question and answer session following their presentation. As mentioned in our recent post, the main theme is on testing media and mobile, however, we will consider proposals on other topics.\n\nApplication\nFor presentation proposals and/or attendance, complete this form. We will be selecting about 200 applicants for the event.\n\nDeadline\nThe due date for both presentation and attendance applications is Jan 23rd, 2013.\n\nFees\nThere are no registration fees. We will send out detailed registration instructions to each invited applicant. We will provide meals. Attendees must arrange their own travel and accommodations.", "By Vojta J\u00edna\n\n[NOTE: After this post was published, Testacular was renamed Karma.]\n\n\u201cTestacular has changed my life. Now I test-drive everything.\u201d\n\n-- Matias Cudich, YouTube on TV team lead\n\n\n\n\nAt Google we believe in testing. On the AngularJS team, it\u2019s even worse - we are super crazy about testing. Every feature of the framework is designed with testability in mind.\n\nWe found that we were struggling with existing tools, so we decided to write our own test runner. We wanted a test runner that would meet all of our needs for both quick development and continuous integration -- a truly spectacular test runner. We've called it Testacular.\n\nLet's walk through some mental tests for what we believe makes for an ideal test runner...\n\nit(\u2018should be fast\u2019)\nIn order to be productive and creative you need instant feedback. Testacular watches the files in your application. Whenever you change any of them, it immediately executes the specified tests and reports the results. You never have to leave your text editor.\n\nThis enables a new way of developing. Instead of moving back and forth between the editor and the browser, you can simply stay in the editor and experiment. You instantly see the results at the command line whenever your changes are saved.\n\nBesides that, our experience says that if test execution is slow, people don\u2019t write tests. Testacular eliminates many barriers that keep folks from writing tests. When developers get instant feedback from their tests, the tests become an asset rather than annoyance.\n\nit(\u2018should use real browsers\u2019)\nJavaScript itself is pretty consistent between different browsers, so one could potentially test browser code in non-browser environments like Node.js. Unfortunately, that\u2019s not the case with the DOM APIs. AngularJS does a lot of DOM manipulation, and we need to be sure that it works across browsers. Executing tests on real browsers is a must.\n\nAnd because Testacular communicates with browsers through a common protocols (eg. HTTP or WebSocket), you can test not only on desktop browsers but also on other devices such as mobile phones and tablets. For instance, the YouTube team uses Testacular to run continuous integration builds on PlayStation 3.\n\nAnother advantage of using real browsers is that you can use any of the tools that the browser provides. For example, you can jump into a debugger and step through your test.\n\nit(\u2018should be reliable/stable\u2019)\nTo be honest, most of these ideas were already implemented in JsTD almost three years ago. I think these are truly great ideas. Unfortunately, the implementation was flaky. It\u2019s very easy to get JsTD into an inconsistent state, so you end up restarting it pretty much all day.\n\nTestacular solves that. It can run for days, without restarting. That\u2019s because every test run is executed in a fresh iframe. It reconnects browsers that have lost their connection to the server. And yep, it can gracefully recover from other issues, like syntax errors in the code under test.\n\nWe'd like to invite you to take Testacular for a spin. You can learn a bit more in this screencast. Please let us know what you think!\n\n\n\n\nThe project is open sourced and developed on GitHub.", "By Anthony F. Voellm (aka Tony the @p3rfguy / G+) and Emily Bedont\n\n\n\n\nOn Wednesday, October 24th, while sitting under the Solar System, 30 software engineers from the Greater Seattle area came together at Google Kirkland to partake in the first ever Test Edition of Ship Wars. Ship Wars was created by two Google Waterloo engineers, Garret Kelly and Aaron Kemp, as a 20% project. Yes, 20% time does exist at Google! \u00a0The object of the game is to code a spaceship that will outperform all others in a virtual universe - algorithm vs algorithm.\n\n\n\n\nThe Kirkland event marked the 7th iteration of the program which was also recently done in NYC. Kirkland however was the first time that the game had been customized to encourage exploratory testing. In the case of \"Ship Wars the Test Edition,\" we planted 4 bugs that the engineering participants were awarded for finding. Well, we ran out of prizes and were quickly reminded that when you put a lot of testing minded people in a room, many bugs will be unveiled! One of the best unveiled bugs was not one of the four planted in the simulator. When turning your ship 90 degrees, the ship actually turned -90 degrees. Oops!\n\n\n\n\nParticipants were encouraged to test their spaceship built on their own machine or a Google Chromebook. While the coding was done in the browser, the simulator and web server were run on Google Compute Engine. Throughout the 90 minutes, people challenged other participants to duels. Head-to-head battles took place on Chromebooks at the front of the room. There were many accolades called out but in the end, there could only be one champion who would walk away with a brand spankin\u2019 new Nexus7. Check out our video of the evening\u2019s activities. \n\n\n\n\nSounds fun, huh? We sure hope our participants, including our first place winner shown receiving the Nexus 7 from Garret, enjoyed the evening! Beyond the battles, our guests were introduced to the revived Google Testing Blog, heard firsthand that GTAC will be back in 2013, learned about testing at Google, and interacted with Googlers in a \"Googley\" environment. Achievement unlocked.\n\n\n\nSpecial thanks to all the Googlers that supported the event!", "By The GTAC Committee\n\nThe next and seventh GTAC (Google Test Automation Conference) will be held on April 23-24, 2013 at the beautiful Google New York office! We had a late start preparing for GTAC, but things are now falling into place. This will be the second time it is hosted at our second largest engineering office. We are also sticking with our tradition of changing the region each year, as the last GTAC was held in California.\n\nThe GTAC event brings together engineers from many organizations to discuss test automation. It is a great opportunity to present, learn, and challenge modern testing technologies and strategies. We will soon be recruiting speakers to discuss their innovations.\n\nThis year\u2019s theme will be \u201cTesting Media and Mobile\u201c. In the past few years, substantial changes have taken place in both the media and mobile areas. Television is no longer the king of media. Over 27 billion videos are streamed in the U.S. per month. Over 1 billion people now own smartphones. HTML5 includes support for audio, video, and scalable vector graphics, which will liberate many web developers from their dependence on third-party media software. These are incredibly complex technologies to test. We are thrilled to be hosting this event in which many in the industry will share their innovations.\n\nRegistration information for speakers and attendees will soon be posted here and on the GTAC site (https://developers.google.com/gtac). Even though we will be focusing on \u201cTesting Media and Mobile\u201d, we will be accepting proposals for talks on other topics.", "By\u00a0Zhanyong Wan -\u00a0Software Engineer\n\nThese days, it seems that everyone is rolling their own C++ testing framework, if they haven't done so already. Wikipedia has a partial list of such frameworks. This is interesting because many OOP languages have only one or two major frameworks. For example, most Java people seem happy with either JUnit or TestNG. Are C++ programmers the do-it-yourself kind?\n\nWhen we started working on Google Test (Google\u2019s C++ testing framework), and especially after we open-sourced it, people began asking us why we were doing it. The short answer is that we couldn\u2019t find an existing C++ testing framework that satisfied all our needs. This doesn't mean that these frameworks were all poorly designed or implemented. Rather, many of them had great ideas and tricks that we learned from. However, Google had a huge number of C++ projects that got compiled on various operating systems (Linux, Windows, Mac OS X, and later Android, among others) with different compilers and all kinds of compiler flags, and we needed a framework that worked well in all these environments and ccould handle many different types and sizes of projects.\n\nUnlike Java, which has the famous slogan \"Write once, run anywhere,\" C++ code is being written in a much more diverse environment. Due to the complexity of the language and the need to do low-level tasks, compatibility between different C++ compilers and even different versions of the same compiler is poor. There is a C++ standard, but it's not well supported by compiler vendors. For many tasks you have to rely on unportable extensions or platform-specific functionality. This makes it hard to write a reasonably complex system that can be built using many different compilers and works on many platforms.\n\nTo make things more complicated, most C++ compilers allow you to turn off some standard language features in return for better performance. Don't like using exceptions? You can turn it off. Think dynamic cast is bad? You can disable Run-Time Type Identification, the feature behind dynamic cast and run-time access to type information. If you do any of these, however, code using these features will fail to compile. Many testing frameworks rely on exceptions. They are automatically out of the question for us since we turn off exceptions in many projects (in case you are curious, Google Test doesn\u2019t require exceptions or run-time type identification by default; when these language features are turned on, Google Test will try to take advantage of them and provide you with more utilities, like the exception assertions.).\n\nWhy not just write a portable framework, then? Indeed, that's a top design goal for Google Test. And authors of some other frameworks have tried this too. However, this comes with a cost. Cross-platform C++ development requires much more effort: you need to test your code with different operating systems, different compilers, different versions of them, and different compiler flags (combine these factors and the task soon gets daunting); some platforms may not let you do certain things and you have to find a workaround there and guard the code with conditional compilation; different versions of compilers have different bugs and you may have to revise your code to bypass them all; etc. In the end, it's hard unless you are happy with a bare-bone system.\n\nSo, I think a major reason that we have many C++ testing frameworks is that C++ is different in different environments, making it hard to write portable C++ code. John's framework may not suit Bill's environment, even if it solves John's problems perfectly.\n\nAnother reason is that some limitations of C++ make it impossible to implement certain features really well, and different people chose different ways to workaround the limitations. One notable example is that C++ is a statically-typed language and doesn't support reflection. Most Java testing frameworks use reflection to automatically discover tests you've written such that you don't have to register them one-by-one. This is a good thing as manually registering tests is tedious and you can easily write a test and forget to register it. Since C++ has no reflection, we have to do it differently. Unfortunately there is no single best option. Some frameworks require you to register tests by hand, some use scripts to parse your source code to discover tests, and some use macros to automate the registration. We prefer the last approach and think it works for most people, but some disagree. Also, there are different ways to devise the macros and they involve different trade-offs, so the result is not clear cut.\n\nLet\u2019s see some actual code to understand how Google Test solves the test registration problem. The simplest way to add a test is to use the TEST\u00a0macro (what else would we name it?):\n\nTEST(Subject, HasCertainProperty) {\n\u00a0 \u2026 testing code goes here \u2026\n}\n\nThis defines a test method whose purpose is to verify that the given subject has the given property. The macro automatically registers the test with Google Test such that it will be run when the test program (which may contain many such TEST definitions) is executed.\n\nHere\u2019s a more concrete example that verifies a Factorial() function works as expected for positive arguments:\n\nTEST(FactorialTest, HandlesPositiveInput) {\n\u00a0 EXPECT_EQ(1, Factorial(1));\n\u00a0 EXPECT_EQ(2, Factorial(2));\n\u00a0 EXPECT_EQ(6, Factorial(3));\n\u00a0 EXPECT_EQ(40320, Factorial(8));\n}\n\nFinally, many C++ testing framework authors neglected extensibility and were happy just providing canned solutions, so we ended up with many solutions, each satisfying a different niche but none general enough. A versatile framework must have a good extension story. Let's face it: you cannot be all things to all people, no matter what. Instead of bloating the framework with rarely used features, we should provide good out-of-box solutions for maybe 95% of the use cases, and leave the rest to extensions. If I can easily extend a framework to solve a particular problem of mine, I will feel less motivated to write my own thing. Unfortunately, many framework authors don't seem to see the importance of extensibility. I think that mindset contributed to the plethora of frameworks we see today. In Google Test, we try to make it easy to expand your testing vocabulary by defining custom assertions that generate informative error messages. For instance, here\u2019s a naive way to verify that an int value is in a given range:\n\nbool IsInRange(int value, int low, int high) {\n\u00a0 return low <= value && value <= high;\n}\n\u00a0 ...\n\u00a0 EXPECT_TRUE(IsInRange(SomeFunction(), low, high));\n\nThe problem is that when the assertion fails, you only know that the value returned by SomeFunction() is not in range [low, high], but you have no idea what that return value and the range actually are -- this makes debugging the test failure harder.\n\nYou could provide a custom message to make the failure more descriptive:\n\n\u00a0 EXPECT_TRUE(IsInRange(SomeFunction(), low, high))\n\u00a0 \u00a0 \u00a0 << \"SomeFunction() = \" << SomeFunction()\u00a0\n\u00a0 \u00a0 \u00a0 << \", not in range [\"\n\u00a0 \u00a0 \u00a0 << low << \", \" << high << \"]\";\n\nExcept that this is incorrect as SomeFunction() may return a different answer each time. \u00a0You can fix that by introducing an intermediate variable to hold the function\u2019s result:\n\n\u00a0 int result = SomeFunction();\n\u00a0 EXPECT_TRUE(IsInRange(result, low, high))\n\u00a0 \u00a0 \u00a0 << \"result (return value of SomeFunction()) = \" << result\n\u00a0 \u00a0 \u00a0 << \", not in range [\" << low << \", \" << high << \"]\";\n\nHowever this is tedious and obscures what you are really trying to do. \u00a0It\u2019s not a good pattern when you need to do the \u201cis in range\u201d check repeatedly. What we need here is a way to abstract this pattern into a reusable construct.\n\nGoogle Test lets you define a test predicate like this:\n\nAssertionResult IsInRange(int value, int low, int high) {\n\u00a0 if (value < low)\n\u00a0 \u00a0 return AssertionFailure()\n\u00a0 \u00a0 \u00a0 \u00a0 << value << \" < lower bound \" << low;\n\u00a0 else if (value > high)\n\u00a0 \u00a0 return AssertionFailure()\n\u00a0 \u00a0 \u00a0 \u00a0 << value << \" > upper bound \" << high;\n\u00a0 else\n\u00a0 \u00a0 return AssertionSuccess()\n\u00a0 \u00a0 \u00a0 \u00a0 << value << \" is in range [\"\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 << low << \", \" << high << \"]\";\n}\n\nThen the statement EXPECT_TRUE(IsInRange(SomeFunction(), low, high)) may print (assuming that SomeFunction() returns 13):\n\n\u00a0 \u00a0Value of: IsInRange(SomeFunction(), low, high)\n\u00a0 \u00a0 \u00a0Actual: false (13 < lower bound 20)\n\u00a0 \u00a0Expected: true\n\nThe same IsInRange() definition also lets you use it in an EXPECT_FALSE context, e.g. EXPECT_FALSE(IsInRange(AnotherFunction(), low, high)) could print:\n\n\u00a0 \u00a0Value of: IsInRange(AnotherFunction(), low, high)\n\u00a0 \u00a0 \u00a0Actual: true (25 is in range [20, 60])\n\u00a0 \u00a0Expected: false\n\nThis way, you can build a library of test predicates for your problem domain, and benefit from clear, declarative test code and descriptive failure messages.\n\nIn the same vein, Google Mock (our C++ mocking framework) allows you to easily define matchers that can be used exactly the same way as built-in matchers. \u00a0Also, we have included an event listener API in Google Test for people to write plug-ins. We hope that people will use these features to extend Google Test/Mock for their own need and contribute back extensions that might be generally useful.\n\nPerhaps one day we will solve the C++ testing framework fragmentation problem, after all. :-)", "By Chaitali Narla and Diego Salas\n\n\n\nConsider a complex and rich web app. Under the hood, it is probably a maze of servers, each performing a different task and most talking to each other. Any user action navigates this server maze on its round-trip from the user to the datastores and back. A lot of Google\u2019s web apps are like this including GMail and Google+. So how do we write end-to-end tests for them?\n\n\n\nThe \u201cEnd-To-End\u201d Test\n\n\n\nAn end-to-end test in the Google testing world is a test that exercises the entire server stack from a user request to response. Here is a simplified view of the System Under Test (SUT) that an end-to-end test would assert. Note that the frontend server in the SUT connects to a third backend which this particular user request does not need.\n\n\n\n\n\n\n\nOne of the challenges to writing a fast and reliable end-to-end test for such a system is avoiding network access. Tests involving network access are slower than their counterparts that only access local resources, and accessing external servers might lead to flakiness due to lack of determinism or unavailability of the external servers.\u00a0\n\n\n\n\nHermetic Servers\n\n\n\nOne of the tricks we use at Google to design end-to-end tests is Hermetic Servers.\n\n\n\nWhat is a Hermetic Server? The short definition would be a \u201cserver in a box\u201d. If you can start up the entire server on a single machine that has no network connection AND the server works as expected, you have a hermetic server! This is a special case of the more general \u201chermetic\u201d concept which applies to an isolated system not necessarily on a single machine.\u00a0\n\n\n\nWhy is it useful to have a hermetic server? Because if your entire SUT is composed of hermetic servers, it could all be started on a single machine for testing; no network connection necessary! The single machine could be a physical or virtual machine.\n\n\n\n\n\nDesigning Hermetic Servers\n\n\n\nThe process for building a hermetic server starts early in the design phase of any new server. Some things we watch out for:\n\n\n\n\nAll connections to other servers are injected into the server at runtime using a suitable form of dependency injection such as commandline flags or Guice.\nAll required static files are bundled in the server binary.\nIf the server talks to a datastore, make sure the datastore can be faked with data files or in-memory implementations.\n\n\n\n\n\nMeeting the above requirements ensures we have a highly configurable server that has potential to become a hermetic server. But it is not yet ready to be used in tests. We do a few more things to complete the package:\n\n\nMake sure those connection points which our test won\u2019t exercise have appropriate fakes or mocks to verify this non-interaction.\nProvide modules to easily populate datastores with test data.\nProvide logging modules that can help trace the request/response path as it passes through the SUT.\n\n\n\n\n\n\n\nUsing Hermetic Servers in tests\n\n\n\nLet\u2019s take the SUT shown earlier and assume all the servers in it are hermetic servers. Here is how an end-to-end test for the same user request would look:\n\n\n\n\n\n\n\n\n\nThe end-to-end test does the following steps:\n\n\nstarts the entire SUT as shown in the diagram on a single machine\nmakes requests to the server via the test client\nvalidates responses from the server\n\n\n\n\n\nOne thing to note here is the mock server connection for the backend is not needed in this test. If we wish to test a request that needs this backend, we would have to provide a hermetic server at that connection point as well.\n\n\n\nThis end-to-end test is more reliable because it uses no network connection. It is faster because everything it needs is available in-memory or in the local hard disk. We run such tests on our continuous builds, so they run at each changelist affecting any of the servers in the SUT. If the test fails, the logging module helps track where the failure occurred in the SUT.\n\n\n\n\n\nWe use hermetic servers in a lot of end-to-end tests. Some common cases include\n\n\nStartup tests for servers using Guice to verify that there are no Guice errors on startup.\nAPI tests for backend servers.\nMicro-benchmark performance tests.\nUI and API tests for frontend servers.\n\n\n\n\n\n\n\nConclusion\n\n\n\nHermetic servers do have some limitations. They will increase your test\u2019s runtime since you have to start the entire SUT each time you run the end-to-end test. If your test runs with limited resources such as memory and CPU, hermetic servers might push your test over those limits as the server interactions grow in complexity. The dataset size you can use in the in-memory datastores will be much smaller than production datastores.\n\n\n\nHermetic servers are a great testing tool. Like all other tools, they need to be used thoughtfully where appropriate.", "By Alan Myrvold\n\nAlan Faulkner is a Google Test Engineer working on DoubleClick Bid Manager, which enables advertising agencies and advertisers to bid on multiple ad exchanges. Bid Manager is the next generation of the Invite Media product, acquired by Google in 2010. Alan Faulkner has been focused on the migration component of Bid Manager, which transitions advertiser information from Invite Media to Bid Manager. He joined Google in August 2011, and works in the Kirkland, WA office.\n\n\n\n\n\nAre you a Test Engineer, or a Software Engineer in Test, and what\u2019s the difference?\nRight now, I\u2019m a Test Engineer, but the two roles can be very similar. As a Test Engineer, you\u2019re more focused on the overall quality of the product and speed of releases, while a Software Engineer in Test might focus more on test frameworks, automation, and refactoring code for testability. I think of the difference as more of a shift in focus and not capabilities, since both roles at Google need to be able to write production quality code. Example test engineering tasks I worked on are introducing an automated release process, identifying areas for the team to improve code coverage, and reducing the manual steps needed to validate data correctness.\n\nWhat is a typical day for you?\nWhen I get in, I look at any code reviews I need to respond to, look for any production bugs from technical account managers that are high priority, and then start writing code. In my current role, I focus my development effort on improving the efficiency and coverage of our large scale integration tests and frameworks. I also work on adding additional features to our product that improve our testability. I typically spend anywhere from 50% to 75% of my time either writing code or participating in code reviews.\n\nDo you write only test code?\nNo, I write a lot of code that is included in the product as well. One of the great things about being an SET or TE at Google is that you can write product code as easily as test code. I write both. My test code focuses on improving test frameworks and enabling developers to write integration tests. The production code that I write focuses on increasing the verification of external inputs. I also focus on adding features that improve testability. This pushes more quality features into the product itself rather than relying on test code for correctness.\n\nWhat programming languages do you use?\nBoth the test and product code are mostly Java. Occasionally I use Python or C++ too.\n\nHow much time to do you spend doing manual testing?\nRight now, with the role I am in, I spend less than 5% of my time doing manual testing. Although some exploratory testing helps develop product knowledge and find risky areas, it doesn\u2019t scale as a repeated process. There are a small amount of manual steps and I focus on ways to help reduce this so our team does not spend our time doing repeated manual steps as part of our data migration.\n\nDo you write unit tests for code that isn\u2019t yours?\nAt Google, the responsibility of testing is shared across all product engineers, not just Test Engineers. Everyone is responsible for writing unit tests as well as integration tests for their components. That being said, I have written unit tests for components that are outside of what I developed but that has been to illustrate how to write a unit test for said component. This component usually involved a abnormally complex set of code or to illustrate using a new mocking framework, such as Mockito.\n\nWhat do you like about working on the Google advertising products?\nI like the challenges of the scalability problems we need to solve, from handling massive amounts of data to dealing with lots of real time ad requests that need to be responded to in milliseconds. I also like the impact, since the products affect a lot of people. It\u2019s rewarding to work on stuff like that.\n\nHow is testing at Google different from your experience at other companies?\nI feel the role is more flexible at Google. There are fewer SET\u2019s and TE\u2019s in my group at Google per developer, and you have the flexibility to pick what is most important. For example, I get to write a lot of production code to fix bugs, make the code more testable, and increasing the visibility into errors encountered during our data migrations. Plus, developers at Google spend a lot of time writing tests, so testing isn\u2019t just my responsibility.\n\nHow does the Google Kirkland office differ from the headquarters in Mountain View?\nWhat I really like about the offices at Google is that each of them has their own local feel and personality. Google encourages this! For instance, the office here in Kirkland has a climbing wall, boats and all the conference rooms in our building are named after local bands in the area. The office in Seattle has kayaks and the New York office has an actual food truck in its cafeteria.\n\nWhat\u2019s the future of testing at Google?\nI think the future is really bright. We have a lot of flexibility to make a big impact on quality, testability and improving our release velocity. We need to release new features faster and with good quality. The problems that we face are complex and at an extreme scale. We need engineering teams focused on ensuring that we have efficient ways to simulate and test. There will always be a need for testers and developers that focus on these areas here at Google.\n\nInterested in test jobs at Google?", "By Anthony F. Voellm (aka Tony the perfguy)\n\nIt\u2019s amazing what has happened in the field of test in the last 20 years... a lot of \u201cart\u201d has turned into \u201cscience\u201d. Computer scientists, engineers, and many other disciplines have worked on provable systems and calculus, pioneered model based testing, invented security fuzz testing, and even settled on a common pattern for unit tests called xunit. The xunit pattern shows up in open source software like JUnit as well as in Microsoft development test tools.\n\nWith all this innovation in test, there\u2019s no wonder test is dead. \u00a0The situation is no different from the late 1800\u2019s when patents were declared dead. Everything had been invented. So now that everything in test has been invented, it\u2019s dead.\n\nWell... if you believe everything in test has been invented then please stop reading now :)\n\nAs an aside: \u00a0\u201cTest is dead\u201d was a keynote at the Google Test Automation Conference (GTAC) in 2011. \u00a0You can watch that talk and many other GTAC test talks on YouTube, and I definitely recommend you check them out here. \u00a0Talks span a wide range of topics ranging from GUI Automation to Cloud.\n\nWhat really excites me these days is that we have closed a chapter on test. A lot of the foundation of writing and testing great software has been laid (examples at the beginning of the post, tools like Webdriver for UI, FIO for storage, and much more), which I think of as Testing 1.0. We all use Testing 1.0 day in and day out. In fact at Google, most of the developers (called Software Engineers or SWEs) do the basic Testing 1.0 work and we have a high bar on quality. \u00a0Knuth once said \"Be careful about using the following code -- I've only proven that it works, I haven't tested it.\"\u00a0\n\nThis brings us to the current chapter in test which I call Testing 1.5. \u00a0This chapter is being written by computer scientists, applied scientists, engineers, developers, statisticians, and many other disciplines. \u00a0These people come together in the Software Engineer in Test (SET) and Test Engineer (TE) roles at Google. SET/TEs focus on; developing software faster, building it better the first time, testing it in depth, releasing it quicker, and making sure it works in all environments. \u00a0We often put deep test focus on Security, Reliability and Performance. \u00a0I sometimes think of the SET/TE\u2019s as risk assessors whose role is to figure out the probability of finding a bug, and then working to reduce that probability. Super interesting computer science problems where we take a solid engineering approach, rather than a process oriented / manual / people intensive based approach. \u00a0We always look to scale with machines wherever possible.\n\nWhile Testing 1.0 is done and 1.5 is alive and well, it\u2019s Testing 2.0 that gets me up early in the morning to start my day. Imagine if we could reinvent how we use and think about tests. \u00a0What if we could automate the complex decisions on good and bad quality that humans are still so good at today? What would it look like if we had a system collecting all the \u201cquality signals\u201d (think: tests, production information, developer behavior, \u2026) and could predict how good the code is today, and what it most likely will be tomorrow? That would be so awesome...\n\nGoogle is working on Testing 2.0 and we\u2019ll continue to contribute to Testing 1.0 and 1.5. Nothing is static... keep up or miss an amazing ride.\n\nPeace.... Tony\n\nSpecial thanks to Chris, Simon, Anthony, Matt, Asim, Ari, Baran, Jim, Chaitali, Rob, Emily, Kristen, Annie, and many others for providing input and suggestions for this post.", "By Anthony Vallone\n\nIf you haven\u2019t noticed, Google has been launching many public APIs recently. These APIs are empowering mobile app, desktop app, web service, and website developers by providing easy access to Google tools, services, and data. In the past couple of years, we have invested heavily in building a new API infrastructure for our APIs. Before this infrastructure, our teams had numerous technical challenges to solve when releasing an API: scalability, authorization, quota, caching, billing, client libraries, translation from external REST requests to internal RPCs, etc. The new infrastructure generically solves these problems and allows our teams to focus on their service. Automating testing of these new APIs turned out to be quite a large problem. Our solution to this problem is somewhat unique within Google, and we hope you find it interesting.\n\n\nSystem Under Test (SUT)\n\nLet\u2019s start with a simplified view of the SUT design:\n\n\n\n\n\nA developer\u2019s application uses a Google-supplied API Client Library to call Google API methods. The library connects to the API Infrastructure service and sends the request. Part of the request defines the particular API and version being used by the client. This service is knowledgeable of all Google APIs, because they are defined by API Configuration files. These files are created by each API providing team. Configuration files declare API versions, methods, method parameters, and other API settings. Given an API request and information about the API, the API Infrastructure Service can translate the request to Google\u2019s internal RPC format and pass it to the correct API Provider Service. This service then satisfies the request and passes the response back to the developer\u2019s app via the API Infrastructure Service and API Client Library.\n\nNow, the Fun Part\n\nAs of this writing, we have released 10 language-specific client libraries and 35 public APIs built on this infrastructure. Also, each of the libraries need to work on multiple platforms. Our test space has three dimensions: API (35), language (10), and platform (varies by lib). How are we going to test all the libraries on all the platforms against all the APIs when we only have two engineers on the team dedicated to test automation?\n\nStep 1: Create a Comprehensive API\n\nEach API uses different features of the infrastructure, and we want to ensure that every feature works. Rather than use the APIs to test our infrastructure, we create a Test API that uses every feature. In some cases where API configuration options are mutually exclusive, we have to create API versions that are feature-specific. Of course, each API team still needs to do basic integration testing with the infrastructure, but they can assume that the infrastructure features that their API depends on are well tested by the infrastructure team.\n\nStep 2: Client Abstraction Layer in the Tests\n\nWe want to avoid creating library-specific tests, because this would lead to mass duplication of test logic. The obvious solution is to create a test library to be used by all tests as an abstraction layer hiding the various libraries and platforms. This allows us to define tests that don\u2019t care about library or platform.\n\nStep 3: Adapter Servers\n\nWhen a test library makes an API call, it should be able to use any language and platform. We can solve this by setting up servers on each of our target platforms. For each target language, create a language-specific server. These servers receive requests from test clients. The servers need only translate test client requests into actual library calls and return the response to the caller. The code for these servers is quite simple to create and maintain.\n\nStep 4: Iterate\n\nNow, we have all the pieces in place. When we run our tests, they are configured to run over all supported languages and platforms against the test API:\n\n\n\n\n\n\n\nTest Nirvana Achieved\n\nWe have a suite of straightforward tests that focus on infrastructure features. When the tests run, they are quick, reliable, and test all of our supported features, platforms, and libraries. When a feature is added to the API infrastructure, we only need to create one new test, update each adapter server to handle a new call type, and add the feature to the Test API.", "Cross-posted from the Google Student Blog\n\nToday we\u2019re featuring Sabrina Williams, a Software Engineer in Test who joined Google in August 2011. Software Engineers in Test undertake a broad range of challenges on a daily basis, designing and building intelligent systems that can explore various use cases and scenarios for distributed computing infrastructure. Read on to learn more about Sabrina\u2019s path to Google and what she works on now that she\u2019s here!\n\n\n\n\n\nTell us about yourself and how you got to Google.\nI grew up in rural Prunedale, Calif. and went to Stanford where I double-majored in philosophy and computer science. After college I spent six years as a software engineer at HP, working primarily on printer drivers. I began focusing on testing my last two years there\u2014reading books, looking up information and prototyping test tools in my own time. By the time I left, I\u2019d started a project for an automated test framework that most of our lab used.\n\nI applied for a software engineering role at Google four years ago and didn\u2019t do well in my interviews. Thankfully, a Google recruiter called last year and set me up for software engineer (SWE) interviews again. After a day of talking about testing and mocking for every design question I answered, I was told that there were opportunities for me in SWE and SET. I ended up choosing the SET role after speaking with the SET hiring manager. He said two things that convinced me. First, SETs spend as much time coding as SWEs, and I wanted a role where I could write code. Second, the SETs job is to creatively solve testing problems, which sounded more interesting to me than writing features for a product. This seemed like a really unique and appealing opportunity, so I took it!\n\nSo what exactly do SETs do?\nSETs are SWEs who are really into testing. We help SWEs design and refactor their code so that it is more testable. We work with test engineers (TEs) to figure out how to automate difficult test cases. We also write harnesses, frameworks and tools to make test automation possible. SETs tend to have the best understanding of how everything plays together (production code, manual tests, automated tests, tools, etc.) and we have to make that information accessible to everyone on the team.\n\nWhat project do you work on?\nI work on the Google Cloud Print team. Our goal is to make it possible to print anywhere from any device. You can use Google Cloud Print to connect home and work printers to the web so that you (and anyone you share your printers with) can access them from your phone, tablet, Chromebook, PC or any other supported web-connected device.\n\nWhat advice would you give to aspiring SETs?\nFirst, for computer science majors in general: if there\u2019s any other field about which you are passionate, at least minor in it. CS is wonderfully chameleonic in that it can be applied to anything. So if, for example, you love art history, minor in art and you can write software to help restore images of old paintings.\n\nFor aspiring SETs, challenge yourself to write tests for all of the code you write for school. If you can get an internship where you have access to a real-world code base, study how that company approaches testing their code. If it\u2019s well-tested, see how they did it. If it\u2019s not well-tested, think about how you would test it. I don\u2019t (personally) know of a CS program that has even a full course based on testing, so you\u2019ll have to teach yourself. Start by looking up buzzwords like \u201cunit test\u201d and \u201ctest-driven development.\u201d Look up the different types of tests (unit, integration, component, system, etc.). Find a code coverage tool (if a free/cheap one is available for your language of choice) and see how well you\u2019re covering your code with your tests. Write a tool that will run all of your tests every time you build your code. If all of this sounds like fun...well...we need more people like you!\u00a0\n\nIf you\u2019re interested in applying for a Software Engineer in Test position, please apply for our general Software Engineer position, then indicate in your resume objective line that you\u2019re interested in the SET role.\n\nPosted by Jessica Safir, University Programs", "By Anthony Vallone\n\n\n\nWow... it has been a long time since we\u2019ve posted to the blog. This past year has been a whirlwind of change for many test teams as Google has restructured leadership with a focus on products. Now that the dust has settled, our teams are leaner, more focused, and more effective. We have learned quite a bit over the past year about how best to tackle and manage test problems at monumental scale. The next generation of test teams at Google are looking forward to sharing all that we have learned. Stay tuned for a revived Google Testing Blog that will provide deep insight into our latest testing technologies and strategies.", "By Jason Arbon\n\n\n\nAt GTAC, folks asked how well the Record/Playback (RPF) works in the Browser Integrated Test Environment (BITE). We were originally skeptical ourselves, but figured somebody should try. Here is some anecdotal data and some background on how we started measuring the quality of RPF.\nThe idea is to just let users use the application in the browser, record their actions, and save them as a javascript to play back as a regression test or repro later. Like most test tools, especially code generating ones, it works most of the time but its not perfect. Po Hu had an early version working, and decided to test this out on a real world product. Po, the developer of RPF, worked with the chrome web store team to see how an early version would work for them. Why chrome web store? It is a website with lots of data-driven UX, authentication, file upload, and it was changing all the time and breaking existing Selenium scripts: a pretty hard web testing problem, only targeted the chrome browser, and most importantly they were sitting 20 feet from us. \n\nBefore sharing with the chrome web store test developer Wensi Liu, we invested a bit of time in doing something we thought was clever: fuzzy matching and inline updating of the test scripts. Selenium rocks, but after an initial regression suite is created, many teams end up spending a lot of time simply maintaining their Selenium tests as the products constantly change. Rather than simply fail like the existing Selenium automation would do when a certain element isn\u2019t found, and require some manual DOM inspection, updating the Java code and re-deploying, re-running, re-reviewing the test code what if the test script just kept running and updates to the code could be as simple as point and click? We would keep track of all the attributes in the element recorded, and when executing we would calculate the percent match between the recorded attributes and values and those found while running. If the match isn\u2019t exact, but within tolerances (say only its parent node or class attribute had changed), we would log a warning and keep executing the test case. If the next test steps appeared to be working as well, the tests would keep executing during test passes only log warnings, or if in debug mode, they would pause and allow for a quick update of the matching rule with point and click via the BITE UI. We figured this might reduce the number of false-positive test failures and make updating them much quicker.\n\nWe were wrong, but in a good way!\n\nWe talked to the tester after a few days of leaving him alone with RPF. He\u2019d already re-created most of his Selenium suite of tests in RPF, and the tests were already breaking because of product changes (its a tough life for a tester at google to keep up with the developers rate of change). He seemed happy, so we asked him how this new fuzzy matching fanciness was working, or not. Wensi was like \u201coh yeah, that? Don\u2019t know. Didn\u2019t really use it...\u201d. We started to think how our update UX could have been confusing or not discoverable, or broken. Instead, Wensi said that when a test broke, it was just far easier to re-record the script. He had to re-test the product anyway, so why not turn recording on when he manually verified things were still working, remove the old test and save this newly recorded script for replay later? \n\nDuring that first week of trying out RPF, Wensi found:\n\n77% of the features in Webstore were testable by RPF\nGenerating regression test scripts via this early version of RPF was about 8X faster than building them via Selenium/WebDriver\nThe RPF scripts caught 6 functional regressions and many more intermittent server failures.\nCommon setup routines like login should be saved as modules for reuse (a crude version of this was working soon after)\nRPF worked on Chrome OS, where Selenium by definition could never run as it required client-side binaries. RPF worked because it was a pure cloud solution, running entirely within the browser, communicating with a backend on the web.\nBugs filed via bite, provided a simple link, which would install BITE on the developers machine and re-execute the repros on their side. No need for manually crafted repro steps. This was cool.\nWensi wished RPF was cross browser. It only worked in Chrome, but people did occasionally visit the site with a non-Chrome browser.\n\nSo, we knew we were onto something interesting and continued development. In the near term though, chrome web store testing went back to using Selenium because that final 23% of features required some local Java code to handle file upload and secure checkout scenarios. In hindsight, a little testability work on the server could have solved this with some AJAX calls from the client.\n\nWe performed a check of how RPF faired on some of the top sites of the web. This is shared on the BITE project wiki. This is now a little bit out of date, with lots more fixes, but it gives you a feel for what doesn\u2019t work. Consider it Alpha quality at this point. It works for most scenarios, but there are still some serious corner cases. \n\nJoe Muharsky drove a lot of the UX (user experience) design for BITE to turn our original and clunky developer and functional-centric UX into something intuitive. Joe\u2019s key focus was to keep the UX out of the way until it is needed, and make things as self-discoverable and findable as possible. We\u2019ve haven't done formal usability studies yet, but have done several experiments with external crowd testers using these tools, with minimal instructions, as well as internal dogfooders filing bugs against Google Maps with little confusion. Some of the fancier parts of RPF have some hidden easter eggs of awkwardness, but the basic record and playback scenarios seem to be obvious to folks.\n\nRPF has graduated from the experimental centralized test team to be a formal part of the Chrome team, and used regularly for regression test passes. The team also has an eye on enabling non-coding crowd sourced testers generate regression scripts via BITE/RPF.\n\nPlease join us in maintaining BITE/RPF, and be nice to Po Hu and Joel Hynoski who are driving this work forward within Google.", "By James Whittaker\n\nAll the GTAC 2011 talks are now available at\u00a0http://www.gtac.biz/talks\u00a0and also up on You Tube. A hearty thanks to all the speakers who helped make this the best GTAC ever.\u00a0\n\nEnjoy!", "By Ekaterina Kamenskaya, Software Engineer in Test, YouTube\n\n\n\nToday we introduce the Javascript coverage analysis tool, ScriptCover. It is a Chrome extension that provides line-by-line Javascript code coverage statistics for web pages in real time without any user modifications required. The results are collected both when the page loads and as users interact with it. The tool reports details about total web page coverage and for each external/internal script, as well as annotated code sources with individually highlighted executed lines.\n\n\n\n\nShort report in Chrome extension\u2019s popup, detailing both overall scores and per-script coverage. \n\n\nMain features:\n\nReport current and previous total Javascript coverage percentages and total number of instrumented code instructions.\nReport Javascript coverage per individual instruction for each internal and external script.\nDisplay detailed reports with annotated Javascript source code.\nRecalculate coverage statistics while loading the page and on user actions.\n\n\n\n\nSample of annotated source code from detailed report. First two columns are line number and number of times each instruction has been executed.\n\nHere are the benefits of ScriptCover over other existing tools:\n\nPer instructions coverage for external and internal scripts: The tool formats original external and internal Javascript code from \u2018<script>\u2019 tags to ideally place one instruction per line and then calculates and displays Javascript coverage statistics. It is useful even when the code is compressed to one line.\n\n\n\nDynamic: Users can get updated Javascript coverage statistics while the web page is loading and while interacting with the page.\n\n\n\nEasy to use: Users with different levels of expertise can install and use the tool to analyse coverage. Additionally, there is no need to write tests, modify the web application\u2019s code, save the inspected web page locally, manually change proxy settings, etc. When the extension is activated in a Chrome browser, users just navigate through web pages and get coverage statistics on the fly.\n\n\n\nIt\u2019s free and open source!\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nWant to try it out? Install ScriptCover and let us know what you think.\n\nWe envision many potential features and improvements for ScriptCover. If you are passionate about code coverage, read our documentation and participate in discussion group. Your contributions to the project\u2019s design, code base and feature requests are welcome!", "By Jim Reardon\n\nThe test plan is dead!\n\nWell, hopefully. \u00a0At a STAR West session this past week, James Whittaker asked a group of test professionals about test plans. \u00a0His first question: \u201cHow many people here write test plans?\u201d \u00a0About 80 hands shot up instantly, a vast majority of the room. \u00a0\u201cHow many of you get value or refer to them again after a week?\u201d \u00a0Exactly three people raised their hands.\n\nThat\u2019s a lot of time being spent writing documents that are often long-winded, full of paragraphs of details on a project everyone already knows to get abandoned so quickly.\n\nA group of us at Google set about creating a methodology that can replace a test plan -- it needed to be comprehensive, quick, actionable, and have sustained value to a project. \u00a0In the past few weeks, James has posted a few blogs about this methodology, which we\u2019ve called ACC. \u00a0It's a tool to break down a software product into its constituent parts, and the method by which we created \"10 Minute Test Plans\" (that only take 30 minutes!)\n\nComprehensive\nThe ACC methodology creates a matrix that describes your project completely; several projects that have used it internally at Google have found coverage areas that were missing in their conventional test plans.\n\nQuick\nThe ACC methodology is fast; we\u2019ve created ACC breakdowns for complex projects in under half an hour. \u00a0Far faster than writing a conventional test plan.\n\nActionable\nAs part of your ACC breakdown, risk is assessed to the capabilities of your appliciation. \u00a0Using these values, you get a heat map of your project, showing the areas with the highest risk -- great places to spend some quality time testing.\n\nSustained Value\nWe\u2019ve built in some experimental features that bring your ACC test plan to life by importing data signals like bugs and test coverage that quantify the risk across your project.\n\nToday, I'm happy to announce we're open sourcing Test Analytics, a tool built at Google to make generating an ACC simple -- and which brings some experimental ideas we had around the field of risk-based testing that work hand-in-hand with the ACC breakdown.\n\n\n\n\n\n\nDefining a project\u2019s ACC model.\n\nTest Analytics has two main parts: first and foremost, it's a step-by-step tool to create an ACC matrix that's faster and much simpler than the Google Spreadsheets we used before the tool existed. \u00a0It also provides visualizations of the matrix and risks associated with your ACC Capabilities that were difficult or impossible to do in a simple spreadsheet.\n\n\n\n\n\n\nA project\u2019s Capabilities grid.\n\nThe second part is taking the ACC plan and making it a living, automatic-updating risk matrix. \u00a0Test Analytics does this by importing quality signals from your project: Bugs, Test Cases, Test Results, and Code Changes. \u00a0By importing these data, Test Analytics lets you visualize risk that isn't just estimated or guessed, but based on quantitative values. \u00a0If a Component or Capability in your project has had a lot of code change or many bugs are still open or not verified as working, the risk in that area is higher. \u00a0Test Results can provide a mitigation to those risks -- if you run tests and import passing results, the risk in an area gets lower as you test.\n\n\n\n\n\n\nA project\u2019s risk, calculated as a factor of inherent risk as well as imported quality signals.\n\nThis part's still experimental; we're playing around with how we calculate risk based on these signals to best determine risk. \u00a0However, we wanted to release this functionality early so we can get feedback from the testing community on how well it works for teams so we can iterate and make the tool even more useful. \u00a0It'd also be great to import even more quality signals: code complexity, static code analysis, code coverage, external user feedback and more are all ideas we've had that could add an even higher level of dynamic data to your test plan.\n\n\n\n\n\n\nAn overview of test results, bugs, and code changes attributed to a project\u2019s capability. \u00a0The Capability\u2019s total risk is affected by these factors. \n\nYou can check out a live hosted version, browse or check out the code along with documentation, and of course if you have any feedback let us know - there's a Google Group set up for discussion, where we'll be active in responding to questions and sharing our experiences with Test Analytics so far.\n\nLong live the test plan!", "By Aaron Jacobs\n\n\n\nGoogle JS Test\u00a0is a JavaScript unit testing framework that runs on the\u00a0V8 JavaScript Engine, the same open source project that is responsible for Google Chrome\u2019s super-fast JS execution speed. Google JS Test is used internally by several Google projects, and we\u2019re pleased to announce that it has been released as an open source project.\n\nFeatures of Google JS Test include:\n\nExtremely fast startup and execution time, without needing to run a browser.\n\n\nClean, readable output in the case of both passing and failing tests.\n\n\nAn optional browser-based test runner that can simply be refreshed whenever JS is changed.\n\n\nStyle and semantics that resemble\u00a0Google Test\u00a0for C++.\n\n\nA built-in mocking framework that requires minimal boilerplate code (e.g. no\u00a0$tearDown\u00a0or$verifyAll\u00a0calls), with style and semantics based on the\u00a0Google C++ Mocking Framework.\n\n\nA system of\u00a0matchers\u00a0allowing for expressive tests and easy to read failure output, with many built-in matchers and the ability for the user to add their own.\n\nSee the Google JS Test\u00a0project home page\u00a0for a quick introduction, and the\u00a0getting started\u00a0page for a tutorial that will teach you the basics in just a few minutes.", "In a time when more and more of the web is becoming streamlined, the process of filing bugs for websites remains tedious and manual. Find an issue. Switch to your bug system window. Fill out boilerplate descriptions of the problem. Switch back to the browser, take a screenshot, attach it to the issue. Type some more descriptions.  The whole process is one of context switching; from the tools used to file the bug, to gather information about it, to highlight problematic areas, most of your focus as the tester is pulled away from the very application you\u2019re trying to test.\n\nThe Browser Integrated Testing Environment, or BITE, is an open source Chrome Extension which aims to fix the manual web testing experience. To use the extension, it must be linked to a server providing information about bugs and tests in your system. BITE then provides the ability to file bugs from the context of a website, using relevant templates.\n\n\n\nWhen filing a bug, BITE automatically grabs screenshots, links, and problematic UI elements and attaches them to the bug.  This gives developers charged with investigating and/or fixing the bug a wealth of information to help them determine root causes and factors in the behavior.\n\n\n\nWhen it comes to reproducing a bug, testers will often labor to remember and accurately record the exact steps taken.  With BITE, however, every action the tester takes on the page is recorded in JavaScript, and can be played back later.  This enables engineers to quickly determine if the steps of a bug repro in a specific environment, or whether a code change has resolved the issue.\n\nAlso included in BITE is a Record/Playback console to automate user actions in a manual test.  Like the BITE recording experience, the RPF console will automatically author javascript that can be used to replay your actions at a later date.  And BITE\u2019s record and playback mechanism is fault tolerant; UI automation tests will fail from time to time, and when they do, it tends to be for test issues, rather than product issues.  To that end, when a BITE playback fails, the tester can fix their recording in real-time, just by repeating the action on the page.  There\u2019s no need to touch code, or report a failing test; if your script can\u2019t find a button to click on, just click on it again, and the script will be fixed!  For those times when you do have to touch the code, we\u2019ve used the Ace (http://ace.ajax.org/) as an inline editor, so you can make changes to your javascript in real-time.\n\nCheck out the BITE project page at http://code.google.com/p/bite-project. Feedback is welcome at bite-feedback@google.com.  Posted by Joe Allan Muharsky from the Web Testing Technologies Team (Jason Stredwick, Julie Ralph, Po Hu and Richard Bustamante are the members of the team that delivered the product).", "By Richard Bustamante\n\nAre you a website developer that wants to know if Chrome updates will break \nyour website before they reach the stable release channel? Have you ever wished \nthere was an easy way to compare how your website appears in all channels of \nChrome? Now you can!\n\nQualityBots is a new open source tool for web developers \ncreated by the Web Testing team at Google. It\u2019s a comparison tool that examines \nweb pages across different Chrome channels using pixel-based DOM analysis. As \nnew versions of Chrome are pushed, QualityBots serves as an early warning system \nfor breakages. Additionally, it helps developers quickly and easily understand \nhow their pages appear across Chrome channels.\n\n\n\n\nQualityBots \nis built on top of Google AppEngine for the frontend and Amazon EC2 for the \nbackend workers that crawl the web pages. Using QualityBots requires an Amazon \nEC2 account to run the virtual machines that will crawl public web pages with \ndifferent versions of Chrome. The tool provides a web frontend where users can \nlog on and request URLs that they want to crawl, see the results from the latest \nrun on a dashboard, and drill down to get detailed information about what \nelements on the page are causing the trouble.\n\nDevelopers and testers can \nuse these results to identify sites that need attention due to a high amount of \nchange and to highlight the pages that can be safely ignored when they render \nidentically across Chrome channels. This saves time and the need for tedious \ncompatibility testing of sites when nothing has changed.\n\n\n\nWe \nhope that interested website developers will take a deeper look and even join \nthe project at the QualityBots project page. Feedback is more than welcome at \nqualitybots-discuss@googlegroups.com.\nPosted by Ibrahim El Far, Web Testing Technologies\u00a0Team (Eriel Thomas, Jason Stredwick, Richard Bustamante, and Tejas Shah are the members of the team that delivered this product)", "By James Whittaker\n\nThe GTAC agenda is now finalized and available at:\u00a0http://www.gtac.biz/agenda. Looking forward to seeing everyone there. Stay tuned to this blog for updates to any pre- and post- events.", "By James Whittaker\n\nAnything in software development that takes ten minutes or less to perform is either trivial or is not worth doing in the first place. If you take this rule of thumb at face value, where do you place test planning? Certainly it takes more than 10 minutes. In my capacity as Test Director at Google I presided over teams that wrote a large number of test plans and every time I asked how long one would take I was told \u201ctomorrow\u201d or \u201cthe end of the week\u201d and a few times, early in the day, I was promised one \u201cby the end of the day.\u201d So I\u2019ll establish the task of test planning to be of the hours-to-days duration.\n\nAs to whether it is worth doing, well, that is another story entirely. Every time I look at any of the dozens of test plans my teams have written, I see dead test plans. Plans written, reviewed, referred to a few times and then cast aside as the project moves in directions not documented in the plan. This begs the question: if a plan isn\u2019t worth bothering to update, is it worth creating in the first place?\n\nOther times a plan is discarded because it went into too much detail or too little; still others because it provided value only in starting a test effort and not in the ongoing work. Again, if this is the case, was the plan worth the cost of creating it given its limited and diminishing value?\n\nSome test plans document simple truths that likely didn\u2019t really need documenting at all or provide detailed information that isn\u2019t relevant to the day to day job of a software tester. In all these cases we are wasting effort. Let\u2019s face facts here: there is a problem with the process and content of test plans.\n\nTo combat this, I came up with a simple task for my teams: write a test plan in 10 minutes. The idea is simple, if test plans have any value at all then let\u2019s get to that value as quickly as possible.\n\nGiven ten minutes, there is clearly no room for fluff. It is a time period so compressed that every second must be spent doing something useful or any hope you have of actually finishing the task is gone. This was the entire intent behind the exercise from my point of view: boil test planning down to only the essentials and cut all fat and fluff. Do only what is absolutely necessary and leave the details to the test executors as opposed to the test planners. If I wanted to end the practice of writing test plans that don\u2019t stand the test of time, this seemed a worthwhile exercise.\n\nHowever, I didn\u2019t tell the people in the experiment any of this. I told them only: here is an app, create a test plan in 10 minutes or less. Remember that these people work for me and, technically, are paid to do as I tell them. And, again\u00a0technically\u00a0I am uniquely positioned to begin termination procedures with respect to their Google employment. On top of that I am presuming they have some measure of respect for me, which means they were likely convinced I actually thought they could do it. This was important to me. I wanted them to expect to succeed!\n\nAs preparation they could spend some time with the app in question and familiarize themselves with it. However, since many of the apps we used (Google Docs, App Engine, Talk Video, etc.) were tools they used every week, this time was short.\n\nSo here's how the task progressed:\n\nThey started, did some work and when ten minutes passed I interrupted them. They stated they weren't done yet. I responded by telling them they were out of time, nice try, here's a different problem to work on. 10 minutes later, the same thing happened and I changed the problem again. They began working faster and trying different angles, things that were too time consuming or not worth the effort got jettisoned really quick!\n\nIn each case, the teams came up with techniques that helped speed things along. They chose to jot down lists and create grids over writing long paragraphs of prose. Sentences \u2026 yes, paragraphs \u2026 no. They wasted little time on formatting and explanations and chose instead to document capabilities. Indeed, capabilities or\u00a0what the software actually does, were the one commonality of all the plans. Capabilities were the one thing that all the teams gravitated toward as the most useful way to spend the little time they were given.\n\nThe three things that emerged as most important:\n\n1.\u00a0Attributes\u00a0the adverbs and adjectives that describe the high level concepts testing is meant to ensure. Attributes such as fast, usable, secure, accessible and so forth.\n\n2.\u00a0Components\u00a0the nouns that define the major code chunks that comprise the product. These are classes, module names and features of the application.\n\n3.\u00a0Capabilities\u00a0the verbs that describe user actions and activities.\n\nNone of the teams finished the experiment in the 10 minutes allotted. However, in 10 minutes they were all able to get through both the Attributes and Components (or things that served a similar purpose) and begin documenting Capabilities. At the end of an additional 20 minutes most of the experiments had a large enough set of Capabilities that it would have been a useful starting point for creating user stories or test cases.\n\nWhich, at least to me, made the experiment a success. I gave them 10 minutes and hoped for an hour. They had 80% of the work complete in 30 minutes. And really isn\u2019t 80% enough? We know full well that we are not going to test everything so why document everything? We know full well that as we start testing, things (schedules, requirements, architecture, etc.) are going to change so insisting on planning precision when nothing else obeys such a calling for completeness seems out of touch with reality.\n\n80% complete in 30 minutes or less. Now that\u2019s what I call a 10 minute test plan!", "By James Whittaker\n\nGoogle Developer Day is gearing up for a fantastic fall season of tours that crawl the continents. And a surprise this year ... yours truly will be the keynote for the Developer Day in Sao Paulo Brazil and Buenos Aires Argentina in September. \n\nGoogle Developer Day is a deep dive into the future of Web, Mobile and Cloud technologies crafted specifically for software engineering professionals. And this year we are adding the element of Social to tie it all together. Google+ is only the start. \n\nIf you are attending, please stop by and say hello!\n\nClick here for more information about dates and agenda.", "We've completed the agenda for GTAC 2011 and are in the process of notifying accepted speakers and attendees. Once we have firm accepts we'll be publicizing the agenda.", "Have you ever poured your heart and soul and blood, sweat and tears to help test and perfect a product that, after launch, flopped miserably?  Not because it was not working right (you tested the snot out of it), but because it was not the right product.\n\nAre you currently wasting your time testing a new product or feature that, in the end, nobody will use?\n\n\nTesting typically revolves around making sure that we have built something right.  Testing activities can be roughly described as \u201cverifying that something works as intended, or as specified.\u201d  This is critical.  However, before we take steps and invest time and effort to make sure that something built right, we should make sure that the thing we are testing, whether its a new feature or a whole new product, is the right thing to build in the first place.\n\n\nSpending time, money and effort to test something that nobody ends up using is a waste of time.\n\n\nFor the past couple of years, I\u2019ve been thinking about, and working on, a concept called pretotyping.\n\n\nWhat is pretotyping?  Here\u2019s a somewhat formal definition \u2013 the dry and boring kind you\u2019d find in a dictionary:\n\n\nPretotyping [pree-tuh-tahy-ping], verb: Testing the initial appeal and actual usage of a potential new product by simulating its core experience with the smallest possible investment of time and money.\n\n\nHere\u2019s a less formal definition: \n\n\nPretotyping is a way to test an idea quickly and inexpensively by creating extremely simplified, mocked or virtual versions of that product to help validate the premise that \"If we build it, they will use it.\"\n\n\nMy favorite definition of pretotyping, however, is this:\n\n\nMake sure \u2013 as quickly and as cheaply as you can \u2013 that you are building the right it before you build it right.\n\n\nMy thinking on pretotyping evolved from my positive experiences with Agile and Test Driven Development.  Pretotyping applies some of the core ideas from these two models and applies them further upstream in the development cycle.\n\n\nI\u2019ve just finished writing the first draft of a booklet on pretotyping called \u201cPretotype It\u201d.\n\n\nYou can download a PDF of the booklet from Google Docs or Scribd.\n\n\nThe \"Pretotype It\" booklet is itself a pretotype and test.  I wrote this first-draft to test my (possibly optimistic) assumption that people would be interested in it, so please let me know what you think of it.\n\n\nYou can follow my pretotyping work on my pretotyping blog.\n\n\nPost contentPosted by Alberto Savoia", "By James Whittaker\n\nThe call for proposals and participation is now closed. Over the next few weeks we will be announcing the full agenda and notifying accepted participants. In the meantime, the keynote lineup is now locked. It consists of two famous Googlers and two famous external speakers that I am very pleased to have join us.\n\nOpening Keynote: Test is Dead by Alberto Savoia\n\nThe way most software is designed, developed and launched has changed dramatically over the last decade \u2013 but what about testing?  Alberto Savoia believes that software testing as we knew it is dead \u2013 or at least moribund \u2013 in which case we should stick a fork in it and proactively take it out of its misery for good.  In this opening keynote of biblical scope, Alberto will cast stones at the old test-mentality and will try his darnedest to agitate you and convince you that these days most testers should follow a new test-mentality, one which includes shifting their focus and priority from \u201cAre we building it right?\u201d to \u201cAre we building the right it?\u201d  The subtitle of this year\u2019s GTAC is \u201ccloudy with a chance of tests,\u201d and if anyone can gather the clouds into a hurricane, it's Alberto \u2013 it might be wise to bring your umbrella.\n\nAlberto Savoia is Director of Engineering and Innovation Agitator at Google. In addition to leading several major product development efforts (including the launch of Google AdWords), Alberto has been a lifelong believer, champion, innovator and entrepreneur in the area of developer testing and test automation tools.  He is a frequent keynote speaker and the author of many articles on testing, including the classic booklet \u201cThe Way of Testivus\u201d and \u201cBeautiful Tests\u201d in O\u2019Reilly\u2019s Beautiful Code.  His work in software development tools has won him several awards including the 2005 Wall Street Journal Technical Innovator Award, InfoWorld\u2019s Technology of the Year award, and no less than four Software Development Magazine Jolt Awards.\n\nDay 1 Closer: Redefining Security Vulnerabilities: How Attackers See Bugs by Herbert H. Thompson\n\nDevelopers see features, testers see bugs, and attackers see \u201copportunities.\u201d Those opportunities are expanding beyond buffer overflows, cross site scripting, etc. into logical bugs (and features) that allow attackers to use the information they find to exploit trusting users. For example, attackers can leverage a small information disclosure issue in an elaborate phishing attempt. When you add people in the mix, we need to reevaluate which \u201cbugs\u201d are actual security vulnerabilities. This talk is loaded with real world examples of how attackers are using software \u201cfeatures\u201d and information tidbits (many of which come from bugs) to exploit the biggest weakness of all: trusting users.\n\nDr. Herbert H. Thompson is Chief Security Strategist at People Security and a world-renown expert in application security. He has co-authored four books on the topic including, How to Break Software Security: Effective Techniques for Security Testing (with Dr. James Whittaker) and The Software Vulnerability Guide (with Scott Chase). In 2006 he was named one of the \u201cTop 5 Most Influential Thinkers in IT Security\u201d by SC Magazine. Thompson continually lends his perspective and expertise on secure software development and has been interviewed by top news organizations including CNN, MSNBC, BusinessWeek, Forbes, Associated Press, and the Washington Post.  He is also Program Committee Chair for RSA Conference, the world\u2019s leading information security gathering. He holds a Ph.D. in Applied Mathematics from Florida Institute of Technology, and is an adjunct professor in the Computer Science department at Columbia University in New York.\n\nDay 2 Opener: Engineering Productivity: Accelerating Google Since 2006 by Patrick Copeland\n\nPatrick Copeland is the founder and architect of Google's testing and productivity strategy and in this \"mini keynote\" he tells the story and relates the pain of taking a company from ad hoc testing practices to the pinnacle of what can be accomplished with a well oiled test engineering discipline. \n\nConference Closer: Secrets of World-Class Software Organizations by Steve McConnell\n\nConstrux consultants work with literally hundreds of software organizations each year. Among these organizations a few stand out as being truly world class. They are exceptional in their ability to meet their software development goals and exceptional in the contribution they make to their companies' overall business success. Do world class software organizations operate differently than average organizations? In Construx's experience, the answer is a resounding \"YES.\" In this talk, award-winning author Steve McConnell reveals the technical, management, business, and cultural secrets that make a software organization world class.\n\nSteve McConnell is CEO and Chief Software Engineer at Construx Software where he consults to a broad range of industries, teaches seminars, and oversees Construx\u2019s software engineering practices. Steve is the author of Software Estimation: Demystifying the Black Art (2006), Code Complete (1993, 2004), Rapid Development (1996), Software Project Survival Guide (1998), and Professional Software Development (2004), as well as numerous technical articles. His books have won numerous awards for \"Best Book of the Year,\" and readers of Software Development magazine named him one of the three most influential people in the software industry along with Bill Gates and Linus Torvalds.", "By Jason Arbon and Tejas Shah\n\nGoogle Instant Pages are a cool new way that Google speeds up your search experience.  When Google thinks it knows which result you are likely to click, it preloads that page in the background, so when you click the page it renders instantly, saving the user about 5 seconds.  5 seconds is significant when you think of how many searches are performed each day--and especially when you consider that the rest of the search experience is optimized for sub-second performance.\n\nThe testing problem here is interesting. This feature requires client and server coordination, and since we are pre-loading and rendering the pages in an invisible background page, we wanted to make sure that nothing major was broken with the page rendering.\n\nThe original idea was for developers to test out a few pages as they went.But, this doesn\u2019t scale to a large number of sites and is very expensive to repeat. Also, how do you know what the pages should look like? To write Selenium tests to functionally validate thousands of sites would take forever--the product would ship first. The solution was to perform automated test runs that load these pages from search results with Instant Pages turned on, and another run with Instant Pages turned off. The page renderings from each run were then compared.\n\nHow did we compare the two runs? How to compare pages when content and ads on web pages are constantly changing and we don't know what the expected behavior is? We could have used cached versions of these pages, but that wouldn\u2019t be the realworld experience we were testing and would take time setting up, and the timing would have been different. We opted to leverage some other work that compares pages using the Document Object Model (DOM). We automatically scan each page, pixel by pixel, but look at what element is visible at the point on the page, not the color/RGB values. We then do a simple measure of how closely these pixel measurements match. These so-called \"quality bots\" generate a score of 0-100%, where 100% means all measurements were identical.\n\nWhen we performed the runs, the vast majority (~95%) of all comparisons were almost identical, like we hoped. Where the pages where different we built a web page that showed the differences between the two pages by rendering both images and highlighting the difference. It was quick and easy for the developers to visually verify that the differences were only due to content or other non-structural differences in the rendering.  Anytime test automation scales, is repeatable, quantified, and developers can validate the results without us is a good thing!\n\nHow did this testing get organized? As with many things in testing at Google, it came down to people chatting and realizing their work can be helpful for other engineers.  This was bottom up, not top down. Tejas Shah was working on a general quality bot solution for compatibility (more on that in later posts) between Chrome and other browsers. He chatted with the Instant Pages developers when he was visiting their building and they agreed his bot might be able to help. He then spend the next couple of weeks pulling it all together and sharing the results with the team. \n\nAnd now more applications of the quality bot are surfacing. What if we kept the browser version fixed, and only varied the version of the application? Could this help validate web applications independent of a functional spec and without custom validation script development and maintenance?  Stay tuned...", "Attending conferences can be a great way to network and learn new concepts. However, taking those concepts back to your office and trying to convince your team apply them can be daunting. In order to make GTAC attendees more successful at implementing what they learn at this conference we are going to give preference to teammates from the same company applying for attendance. Bring another developer or tester (or two or three) and attend as a team so you can discuss what you learn and experience, hopefully increasing your chances of putting it into practice when you return to work.\n\nWe're extending the deadline for attendees until the end of July to give you a chance to round up some teammates.", "By James Whittaker\n\nSTAR West will feature something unprecedented this year: back-to-back tutorials by Googlers plus a keynote and track session.\n\nThe tutorials will be Monday October 3. I have the morning session on \"How Google Tests Software\" and my colleague Ankit Mehta has the afternoon session on \"Testing Rich Internet AJAX-based Applications.\" You can spend the whole day in Google Test Land.\n\nI highly recommend Ankit's tutorial. He is one of our top test managers and has spent years minding Gmail as it grew up from a simple cloud-based email system into the mass-scale, ubiquitous rich web app that it is today. Ankit now leads all testing efforts around our social offerings (which are already starting to appear). Anyone struggling to automate the testing of rich web apps will have plenty to absorb in his session. He's not spouting conjecture and generalities; he's speaking from the position of actual accomplishment. Bring a laptop. \n\nJason Arbon and Sebastian Schiavone are presenting a track talk on \"Google's New Methodology for Risk Driven Testing\" and will be demonstrating some of the latest tools coming out of Google Test Labs. Tools that were born of real need built to serve that need. I am expecting free samples! Jason was test lead for Chrome and Chrome OS before taking over Google Test Labs where incredibly clever code is woven into useful test tools. Sebastian is none other than my TPM (technical program manager) who is well known for taking my vague ideas about how things should be done and making them real. \n\nOh and the keynote, well that's me again, something about testing getting in the way of quality. I wrote this talk while I was in an especially melancholy mood about my place in the universe. It's a wake-up call to testers: the world is changing and your relevance is calling ... will you answer the call or ignore it and pretend that yesterday is still today?", "By James Whittaker\n\nIf your name is Larry Page, stop reading this now. \n\nLet me first admit that as I write this I am sitting in a company lounge reminiscent of a gathering room in a luxury hotel with my belly full of free gourmet food waiting for a meeting with the lighthearted title \"Beer and Demos\" to start. \n\nLet me secondly admit that none of this matters. It's all very nice, and I hope it continues in perpetuity, but it doesn't matter. Engineers don't need to be spoiled rotten to be happy. The spoiling of engineers has little to do with the essence of a 21st century tech career. \n\nNow, what exactly does matter? What is the essence of a 21st century tech career that keeps employees loyal and engaged with productivity that would shame the most seasoned agile-ist? I don't yet have the complete story, but here are three important ingredients:\n\nFailing Fast. Nothing destroys morale more than a death march. Projects going nowhere should do so with the utmost haste. The ability of a company to implode pet projects quickly correlates directly to a great place to work. Engineers working on these project gain not only valuable engineering experience, they experience first-hand the company's perception of what is important (and, in the case of their project, what is not important). It's a built-in lesson on company priorities and it ensures good engineers don't get monopolized by purposeless projects. You gotta like a company willing to experiment. You have to love a company willing to laugh at itself when the experiments don't pan out.\n\n20% Time. Any company worth working for has any number of projects that are worth working on. It's frustrating for many super-sharding engineers to see cool work going on down the hall or in the next building and not being part of it. A day job that takes all day is tiresome. Enter 20% time, a concept meant to send a strong message to all engineers: you always have a spare day. Use it wisely.\n\nProject Mobility. Staying fresh by changing projects is part of mobility. Continuous cycling of fresh ideas from new project members to existing projects is another part. The downside here is obviously projects with a steep learning curve but I scoff in the general direction of this idea. Whose fault is it when a wicked smart engineer can't learn the system fast enough to be useful in some (even a small) context? Only the weakest organization with the poorest documentation can use that excuse. The only good reason for keeping people on a project is because they have no desire to leave. \n\nThese three concepts are better than all the lounges and free food any company can provide. Here's an example, a real example, of how it worked recently for an employee I'll call Paul (because that happens to be his name!). \n\nPaul joined Google a little over a year ago and spent two months on a project that was then cancelled. He learned enough to be useful anywhere but was new enough that he really didn't have great context on what project he wanted next. Solution: I assigned him to a project that was a good skill set match.\n\nLess than a year later, his new project ships. He played an important role in making this happen but in that time he also realized that the role was leaning toward feature development and he was more interested in a pure test development role. However, he was steeped in post-ship duties and working on the next release. A cycle that, happily, can be broken pretty easily here. \n\nAnother project had a test developer opening that suited Paul perfectly. He immediately signed up for 20% on this new project and spent his 80% ramping down in his old project. At some point these percentages will trade places and he'll spend 20% of his time training his replacement on the old project. This is a friction-less process. His manager cannot deny him his day to do as he pleases and now he can spend his time getting off the critical path of his old project and onto the critical path of his new project. \n\nMobility means a constant stream of openings on projects inside Google. It also creates a population of engineering talent with an array of project experiences and a breadth of expertise to fill those positions. 20% time is a mechanism for moving onto and off of projects without formal permissions, interviews and other make-work processes engineers deplore. \n\nLet's face it, most benefits are transient. I enjoy a good meal for the time it is in front of me. I enjoy great medical when I am sick. I appreciate luxury when I have time for it. Even my paycheck comes with such monotonous regularity that it is an expectation that brings little joy apart from the brief moment my bank balance takes that joyful upward tick. But if I am unhappy the rest of the day, none of those islands of pampering mean squat. Empower me as an engineer during the much larger blocks of my time when I am doing engineering. Feed my creativity. Remove the barriers that prevent me from working on the things I want to work on. \n\nDo these things and you have me. Do these things and you make my entire work day better. This is the essence of a 21st century tech career: make the hours I spend working better. Anything more is so dot com. \n\nOk, Larry you can start reading again.", "By Radoslav Vasilev from Google Zurich\n\nEvery day modern web applications are becoming increasingly sophisticated, and as their complexity grows so does their attack surface. Previously we introduced open source tools such as Skipfish and Ratproxy to assist developers in understanding and securing these applications.\n\nAs existing tools focus mostly on testing server-side code, today we are happy to introduce DOM Snitch \u2014 an experimental* Chrome extension that enables developers and testers to identify insecure practices commonly found in client-side code. To do this, we have adopted several approaches to intercepting JavaScript calls to key and potentially dangerous browser infrastructure such as document.write or HTMLElement.innerHTML (among others). Once a JavaScript call has been intercepted, DOM Snitch records the document URL and a complete stack trace that will help assess if the intercepted call can lead to cross-site scripting, mixed content, insecure modifications to the same-origin policy for DOM access, or other client-side issues.\n\n\n\n\n\nHere are the benefits of DOM Snitch:\n\nReal-time: Developers can observe DOM modifications as they happen inside the browser without the need to step through JavaScript code with a debugger or pause the execution of their application.\n\nEasy to use: With built-in security heuristics and nested views, both advanced and less experienced developers and testers can quickly spot areas of the application being tested that need more attention.\n\nEasier collaboration: Enables developers to easily export and share captured DOM modifications while troubleshooting an issue with their peers.\n\n\n\nDOM Snitch is intended for use by developers, testers, and security researchers alike. Click here to download DOM Snitch. To read the documentation, please visit this page.\n\n*Developers and testers should be aware that DOM Snitch is currently experimental. We do not guarantee that it will work flawlessly for all web applications. More details on known issues can be found here or in the project\u2019s issues tracker.", "By James Whittaker\n\nI am pleased to confirm 3 of our keynote speakers for GTAC 2011 at the Computer History Museum in Mountain View CA.\n\nGoogle's own Alberto Savoia, aka Testivus.\n\nSteve McConnell the best selling author of Code Complete and CEO of Construx Software.\n\nAward winning speaker (\"the Jon Stewart of Software Security\") Hugh Thompson.\n\nThis is the start of an incredible lineup. Stay tuned for updates concerning their talks and continue to nominate additional speakers and keynotes. We're not done yet and we're taking nominations through mid July.\n\nIn addition to the keynotes, we're going to be giving updates on How Google Tests Software from teams across the company including Android, Chrome, Gmail, You Tube and many more.", "(Cross-posted from the Google Engineering Tools blog)\n \nBy Pooja Gupta, Mark Ivey and John Penix\n\n\n\nContinuous integration systems play a crucial role in keeping software working while it is being developed. The basic steps most continuous integration systems follow are:\n\n\n\n1. Get the latest copy of the code.\n\n2. Run all tests.\n\n3. Report results.\n\n4. Repeat 1-3.\n\nThis works great while the codebase is small, code flux is reasonable and tests are fast. As a codebase grows over time, the effectiveness of such a system decreases. As more code is added, each clean run takes much longer and more changes gets crammed into a single run. If something breaks, finding and backing out the bad change is a tedious and error prone task for development teams.\n\nSoftware development at Google is big and fast. The code base receives 20+ code changes per minute and 50% of the files change every month! Each product is developed and released from \u2018head\u2019 relying on automated tests verifying the product behavior. Release frequency varies from multiple times per day to once every few weeks, depending on the product team. \n\nWith such a huge, fast-moving codebase, it is possible for teams to get stuck spending a lot of time just keeping their build \u2018green\u2019. A continuous integration system should help by providing the exact change at which a test started failing, instead of a range of suspect changes or doing a lengthy binary-search for the offending change. To find the exact change that broke a test, we could run every test at every change, but that would be very expensive.\n\nTo solve this problem, we built a continuous integration system that uses dependency analysis to determine all the tests a change transitively affects and then runs only those tests for every change. The system is built on top of Google\u2019s cloud computing infrastructure enabling many builds to be executed concurrently, allowing the system to run affected tests as soon as a change is submitted.\n\nHere is an example where our system can provide faster and more precise feedback than a traditional continuous build. In this scenario, there are two tests and three changes that affect these tests. The gmail_server_tests are broken by the second change, however a typical continuous integration system will only be able to tell that either change #2 or change #3 caused this test to fail. By using concurrent builds, we can launch tests without waiting for the current build/test cycle to finish. Dependency analysis limits the number of tests executed for each change, so that in this example, the total number of test executions is the same as before.\n\n\n\n\n\n\n\n\n\n\n\nLet\u2019s look deeper into how we perform the dependency analysis. \n\nWe maintain an in-memory graph of coarse-grained dependencies between various tests and build rules across the entire codebase. This graph, several GBs in-memory, is kept up-to-date with each change that gets checked in. This allows us to transitively determine all tests that depend on the code modified in a given change and hence need to be re-run to know the current state of the build. Let\u2019s walk through an example.\n\nConsider two sample projects, each containing a different set of tests:\u00a0\n\n\n\n\n\n\n\n\nwhere the build dependency graph looks like this:\n\n\n\n\n\n\n\nWe will see how two isolated code changes, at different depths of the dependency tree, are analyzed to determine affected tests, that is the minimal set of tests that needs to be run to ensure that both Gmail and Buzz projects are \u201cgreen\u201d.\n\nCase1: Change in common library\n\n\n\n\nFor first scenario, consider a change that modifies files in common_collections_util.\n\n\n\n\n\n\n\n\nAs soon as this change is submitted, we start a breadth-first search to find all tests that depend on it.\n\n\n\n\n\n\n\n\n\n\n\nOnce all the direct dependencies are found, continue BFS to collect all transitive dependencies till we reach all the leaf nodes.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen done, we have all the tests that need to be run, and can calculate the projects that will need to update their overall status based on results from these tests.\n\n\n\n\n\n\n\n\n\n\n\n\nCase2: Change in a dependent project:\n\n\n\nWhen a change modifying files in youtube_client is submitted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe perform the same analysis to conclude that only buzz_client_tests is affected and status of Buzz project needs to be updated:\n\n\n\n\n\n\n\n\n\n\n\n\nThe example above illustrates how we optimize the number of tests run per change without sacrificing the accuracy of end results for a project. A lesser number of tests run per change allows us to run all affected tests for every change that gets checked in, making it easier for a developer to detect and deal with an offending change.\n\nUse of smart tools and cloud computing infrastructure in the continuous integration system makes it fast and reliable. While we are constantly working on making improvements to this system, thousands of Google projects are already using it to launch-and-iterate quickly and hence making faster user-visible progress.", "By James WhittakerThe Life of a TEThe Test Engineer is a newer role within Google than either SWEs or SETs. As such, it is a role still in the process of being defined. The current generation of Google TEs are blazing a trail which will guide the next generation of new hires for this role. It is the process that is emerging as the best within Google that we present here.Not all products require the services of a TE. Experimental efforts and early stage products without a well-defined mission or user story are certainly projects that won\u2019t get a lot of TE attention. If the product stands a good chance of being cancelled (in the sense that as a proof of concept it fails to pass muster) or has yet to engage users or have a well defined set of features, testing it is largely something that should be done by the people developing it.Even if it is clear that a product is going to get shipped, Test Engineers have little to do early in the development cycle when features are still in flux and the final feature list and scope is undetermined. Overinvesting in testing too early can mean a lot of things get thrown away. Likewise, early testing planning requires fewer test engineers than later cycle exploratory testing when the product is close to final form and the hunt for missed bugs has a greater urgency.The trick in staffing a project with Test Engineers has to do with risk and return on investment. Risk to the customer and to the enterprise means more testing effort and requires more TEs. But that effort needs to be in proportion to the potential return. We need the right number of TEs and we need them to engage at the right time and with the right impact.Once engaged, TEs do not have to start from scratch. There is a great deal of test engineering and quality-oriented work performed by SWEs and SETs which is the starting point for additional TE work. The initial engagement of the TE is to decide things such as:\u00b7       Where are the weak points in the software?\u00b7       What are the security, privacy, performance and reliability concerns?\u00b7       Do all the primary user scenarios work as expected? For all international audiences?\u00b7       Does the product interoperate with other products (hardware and software)?\u00b7       In the event of a problem, how good are the diagnostics?All of this combines to speak to the risk profile of releasing the software in question. TEs don\u2019t necessarily do all of this work, but they ensure that it gets done and they leverage the work of others is assessing where additional work is required. Ultimately, test engineers are paid to protect users and the business from bad design, confusing UX, functional bugs, security and privacy issues and so forth. At Google, TEs are the only people on a team whose full-time job is to look at the product or service holistically for weak points. As such, the life of a Test Engineer is much less prescriptive and formalized than that of an SET. TE\u2019s are asked to help on projects in all stages of readiness: everything from the idea stage to version 8, or even watching over a deprecated or \u201cmothballed\u201d project. Often, a single TE will even span multiple projects particularly those with specialty type skills like security.Obviously, the work of a TE varies greatly depending on the project. Some TE\u2019s spend much of their time programming, much like an SET, but with more of a focus on end-to-end user scenarios. Other TE's take existing code and designs determine failure modes and look for errors that will cause those failures. In such a role a TE might modify code but not create it from scratch. TE's must be more systematic and thorough in their test planning and completeness with a focus on the actual usage and system experience. TE's excel at dealing with ambiguity in requirements and at reasoning and communicating about fuzzy problems.Successful TEs accomplish all this while navigating the sensitivities and sometimes strong personalities of the development and product team members. When weak points are found, test engineers happily break the software, and drive to get these issues resolved with the SWEs, PMs, and SETs.Such a job description is a frightening prospect given the mix of technical skill, leadership, and deep product understanding and without proper guidance it is a role in which many would expect to fail. But at Google a strong community of test engineers has emerged to counter this. Of all job functions, the TE role is perhaps the best peer supported role in the company and the insight and leadership required to perform it successfully means that many of the top test managers in the company come from the TE ranks.There is a fluidity to the work of a Google Test Engineer that belies any prescriptive process for engagement. TE\u2019s can enter a project at any point and must assess the state of the project, code, design, and users quickly and decide what to focus on first. If the project is just getting started, test planning is often the first order of business. Sometimes TEs are pulled in late in the cycle to evaluate whether a project is ready for ship or if there are any major issues before an early \u2018beta\u2019 goes out. If they are brought into a newly acquired application or one in which they have little prior experience, they will often start doing some exploratory testing with little to no planning. Sometimes projects haven\u2019t been released for quite a while and just need some touchups/security fixes, or UX updates\u2014calling for an even different approach. One size rarely fits all for TEs at Google.", "By James WhittakerI am happy to announce that GTAC 2011 is now open for nominations. We're going to try and have an executive session, depending on interest, the afternoon/evening of October 25th at the Googleplex in Mountain View. This session is intended for top testing decision makers at top web, technology and software companies worldwide. It will be a chance for frank and open discussion about ours and the industry's collective challenges. It's intended to be a meeting of key decision makers and budget owners to share information, ideas and with a little luck spur some collaborations that will be good for the testing industry overall. Nominate your executive here. The general session is by invitation only and prospective attendees and speakers must register and be selected. Speaker nominees are encouraged to point us to videos of prior presentations and any other material to help make our decision easier. Please leave comments if there is some technology, tool or product you want to hear about so we end up with the best possible agenda. I hope to see a lot of our readers in Mountain View in October!", "By James WhittakerNew material for the this series is coming more slowly. I am beginning to get into areas where I want to start posting screen shots of internal Google tools and describe how our infrastructure works. This is material that takes longer to develop and also requires some scrutiny before being published externally. So in the meantime, I am pausing to answer some of the questions you've posted in the comments. I am going to start with Lilia (because she likes Neil Young mainly, but also because she can run further than me and those two things combine to impress me to no small end) who asks about SET-SWE conversion and vice-versa and which I have seen the most. There is also the broader question of whether there is a ceiling on the SET career path. SETs and SWEs are on the same pay scale and virtually the same job ladder. Both roles are essentially 100% coding roles with the former writing test code and the latter doing feature development. From a coding perspective the skill set is a dead match. From a testing perspective we expect a lot more from SETs. But the overlap on coding makes SETs a great fit for SWE positions and vice versa. Personally I think it is a very healthy situation to have conversions. Since I have both roles reporting to me I can speak from first hand experience that many of my best coders are former SETs and some of my best testers are former SWEs. Each is excellent training ground for the other. On my specific team I am even on the conversions from one role to the other. But I suspect that Google-wide there are more SETs who become SWEs. Why convert in the first place? Well at Google it isn't for the money. It also isn't for the prestige as we have a lot more SWEs than SETs and it is a lot harder to standout. The scarcity of our SETs creates somewhat of a mystique about these folk. Who are these rare creatures who keep our code bases healthy and make our development process run so smoothly? Actually, most SWEs care more about making the SETs happy so they continue doing what they do. Why would any dev team force a conversion of a great developer from SET to SWE when finding a suitable SET replacement is so much harder than adding another feature developer? SWEs ain't that stupid. Now pausing before I take another hit of the corp kool-aid, let me be honest and say that there are far more senior SWEs than SETs. Percentage wise we test folk are more outnumbered at the top of the org than at the middle and bottom. But keep in mind that developers have had a large head start on us. We have developers who have been at Google since our founding and testers ... well ... less time than that. Where do TEs fit into this mix? TE is an even newer role than SET but already we have a number climbing to the Staff ranks and pushing on the senior most positions in the company. There is no ceiling, but the journey to the top takes some time. Raghev among others has asked about the career path and whether remaining an IC (individual contributor) is an option over becoming a manager. I have mixed feelings about answering this. As a manager myself, I see the role as one with much honor and yet I hear in your collective voices a hint of why do I have to become a manager? Ok, I admit, Dilbert is funny. For me, being a manager is a chance to impart some of my experience and old-guy judgement on less experienced but more technically gifted ICs. The combination of an experienced manager's vision and an ICs technical skill can be a fighting force of incredible power. And yet, why should someone who does not want to manage be forced to do so in order to continue their career advancement? Well, fortunately, Google does not make us choose. Our managers are expected to have IC tasks they perform. They are expected to be engaged technically and lead as opposed to just manage. And our ICs are expected to have influence beyond their personal work area. When you get to the senior/staff positions here you are a leader, period. Some leaders lead more than they manage and some leaders manage more than they lead. But either way, the view from the top means that a lot of people are looking to you for direction ... whether you manage them or not.", "By James WhittakerThe Life of an SETSETs are Software Engineers in Test. They are software engineers who happen to write testing functionality. First and foremost, SETs are developers and the role is touted as a 100% coding role in our recruiting literature and internal job promotion ladders. When SET candidates are interviewed, the \u201ccoding bar\u201d is nearly identical to the SWE role with more emphasis that SETs know how to test the code they create. In other words, both SWEs and SETs answer coding questions. SETs are expected to nail a set of testing questions as well.As you might imagine, it is a difficult role to fill and it is entirely possible that the low numbers of SETs isn\u2019t because Google has created a magic formula for productivity but more of a result of adapting our engineering practice around the reality that the SET skill set is really hard to find. We optimize on this very important task and build processes around the people who do it. It is usually the case that SETs are not involved early in the design phase. Their exclusion is not so much purposeful as it is a by-product of how a lot of Google projects are born. A common scenario for new project creation is that some informal 20% effort takes a life of its own as an actual Google branded product. Gmail and Chrome OS are both projects that started out as ideas that were not formally mandated by Google but over time grew into shipping products with teams of developers and testers working on them. In such cases early development is not about quality, it is about proving out a concept and working on things like scale and performance that must be right before quality could even be an issue. If you can't build a web service that scales, testing is not your biggest problem! Once it is clear that a product can and will be built and shipped, that's when the development team seeks out test involvement.You can imagine a process like this: someone has an idea, they think about it, write experimental code, seek out opinions of others, write some more code, get others involved, write even more code, realize they are onto something important, write more code to mold the idea into something that they can present to others to get feedback ... somewhere in all this an actual project is created in Google's project database and the project becomes real. Testers don't get involved until it becomes real. Do all real projects get testers? Not by default. Smaller projects and those meant for limited users often get tested exclusively by the people who build it. Others that are riskier to our users or the enterprise (much more about risk later) get testing attention.The onus is on the development teams to solicit help from testers and convince them that their project is exciting and full of potential. Dev Directors explain their project, progress and ship schedule to Test Directors who then discuss how the testing burden is to be shared and agree on things like SWE involvement in testing, expected unit testing levels and how the duties of the release process are going to be shared. SETs may not be involved at project inception, but once the project becomes real we have vast influence over how it is to be executed.And when I say \"testing\" I don't just mean exercising code paths. Testers might not be involved from the beginning ... but testing is. In fact, an SET's impact is felt even before a developer manages to check code into the build. Stay tuned to understand what I am talking about.", "By James WhittakerHas it only been 179 days since the last GTAC? My how time flies when you have lots of testing to do!It is my pleasure to announce that not only is our collective attention being drawn back to this most intriguing test conference but yours truly, along with \"Shoeless\" Brad Green, have been asked to plan it. Given that Brad and I both spend a lot of time with our heads in the Cloud, this year's theme is cloudy with a chance of tests. The composite meaning of this theme is purposeful: Testing apps that reside in the Cloud is itself a cloudy, as in opaque, process. Clouds on the horizon often signal change and testing in the Cloud certainly changes things. The Cloud breaks old testing paradigms and tools requiring that even the tried-and-true be rethought and recast. And, yes, the future of testing itself is cloudy and the need for testers and testing as it exists today is unclear. There are clouds gathering on the horizons of the discipline and this conference will be dedicated to interpreting their meaning and planning for their arrival. As always, GTAC will attempt to bring together people who have thought deeply about these subjects and are responsible for actual progress, technology and insights that will benefit others in the community. We are still in the very earliest stages of planning, but here are the details as we know them:GTAC 2011 will be in Mountain View, CA the week of October 25th and will be held at the Computer History Museum. GTAC or no GTAC, this place is well worth a visit and as an actual conference venue just might be the coolest location for GTAC ever. As in the past, GTAC 2011 will feature a single track, all-keynote format with both internal and external speakers. We will be soliciting feedback from potential attendees about what topics and speakers are the most interesting. Our opening keynote has been determined already and it is none other than our most famous Alberto Savoia, translator of the ancient tome The Way of Testivus, and agitator extraordinaire with quotes like \"Building the right 'it' is more important than building 'it' right.\" If anyone can gather the clouds into a hurricane, it's Alberto. You can also expect updates on our open source test tools strategy. One important addition we seek to make this year is to have a Test Executive Session sometime during the event. The idea is to gather top decision makers and budget owners at the biggest/best/most influential web companies on the planet. It will be a discussion about testing culture, organizational structure, technology deployment, innovation and so forth by the very people who can make change happen within their company. Our readers will be asked to nominate their Directors, VPs, SVPs and so forth who own their companies' testing charter.Stay tuned!", "The April 2011 issue of Computer,\u00a0the flagship publication of the IEEE Computer Society,\u00a0features a cover article written by\u00a0Alberto Savoia and\u00a0Patrick Copeland (@copelandpatrick). The article,\u00a0Entrepreneurial Innovation at Google, explores some of the ideas we use to encourage engineers to well, innovate. For a limited time you can see a digital version of the article on\u00a0Computing Now \u2013 see\u00a0http://www.computer.org/portal/web/computingnow.\n\n\nEnjoy,\nAlberto & Pat", "By James Whittaker\n\nI've had a number of questions about the SET role and it seems I have confused folks when I say that the SWE is a tester and the SET is a tester and at the same time the SWE is a developer and the SET is a developer. What could possibly be confusing about that?\n\nOh, yeah. Right.\n\nMy next series of posts are going to detail the role of the SET and all will eventually be clear but some clarification on career path seems worthwhile. \n\nSETs are developers who write test code and automation as their primary task. They are in every sense of the word a developer. When we interview SETs, SWEs are on the interview loop and SWE questions are asked. They are not all of the interview, but they are part of it. \n\nThis means that the skill set that our SETs possess makes them perfect candidates for switching to the SWE role. There is neither incentive nor deterrent to do so. SETs and SWEs are on the same pay scale and bonus structure (I have both roles reporting to me so I have real visibility into salary data) and their promotion velocity (again based on actual data) is roughly equivalent. This means that SETs have no outside influences to prompt them one way or the other. \n\nThe key factor is really the type of work you are doing. SETs who find themselves involved in SWE work usually convert to SWE. SWEs are also drawn in the opposite direction. Much of this happens through our 20% time work. Any SET interested in SWE work can take on a 20% task doing feature development. Any SWE interested in automation can find a group and sign up for a 20%. Right now I have both SWEs and SETs involved in such cross pollination. \n\nThe ideal situation is that the title reflects the actual work that you are involved in. So if an SET starts doing more feature dev work than automation, he or she should convert, same for SWEs doing automation work. In my time here, conversions in both directions have happened, but it is not all that common. The work of both roles is engaging, interesting and intense. Few Googlers are walking around bored. \n\nBottom line: do the work you are passionate about and capable of and the right job title will find you.", "For our followers on twitter, we didn't realize our feed we broken until today! Shame, lots of good posts recently. Come check them out. @googletesting", "By James WhittakerInstead of distinguishing between code, integration and system testing, Google uses the language of small, medium and large tests emphasizing scope over form. Small tests cover small amounts of code and so on. Each of the three engineering roles may execute any of these types of tests and they may be performed as automated or manual tests. Small Tests are mostly (but not always) automated and exercise the code within a single function or module. They are most likely written by a SWE or an SET and may require mocks and faked environments to run but TEs often pick these tests up when they are trying to diagnose a particular failure. For small tests the focus is on typical functional issues such as data corruption, error conditions and off by one errors. The question a small test attempts to answer is does this code do what it is supposed to do?Medium Tests can be automated or manual and involve two or more features and specifically cover the interaction between those features. I've heard any number of SETs describe this as \"testing a function and its nearest neighbors.\" SETs drive the development of these tests early in the product cycle as individual features are completed and SWEs are heavily involved in writing, debugging and maintaining the actual tests. If a test fails or breaks, the developer takes care of it autonomously. Later in the development cycle TEs may perform medium tests either manually (in the event the test is difficult or prohibitively expensive to automate) or with automation. The question a medium test attempts to answer is does a set of near neighbor functions interoperate with each other the way they are supposed to?Large Tests cover three or more (usually more) features and represent real user scenarios to the extent possible. There is some concern with overall integration of the features but large tests tend to be more results driven, i.e., did the software do what the user expects? All three roles are involved in writing large tests and everything from automation to exploratory testing can be the vehicle to accomplish accomplish it. The question a large test attempts to answer is does the product operate the way a user would expect?The actual language of small, medium and large isn\u2019t important. Call them whatever you want. The important thing is that Google testers share a common language to talk about what is getting tested and how those tests are scoped. When some enterprising testers began talking about a fourth class they dubbed enormous every other tester in the company could imagine a system-wide test covering nearly every feature and that ran for a very long time. No additional explanation was necessary. The primary driver of what gets tested and how much is a very dynamic process and varies wildly from product to product. Google prefers to release often and leans toward getting a product out to users so we can get feedback and iterate. The general idea is that if we have developed some product or a new feature of an existing product we want to get it out to users as early as possible so they may benefit from it. This requires that we involve users and external developers early in the process so we have a good handle on whether what we are delivering is hitting the mark. Finally, the mix between automated and manual testing definitely favors the former for all three sizes of tests. If it can be automated and the problem doesn\u2019t require human cleverness and intuition, then it should be automated. Only those problems, in any of the above categories, which specifically require human judgment, such as the beauty of a user interface or whether exposing some piece of data constitutes a privacy concern, should remain in the realm of manual testing. Having said that, it is important to note that Google performs a great deal of manual testing, both scripted and exploratory, but even this testing is done under the watchful eye of automation. Industry leading recording technology converts manual tests to automated tests to be re-executed build after build to ensure minimal regressions and to keep manual testers always focusing on new issues. We also automate the submission of bug reports and the routing of manual testing tasks. For example, if an automated test breaks, the system determines the last code change that is the most likely culprit, sends email to its authors and files a bug. The ongoing effort to automate to within the \u201clast inch of the human mind\u201d is currently the design spec for the next generation of test engineering tools Google is building. Those tools will be highlighted in future posts. However, my next target is going to revolve around The Life of an SET. I hope you keep reading.", "by Patrick Copeland\nGoogle Innovation and the Pretotyping Manifesto*: I had the pleasure of keynoting at the sold out San Francisco and London QCon conferences. Combined about 1300 people attended. They recently published the video and I'm including it here with some reviews. Enjoy!\nThe slides and video.\n\n\nReviews and commentary from attendees:http://blog.rodger-brown.com/2011/03/qcon-2011-london-day-two.htmlhttp://alblue.bandlem.com/2011/03/qcon-day-2.htmlhttp://blogg.altran.se/cis/2011/03/12/qcon-2011-second-day-first-half/http://orange-coding.net/?p=23http://craignicol.wordpress.com/http://tweetmeme.com/story/4236295014/infoq-qcon-keynote-innovation-at-google\n*not your typical testing post, but once you see the talk you'll understand why we think testing and innovation go hand in hand.", "By James WhittakerCrawl, walk, run.One of the key ways Google achieves good results with fewer testers than many companies is that we rarely attempt to ship a large set of features at once. In fact, the exact opposite is often the goal: build the core of a product and release it the moment it is useful to as large a crowd as feasible, then get their feedback and iterate. This is what we did with Gmail, a product that kept its beta tag for four years. That tag was our warning to users that it was still being perfected. We removed the beta tag only when we reached our goal of 99.99% uptime for a real user\u2019s email data. Obviously, quality is a work in progress!It\u2019s not as cowboy a process as I make it out to be. In fact, in order to make it to what we call the beta channel release, a product must go through a number of other channels and prove its worth. For Chrome, a product I spent my first two years at Google working on, multiple channels were used depending on our confidence in the product\u2019s quality and the extent of feedback we were looking for. The sequence looked something like this:Canary Channel is used for code we suspect isn\u2019t fit for release. Like a canary in a coalmine, if it failed to survive then we had work to do. Canary channel builds are only for the ultra tolerant user running experiments and not depending on the application to get real work done.Dev Channel is what developers use on their day-to-day work. All engineers on a product are expected to pick this build and use it for real work.Test Channel is the build used for internal dog food and represents a candidate beta channel build given good sustained performance.The Beta Channel or Release Channel builds are the first ones that get external exposure. A build only gets to the release channel after spending enough time in the prior channels that is gets a chance to prove itself against a barrage of both tests and real usage. This crawl, walk, run approach gives us the chance to run tests and experiment on our applications early and obtain feedback from real human beings in addition to all the automation we run in each of these channels every day. There are analytical benefits to this process as well. If a bug is found in the field a tester can create a test that reproduces it and run it against builds in each channel to determine if a fix has already been implemented.", "Note: This post is rated PG-13 for use of a mild expletive.  If you are likely to be offended by the repeated use a word commonly heard in elementary school playgrounds, please don\u2019t read any further.\nCRAP is short for Change Risk Anti-Patterns \u2013 a mildly offensive acronym to protect you from deeply offensive code.  CRAP was originally developed and launched in 2007 by yours truly (Alberto Savoia) and my colleague and partner in crime Bob Evans.\n\nWhy call it CRAP?  When a developer or tester has to work with someone else\u2019s (bad) code, they rarely comment on it by saying things like: \u201cThe median cyclomatic complexity is unacceptable,\u201d or \u201cThe efferent coupling values are too high.\u201d  Instead of citing a particular objective metric, they summarize their subjective evaluation and say things like: \u201cThis code is crap!\u201d At least those are the words the more polite developers use; I\u2019ve heard and read far more colorful adjectives and descriptions over the years.  So Bob and I decided to coin an acronym that, in addition to being memorable \u2013 even if it\u2019s for the wrong reasons, is a good match with the language that its intended users use and it\u2019s guaranteed to grab a developer\u2019s attention: \u201cHey, your code is CRAP!\u201d\n\nBut what makes a particular piece of code CRAP? There is, of course, no fool-proof, 100% objective, and accurate way to determine CRAPpiness. However, our experience and intuition \u2013 backed by a bit of research and a lot of empirical evidence \u2013 suggested the possibility that there are detectable and measurable patterns that indicate the possible presence of CRAPpy code.  That was enough to get us going with the first anti-pattern (which I\u2019ll describe shortly.)\n\nSince its inception, the original version of CRAP has gained quite a following; it has been ported to various languages and platforms (e.g. Java, .NET, Ruby, PHP, Maven, Ant) and it\u2019s showing up both in free and commercial code analysis tools such as Hudson\u2019s Cobertura and Atlassian\u2019s Clover.  Do a Google search for \u201cCRAP code metric\u201d and you\u2019ll see quite a bit of activity.  All of which is making Bob and I feel mighty proud, but we haven\u2019t been resting on our laurels.  Well, actually we have done precisely that.  After our initial work (which included  the Crap4J Eclipse plug-in and the, now mostly abandoned, crap4j.org website) we both went to work for Google and got busy with other projects.  However, the success and adoption of CRAP is a good indication that we were on to something and I believe it\u2019s time to invest a bit more in it and move it forward.\n\n\n\nOver the next few weeks I will post about the past, present and future of CRAP.  By the time I\u2019m done, you will have the tools to:\n\n- Know you CRAP\n- Cut the CRAP, and\n- Don\u2019t take CRAP from nobody!\n\nI\u2019ll finish today\u2019s entry with a bit of background on the original CRAP metric.\n\nA Brief History of CRAP\nAs the CRAP acronym suggests, there are several possible patterns that make a piece of code CRAPpy, but we had to start somewhere. Here is the first version of the (in)famous formula to help detect CRAPpy Java methods.  Let\u2019s call it CRAP1, to make clear that this covers just one of the many interesting anti-patterns and that there are more to come.\n\nCRAP1(m) = comp(m)^2 * (1 \u2013 cov(m)/100)^3 + comp(m)\n\nWhere CRAP1(m) is the CRAP1 score for a method m, comp(m) is the cyclomatic complexity of m, and cov(m) is the basis path code coverage from automated tests for m.\n\nIf CRAP1(m) > 30, we consider the method to be CRAPpy.\n\nThis CRAP1 formula did not materialize out of thin air.  We arrived at this particular function empirically; it\u2019s the result of a best fit curve achieved through a lot of trial-and-error.  At the time we had access to the source code for a large number of open source and commercial Java projects, along with their associated JUnit tests.  This allowed us to rank code for CRAPpiness using one formula, ask our colleagues if they agreed and kept iterating until we reached diminishing returns.  This way we were able to come up with a curve that was a pretty good fit for the more subjective data we got from our colleagues.\n\nHere\u2019s why we think that CRAP1 is a good anti-pattern to detect.  Writing automated tests (e.g., using JUnit) for complex and convoluted code is particularly challenging, so crappy code usually comes with few, if any, automated tests. This means that the presence of automated tests implies not only some degree of testability (which in turn seems to be associated with better, or more thoughtful, design), but it also means that the developers cared enough, knew enough and had enough time to write tests \u2013 which is another good sign for the people inheriting the code.  These sounded like reasonable assumptions at the time, and the adoption of CRAP1 \u2013 especially by the Agile community \u2013 reflects that.\n\nLike all software metrics, CRAP1 is neither perfect nor complete. We know very well, for example, that you can have great code coverage and lousy tests. In addition, sometimes complex code is either unavoidable or preferable; there might be instances where a single higher complexity method might be easier to understand than three simpler ones. We are also aware that the CRAP1 formula doesn\u2019t currently take into account higher-order, more design-oriented metrics that are relevant to maintainability (such as cohesion and coupling) \u2013 but it\u2019s a start, the plan is to add more anti-patterns.\n\nUse CRAP On Your Project\nEven though Bob and I haven't actively developed or maintained Crap4J in the past few years (shame on us!), other brave developers have been busy porting CRAP to all sorts of languages and environments.  As a result, there are many versions of the CRAP metric in open source and commercial tools.  If you want to try CRAP on your project, the best thing to do is to run a Google search for the language and tools you are currently using.  \n\nFor example, a search for \"crap metric .net\" returned several projects, including crap4n and one called crap4net.  If you use Clover, here's how you can use it to implement CRAP.  PHP? No problem, someone implemented CRAP for PHPUnit.  However, apparently nobody has implemented CRAP for COBOL yet ... here's your big chance!\n\nUntil the next blog on CRAP, you might enjoy this vintage video on Crap4J.  Please note, however, that the Eclipse plug-in shown in the demo does not work with versions of Eclipse newer than 3.3 - we did say it was a vintage video and that Bob and I have been resting on our laurels!\n\nPosted by Alberto Savoia", "By James WhittakerThese posts have garnered a number of interesting comments. I want to address two of the negative ones in this post. Both are of the same general opinion that I am abandoning testers and that Google is not a nice place to ply this trade. I am puzzled by these comments because nothing could be further from the truth. One such negative comment I can take as a one-off but two smart people (hey they are reading this blog, right?) having this impression requires a rebuttal. Here are the comments:\"A sad day for testers around the world. Our own spokesman has turned his back on us. What happened to 'devs can't test'?\" by Gengodo\"I am a test engineer and Google has been one of my dream companies. Reading your blog I feel that Testers are so unimportant at Google and can be easily laid off. It's sad.\" by MaggiFirst of all, I don't know of any tester or developer for that matter being laid off from Google. We're hiring at a rapid pace right now. However, we do change projects a lot so perhaps you read 'taken off a project' to mean something far worse than the reality of just moving to another project. A tester here may move every couple of years or so and it is a badge of honor to get to the point where you've worked yourself out of a job by building robust test frameworks for others to contribute tests to or to pass off what you've done to a junior tester and move on to a bigger challenge. Maggi, please keep the dream alive. If Google was a hostile place for testers, I would be working somewhere else. Second, I am going to dodge the negative undertones of the developer vs tester debate.  Whether developers can test or testers can code seems downright combative. Both types of engineers share the common goal of shipping a product that will be successful. There is enough negativity in this world and testers hating developers seems so 2001.In fact, I feel a confession coming on. I have had sharp words with developers in the past. I have publicly decried the lack of testing rigor in commercial products. If you've seen me present you've probably witnessed me showing colorful bugs, pointing to the screen and shouting \"you missed a spot!\" I will admit, that was fun. Here are some other quotes I have directed at developers:\"You must be smarter than me because I couldn't write this bug if I was trying to.\"\"What happened, did the compiler get your eye?\"\"What do you say to a developer with two black eyes? Nothing, he's already been told twice.\" \"Did you hear about the developer who locked himself in his car?\"Ah, those were the good old days! But it's 2011 now and I am objective enough to give developers credit when they step up to the plate and do their job. At Google, many have and they are helping to shame the rest into following suit. And this is making bugs harder to find. I waste so little time on low hanging fruit that I get to dig deeper to find the really subtle, really critical bugs. The signal to noise ratio is just a whole lot stronger now. Yes there are fewer developer jokes but this is progress. I have to make myself feel good knowing how many bugs have been prevented instead of how many laughs I can get on stage demonstrating their miserable failures. This is progress. And, incidentally developers can test. In some cases far better than testers. Modern testing is about optimizing the places where developers test and where testers test. Getting that mix right means a great product. Getting it wrong puts us back in 2001 where my presentations were a heck of a lot funnier.In what cases are developers better testers that we are? In what cases are they not only poor testers but we're better off not having them touch the product at all? Well, that's the subject of my next couple of posts. In the meantime......Peace.", "By Patrick Copeland\nJust considering last year...This blog was read in 181 countries/territories. \n20% of visitors came to the site at least 4 times.\nAbout 50% of the visits came from the United States, India, United Kingdom, Brazil, Canada, and Germany. \nWithin the US, all states are represented, with a majority of visits coming from the significant technology centers. \nTop 10 world wide cities visiting (outside of the Bay Area): London, Bangalore, New York, Sao Paulo, Chennai, Hyderabad, Tokyo, Redmond, Seoul, Moscow. \nThe average visitors stay about two minutes (enough time to read the post). Although, for some reason people in Switzerland stayed for 12 minutes on average and looked at twice as many pages. \nWe get numerous visits from Central Asia and Melanesia, but the time on site is very small, which indicates from use of bots. As a matter of fact, 30% of the visits are flagged as search engine traffic. \nThe highest read single post was written by Alberto Savoia with 39,778 visits. Followed in (a distant) second place by a post I wrote. BTW, James' recent posts are really catching fire and I think Alberto could be unseated in 2011. \nBelow is the view of the traffic by continent...\nThanks for your visits and we welcome your comments.", "By James WhittakerLots of questions in the comments to the last two posts. I am not ignoring them. Hopefully many of them will be answered here and in following posts. I am just getting started on this topic.At Google, quality is not equal to test. Yes I am sure that is true elsewhere too. \u201cQuality cannot be tested in\u201d is so clich\u00e9 it has to be true. From automobiles to software if it isn\u2019t built right in the first place then it is never going to be right. Ask any car company that has ever had to do a mass recall how expensive it is to bolt on quality after-the-fact. However, this is neither as simple nor as accurate as it sounds. While it is true that quality cannot be tested in, it is equally evident that without testing it is impossible to develop anything of quality. How does one decide if what you built is high quality without testing it? The simple solution to this conundrum is to stop treating development and test as separate disciplines. Testing and development go hand in hand. Code a little and test what you built. Then code some more and test some more. Better yet, plan the tests while you code or even before. Test isn\u2019t a separate practice, it\u2019s part and parcel of the development process itself. Quality is not equal to test; it is achieved by putting development and testing into a blender and mixing them until one is indistinguishable from the other.  At Google this is exactly our goal: to merge development and testing so that you cannot do one without the other. Build a little and then test it. Build some more and test some more. The key here is who is doing the testing. Since the number of actual dedicated testers at Google is so disproportionately low, the only possible answer has to be the developer. Who better to do all that testing than the people doing the actual coding? Who better to find the bug than the person who wrote it? Who is more incentivized to avoid writing the bug in the first place? The reason Google can get by with so few dedicated testers is because developers own quality. In fact, teams that insist on having a large testing presence are generally assumed to be doing something wrong. Having too large a test team is a very strong sign that the code/test mix is out of balance. Adding more testers is not going to solve anything. This means that quality is more an act of prevention than it is detection. Quality is a development issue, not a testing issue. To the extent that we are able to embed testing practice inside development, we have created a process that is hyper incremental where mistakes can be rolled back if any one increment turns out to be too buggy. We\u2019ve not only prevented a lot of customer issues, we have greatly reduced the number of testers necessary to ensure the absence of recall-class bugs. At Google, testing is aimed at determining how well this prevention method is working. TEs are constantly on the lookout for evidence that the SWE-SET combination of bug writers/preventers are screwed toward the latter and TEs raise alarms when that process seems out of whack. Manifestations of this blending of development and testing are all over the place from code review notes asking \u2018where are your tests?\u2019 to posters in the bathrooms reminding developers about best testing practices, our infamous Testing On The Toilet guides. Testing must be an unavoidable aspect of development and the marriage of development and testing is where quality is achieved. SWEs are testers, SETs are testers and TEs are testers. If your organization is also doing this blending, please share your successes and challenges with the rest of us. If not, then here is a change you can help your organization make: get developers fully vested in the quality equation. You know the old saying that chickens are happy to contribute to a bacon and egg breakfast but the pig is fully committed? Well, it's true...go oink at one of your developer and see if they oink back. If they start clucking, you have a problem.", "By James WhittakerIn order for the \u201cyou build it, you break it\u201d motto to be real, there are roles beyond the traditional developer that are necessary. Specifically, engineering roles that enable developers to do testing efficiently and effectively have to exist. At Google we have created roles in which some engineers are responsible for making others more productive. These engineers often identify themselves as testers but their actual mission is one of productivity. They exist to make developers more productive and quality is a large part of that productivity. Here's a summary of those roles: The SWE or Software Engineer is the traditional developer role. SWEs write functional code that ships to users. They create design documentation, design data structures and overall architecture and spend the vast majority of their time writing and reviewing code. SWEs write a lot of test code including test driven design, unit tests and, as we explain in future posts, participate in the construction of small, medium and large tests. SWEs own quality for everything they touch whether they wrote it, fixed it or modified it.  The SET or Software Engineer in Test is also a developer role except their focus is on testability. They review designs and look closely at code quality and risk. They refactor code to make it more testable. SETs write unit testing frameworks and automation. They are a partner in the SWE code base but are more concerned with increasing quality and test coverage than adding new features or increasing performance. The TE or Test Engineer is the exact reverse of the SET. It is a a role that puts testing first and development second. Many Google TEs spend a good deal of their time writing code in the form of automation scripts and code that drives usage scenarios and even mimics a user. They also organize the testing work of SWEs and SETs, interpret test results and drive test execution, particular in the late stages of a project as the push toward release intensifies. TEs are product experts, quality advisers and analyzers of risk. From a quality standpoint, SWEs own features and the quality of those features in isolation. They are responsible for fault tolerant designs, failure recovery, TDD, unit tests and in working with the SET to write tests that exercise the code for their feature. SETs are developers that provide testing features. A framework that can isolate newly developed code by simulating its dependencies with stubs, mocks and fakes and submit queues for managing code check-ins. In other words, SETs write code that allows SWEs to test their features. Much of the actual testing is performed by the SWEs, SETs are there to ensure that features are testable and that the SWEs are actively involved in writing test cases. Clearly SETs primary focus is on the developer. Individual feature quality is the target and enabling developers to easily test the code they write is the primary focus of the SET. This development focus leaves one large hole which I am sure is already evident to the reader: what about the user?User focused testing is the job of the Google TE. Assuming that the SWEs and SETs performed module and feature level testing adequately, the next task is to understand how well this collection of executable code and data works together to satisfy the needs of the user. TEs act as a double-check on the diligence of the developers. Any obvious bugs are an indication that early cycle developer testing was inadequate or sloppy. When such bugs are rare, TEs can turn to their primary task of ensuring that the software runs common user scenarios, is performant and secure, is internationalized and so forth. TEs perform a lot of testing and test coordination tasks among TEs, contract testers, crowd sourced testers, dog fooders, beta users, early adopters. They communicate among all parties the risks inherent in the basic design, feature complexity and failure avoidance methods. Once TEs get engaged, there is no end to their mission. Ok, now that the roles are better understood, I'll dig into more details on how we choreograph the work items among them. Until next time...thanks for your interest.", "By James WhittakerThis is the first in a series of posts on this topic.The one question I get more than any other is \"How does Google test?\" It's been explained in bits and pieces on this blog but the explanation is due an update. The Google testing strategy has never changed but the tactical ways we execute it has evolved as the company has evolved. We're now a search, apps, ads, mobile, operating system, and so on and so forth company. Each of these Focus Areas (as we call them) have to do things that make sense for their problem domain. As we add new FAs and grow the existing ones, our testing has to expand and improve. What I am documenting in this series of posts is a combination of what we are doing today and the direction we are trending toward in the foreseeable future. Let's begin with organizational structure and it's one that might surprise you. There isn't an actual testing organization at Google. Test exists within a Focus Area called Engineering Productivity. Eng Prod owns any number of horizontal and vertical engineering disciplines, Test is the biggest. In a nutshell, Eng Prod is made of:1. A product team that produces internal and open source productivity tools that are consumed by all walks of engineers across the company. We build and maintain code analyzers, IDEs, test case management systems, automated testing tools, build systems, source control systems, code review schedulers, bug databases... The idea is to make the tools that make engineers more productive. Tools are a very large part of the strategic goal of prevention over detection. 2. A services team that provides expertise to Google product teams on a wide array of topics including tools, documentation, testing, release management, training and so forth. Our expertise covers reliability, security, internationalization, etc., as well as product-specific functional issues that Google product teams might face. Every other FA has access to Eng Prod expertise. 3. Embedded engineers that are effectively loaned out to Google product teams on an as-needed basis. Some of these engineers might sit with the same product teams for years, others cycle through teams wherever they are needed most. Google encourages all its engineers to change product teams often to stay fresh, engaged and objective. Testers are no different but the cadence of changing teams is left to the individual. I have testers on Chrome that have been there for several years and others who join for 18 months and cycle off. Keeping a healthy balance between product knowledge and fresh eyes is something a test manager has to pay close attention to. So this means that testers report to Eng Prod managers but identify themselves with a product team, like Search, Gmail or Chrome. Organizationally they are part of both teams. They sit with the product teams, participate in their planning, go to lunch with them, share in ship bonuses and get treated like full members of the team. The benefit of the separate reporting structure is that it provides a forum for testers to share information. Good testing ideas migrate easily within Eng Prod giving all testers, no matter their product ties, access to the best technology within the company. This separation of project and reporting structures has its challenges. By far the biggest is that testers are an external resource. Product teams can't place too big a bet on them and must keep their quality house in order. Yes, that's right: at Google it's the product teams that own quality, not testers. Every developer is expected to do their own testing. The job of the tester is to make sure they have the automation infrastructure and enabling processes that support this self reliance. Testers enable developers to test. What I like about this strategy is that it puts developers and testers on equal footing. It makes us true partners in quality and puts the biggest quality burden where it belongs: on the developers who are responsible for getting the product right. Another side effect is that it allows us a many-to-one dev-to-test ratio. Developers outnumber testers. The better they are at testing the more they outnumber us. Product teams should be proud of a high ratio!  Ok, now we're all friends here right? You see the hole in this strategy I am sure. It's big enough to drive a bug through. Developers can't test! Well, who am I to deny that? No amount of corporate kool-aid could get me to deny it, especially coming off my GTAC talk last year where I pretty much made a game of developer vs. tester (spoiler alert: the tester wins).Google's answer is to split the role. We solve this problem by having two types of testing roles at Google to solve two very different testing problems. In my next post, I'll talk about these roles and how we split the testing problem into two parts.", "I'll be speaking very soon about innovation and The Pretotyping Manifesto (note: not prototyping). It's a concept that works well for any type of engineering, testing, or idea. Here's a brief preview...\nThe talk starts off by discussing the odds against innovators and how the deck is stacked against you from the start. Most engineers begin with trying to come up with THE killer idea. But you quickly realize that ideas are cheap. We all believe that our own ideas are good. As a matter of fact, most of us LOVE our own ideas to the point where it clouds our judgement. In parallel, there is a strong desire to jump in and build something after a great idea has been identified. Sometimes swarms of well intentioned people join in and \"help.\" Quickly innovators can find themselves in the weeds or feeling disconnected from their original dream. So, what can be done? The idea is to use pretotyping and to focus on techniques that allow you to: Build the right it vs. Build it right.\nLast time I did this talk, it was called: \"the best keynote of any tech conference ever!\"\nI'm looking forward to seeing some of you the week of March7th when I'll be in London. In addition to dropping into the Google office, I'll be speaking at QCon London. If you want to attend (at a cheaper rate than normal) here are the details:London event March 7-11, 2011.\nMy personal \"promotion code\" will save you \u00a3100 if they enter this code during registration.\nPromotion code is: COPE100\nHope to see you in London,Patrick CopelandSenior Engineering Director, Google", "By James WhittakerI know many people who laugh at the concept of resolutions, easily made and easily broken. All true. However, I am a runner now because of a resolution I made about a decade ago and my personality has undergone a successful renovation or two over the years as well. When they stick, resolutions can become habits and the emergence of the occasional butterfly makes them a worthwhile exercise. With the optimism of a new year, I present my Google Testing resolutions for 2011 which I hereby declare the Year of the User. 1. I will listen to users more and developers less.Developers, by definition, are engineers lost in the details of implementation. When it comes to testing concerns, such implementation details clog a tester's neural pathways with issues that simply should not be relevant. I resolve to take the high road as often as possible and consider user scenarios, integration issues and end-to-end uses of the system above all other concerns. And, yes, that will mean telling developers \"sorry, dude, your broken build simply is not my concern.\"2. I will push all implementation testing issues to developers. My first resolution may lead readers to believe that testing implementation details isn't important. Let me be clear. Testing implementation details is important. When they go untested they create enough noise that user-oriented testing is compromised by the constant emergence of silly bugs. Silly bugs mask important ones. Find them at the source: ensure that proper unit testing and automated smoke tests are present and owned by the people most qualified to write and maintain them: developers.  I resolve not to be sidetracked by silly bugs but to push back hard on the developers who are happy to write the bug but neglect to write the test for it.  3. I will endeavor to tie together all user-oriented testing. In the run up to releasing Chrome OS for pilot last year it was clear that many of the bugs found during dogfood (internal testing), crowd-sourced and out-sourced testing had already been found by my test team. Not only is there is a lot of repetitive and wasteful testing being performed, my team isn't getting enough credit for finding these important issues early. I resolve to introduce technology that will allow all testers to share testing tactics and see each other's work, ultimately erasing the boundaries between these common phases and allowing testers who join a project late to build upon the work of those who've been there for a while. Finally, I resolve to expose more information about how Google tests internally. I am going to return to the conference circuit this year and talk frankly about what we are doing, the good, the bad and the downright embarrassing in the hopes that other testers at other companies do the same. I am also going to push more Google testers to post their experiences on this blog and join me at industry events to discuss these things with anyone struggling with the same issues.Happy New Year! May it be one that sees a higher level of quality from every corner of our industry.", "The published content for this year's GTAC is finally available. You can find the links to all the material below and also at the GTAC site. Other interesting artifacts can be found via Google Search or Twitter or Facebook.. \nOn behalf of the Committee and Google I want to thank all the speakers, attendees and volunteers  who made this event a great professional engagement. Some moments of the conference are captured in photos.\n\nLooking forward to next year\u2019s GTAC in the city of Google\u2019s headquarters. \n\nHappy Holidays.\nSujay Sahni for the GTAC 2010 Committee\n\n\nDay 1\n\nWelcome and Opening Remarks\nSujay Sahni, Google Inc. & GTAC Committee Chair\nvideo slides\n\nDay 1 Opening Keynote\nWhat Testability Tells us About the Software Performance Envelope\nRobert Victor Binder, Founder and CEO, mVerify\nvideo slides abstract\n\nTwist, a next generation functional testing tool for building and evolving test suites\nVivek Prahlad, ThoughtWorks \nvideo slides   abstract\n\nThe Future of Front-End Testing\nGreg Dennis & Simon Stewart, Google Inc.\nvideo slides  abstract\n\nLightning Talks/Interactive Session\nGTAC Attendees\nvideo slides\n\nTestivus on Testability\nAlberto Savoia, Google Inc.\nvideo slides\n\nLessons Learned from Testability Failures\nEsteban Manchado Vel\u00e1zquez, Opera Software ASA \nvideo slides  abstract\n\nGit Bisect and Testing\nChristian Couder\nvideo slides  abstract\n\nFlexible Design? Testable Design? You Don\u2019t Have To Choose!\nRuss Rufer & Tracy Bialik, Google Inc.  \nvideo slides  abstract\n\n\nDay 2\n\nDay 2 Opening Keynote\nAutomatically Generating Test Data for Web Applications\nJeff Offutt, Professor of Software Engineering, Volgenau School of Information and Technology, George Mason University\nvideo slides  abstract\n\nEarly Test Feedback by Test Prioritisation\nShin Yoo, University College London &  Robert Nilsson, Google Inc.\nvideo slides  abstract\n\nMeasuring and Monitoring Experience in Interactive Streaming Applications\nShreeshankar Chatterjee, Adobe Systems India\nvideo slides  abstract\n\nCrowd Source Testing, Mozilla Community Style\nMatt Evans, Mozilla\nvideo slides  abstract\n\nLightning Talks/Interactive Session\nGTAC Attendees\nvideo slides\n\nClosing Keynote - Turning Quality on its Head\nJames Whittaker, Engineering Director, Google Inc.\nvideo slides  abstract\n\nClosing Panel Discussion\nGTAC Attendees\nvideo\n\nClosing Remarks\nSujay Sahni, Google Inc. & GTAC Committee Chair\nvideo slides", "by Simon Stewart\n\n\n\nWhat do you call a test that tests your application through its UI? An end-to-end test? A functional test? A system test? A selenium test? I\u2019ve heard all them, and more. I reckon you have too. Tests running against less of the stack? The same equally frustrating inconsistency. Just what, exactly, is an integration test? A unit test? How do we name these things?\n\nGah!\n\nIt can be hard to persuade your own team to settle on a shared understanding of what each name actually means. The challenge increases when you encounter people from another team or project who are using different terms than you. More (less?) amusingly, you and that other team may be using the same term for different test types.  \u201cOh! That kind of integration test?\u201d Two teams separated by a common jargon.\n\nDouble gah!\n\nThe problem with naming test types is that the names tend to rely on a shared understanding of what a particular phrase means. That leaves plenty of room for fuzzy definitions and confusion. There has to be a better way. Personally, I like what we do here at Google and I thought I\u2019d share that with you.\n\nGooglers like to make decisions based on data, rather than just relying on gut instinct or something that can\u2019t be measured and assessed. Over time we\u2019ve come to agree on a set of data-driven naming conventions for our tests. We call them \u201cSmall\u201d, \u201cMedium\u201d and \u201cLarge\u201d tests. They differ like so:\n\n\nFeatureSmallMediumLarge\nNetwork accessNolocalhost onlyYes\nDatabaseNoYesYes\nFile system accessNoYesYes\nUse external systemsNoDiscouragedYes\nMultiple threadsNoYesYes\nSleep statementsNoYesYes\nSystem propertiesNoYesYes\nTime limit (seconds)60300900+\n\n\n\n\n\nGoing into the pros and cons of each type of test is a whole other blog entry, but it should be obvious that each type of test fulfills a specific role. It should also be obvious that this doesn\u2019t cover every possible type of test that might be run, but it certainly covers most of the major types that a project will run.\n\nA Small test equates neatly to a unit test, a Large test to an end-to-end or system test and a Medium test to tests that ensure that two tiers in an application can communicate properly (often called an integration test).\n\nThe major advantage that these test definitions have is that it\u2019s possible to get the tests to police these limits. For example, in Java it\u2019s easy to install a security manager for use with a test suite (perhaps using @BeforeClass) that is configured for a particular test size and disallows certain activities. Because we use a simple Java annotation to indicate the size of the test (with no annotation meaning it\u2019s a Small test as that\u2019s the common case), it\u2019s a breeze to collect all the tests of a particular size into a test suite. \n\nWe place other constraints, which are harder to define, around the tests. These include a requirement that tests can be run in any order (they frequently are!) which in turn means that tests need high isolation --- you can\u2019t rely on some other test leaving data behind. That\u2019s sometimes inconvenient, but it makes it significantly easier to run our tests in parallel. The end result: we can build test suites easily, and run them consistently and as as fast as possible.\n\nNot \u201cgah!\u201d at all.", "By James Whittaker\n\nIf you've heard me speak anytime over the past year you have heard me talk about Chrome OS and how we are testing it. Well, we're not done testing it but we are announcing a pilot where you, yes you, can get one of the initial sets of new hardware with Chrome OS pre-installed. The hardware is called Cr-48 (a chromium isotope, how nerdy is that?) and is available in very limited numbers. \n\nHow do you get one? Glad you asked. Go here and tell us why you are an ideal candidate to give it a test drive and provide us with feedback! It's that simple/hard and I would love to see some of these in the hands of the many hardcore testers who read this blog.", "By James WhittakerWhen to stop testing? It\u2019s the age old testing question that many researchers have tried to quantify. In fact, the best answer requires no science whatsoever: never. Since testing is infinite, we can never really stop. A more practical answer also surfaces in the real world: when the software ships, you\u2019re done. Of course this is only true for the duration of the ship party, after that testing continues on the next version. At Google we are experimenting with test completeness measures that describe how well actual testing covers the risk landscape. In other words, we are measuring the extent to which our testing covers the things that require the most testing. Tests that cover the high risk areas well count for more than tests that cover lower risk features. Testing is, after all, a business of risk mitigation.The set of tools necessary to accomplish this were described in my GTAC 2010 talk which should appear on YouTube soon and are collectively being called Google Test Analytics. More about these tools in future posts.", "By James WhittakerThe sixth ingredient is variation. Tests often get stale (i.e., they stop finding bugs) as they are run over and over on build after build as a product is being constructed. On the one hand, it is important to continue running tests to ensure the product still operates as specified. Indeed, I hesitate to throw any test away. However, becoming reliant on stale tests is too risky. Adding variation to existing tests can range from straightforward reordering of the sequence in which tests are run to more involved solutions of either modifying tests or adding new ones. Hopefully new tests will increase overall coverage and add to our confidence that we\u2019ve tested the software in all the ways it needs to be tested.", "Can you spot the error in the following webpage?Unless you are one of the 56 million Internet users who read Arabic, the answer is probably no.  But BidiChecker, a tool for checking webpages for errors in handling of bidirectional text, can find it:Oops! The Arabic movie title causes the line to be laid out in the wrong order, with half of the phrase \"57 reviews\" on one side of it and half on the other.As this example demonstrates, text transposition errors can occur even if your web application is entirely in a left-to-right language.  If the application accepts user input or displays multilingual content, this data may be in one of the right-to-left languages, such as Arabic, Hebrew, Farsi or Urdu.  Displaying right-to-left text in a left-to-right environment, or vice versa, is likely to cause text garbling if not done correctly.  So most user interfaces, whether left-to-right or right-to-left, need to be able to deal with bidirectional (BiDi) text.Handling BiDi text can be tricky and requires special processing at every appearance of potentially BiDi data in the UI.  As a result, BiDi text support often regresses when a developer adds a new feature\u2013and fails to include BiDi support in the updated code.Called from your automated test suite, BidiChecker can catch regressions before they go live.  It features a pure JavaScript API which can easily be integrated into a test suite based on common JavaScript test frameworks such as JSUnit.  Here's a sample test for the above scenario:// Check for BiDi errors with Arabic data in an English UI.\u2028function testArabicDataEnglishUi() {\u2028\u2028\u2028 // User reviews data to display; includes Arabic data.\u2028\u2028\u2028 var reviewsData = [\u2028\u2028\u2028 \u2028\u2028 {'title': 'The Princess Bride', 'reviews': '23'},\u2028\u2028 \u2028\u2028 {'title': '20,000 Leagues Under the Sea', 'reviews': '17'},\u2028\u2028 \u2028\u2028 {'title': '\u0633\u062a\u0627\u0631 \u062a\u0631\u064a\u0643', 'reviews': '57'}  // \u201cStar Trek\u201d\u2028\u2028\u2028 ];\u2028\u2028\u2028\u2028 // Render the reviews in an English UI.\u2028\u2028 var app = new ReviewsApp(reviewsData, testDiv);\u2028\u2028 app.setLanguage('English');\u2028\u2028\u2028 app.render();\u2028\u2028\u2028\u2028\u2028\u2028 // Run BidiChecker.\u2028\u2028\u2028 var errors = bidichecker.checkPage(/* shouldBeRtl= */ false, testDiv);\u2028\u2028 // This assertion will fail due to BiDi errors!\u2028\u2028\u2028 assertArrayEquals([], errors);\u2028}We\u2019ve just released BidiChecker as an open source project on Google Code, so web developers everywhere can take advantage of it.  We hope it makes the web a friendlier place for users of right-to-left languages and the developers who support them.By Jason Elbaum, Internationalization Team", "By James WhittakerI told the crowd at GTAC during my talk: \"I wasn't sure what to expect from India. I was not disappointed.\" Be careful how you quote me on this statement as getting it even a little wrong can make it seem like an insult. It is no such thing. After spending a week there, I still am not sure what to expect. It's a country of such contrasts with extremes on both ends of pretty much every scale you can come up with. India must remain a mystery to me as I have seen so little of it.The Indian people, on the other hand, I think I understand a little better now. Their hunger to contribute. Their hope for the future. Their determination to be part of the solution in every way, shape and form. This is no simple case of outsourcing we have here. That attitude is so last decade. This was the best GTAC yet and the credit must go to the people who ran it and contributed the most to its success. This is a case of India stepping up and doing what London, New York, Seattle, Zurich (and next year Mountain View) did and then raising the bar that much more. Toe-to-toe with the world. There are individuals who can take a bow for GTAC, but the credit has be be far more dispersed. India ... you nailed this one.And I meant what I said at the end of my talk. I am very eager to return. Perhaps one day I will know India well enough to know what to expect. I am very sure I will not be disappointed.", "by Patrick CopelandFYI...I'll be giving a keynote at QCon in SFO next week. Fortunately, or unfortunately as the case my be, the conference is sold out. But they will post a video of the talk following the conference. If you are interested here's the abstract and a pointer to their site (http://qconsf.com/): There are many paths to innovation.  At one extreme, we have large companies who create research labs, staff them with world-class Ph.Ds, and set them working for years to solve extremely complex technical problems.  At the other extreme, we have the proverbial \"two entrepreneurs in the garage\" working on a shoe-string budget.  Between these two extremes, we have all sorts of combinations of organizational structure, team size, budgets and time horizons.  History shows that world-changing innovation is possible through all of these paths.  But history also shows that, as companies grow in size and reputation, they almost inevitably become more conservative and risk-averse when it comes to considering, and investing in, new ideas and disruptive technology \u2013 often with disastrous results.He will describe how Google's core beliefs, culture, organization and infrastructure have successfully encouraged and enabled innovation throughout its growth.  He will conclude by presenting and discussing a practical manifesto to stimulate and leverage innovation in any organization.", "By James Whittaker\n\nWow, India is a lot different than I expected. Visited Golconda fort today and was totally blown away. Plumbing and \"telephones\" all the way back then. No wonder IIT is such a good university. All this history on this trip, first England and now India makes a poor old American wish he had some Native American roots. I feel so ... imported. \n\nI'm almost finished with my GTAC talk. Trial run today in Hyderabad for anyone local who wants to come. Tomorrow I close the conference. I hear they've decided where GTAC 2011 is going to be held but they are holding it secret until the end of this one. I only assume that since I don't know that it is not going to be in the Pacific Northwest but I do expect a return to America.\n\nOn to GTAC!", "By James WhittakerOne of the problems with testing is that testers don\u2019t possess a common vocabulary for the techniques they apply to actually perform testing. Some testers talk about partitioning the input domain and others gravitate toward boundary values but in general there are no catalogues of testing techniques that would allow a conversation such as: run the Landmark Tour on the bookmark sync feature of Chrome. Everyone understands that Chrome is a web browser and that it allows users to sync bookmarks, but how does one test it with a Landmark Tour?\u201cTours\u201d are the metaphor we use at Google to name and describe testing techniques. Every tour encapsulates past testing knowledge, i.e., stuff that worked for other testers on other projects, and can be reused and improved upon. Over time testers get a feel for which tours apply to what type of functionality. It\u2019s a way to identify and store tribal knowledge of the overall team. That's the fifth ingredient: test guidance.", "Update! Speakers & Talks for GTAC 2010We are thrilled to announce the speakers and talks for the 5th Google Test Automation Conference (GTAC). This year\u2019s event will have a total of 11 talks. This includes the three keynotes that we announces earlier and eight other talks. These talks span the three sub-categories of Testing, Testability and Test Automation which are an integral part of this year\u2019s theme \u201cTest to Testability\u201d.As we had shared earlier, for this year\u2019s GTAC we used a new process of letting the selected attendees vote on the talks they wanted be a part of GTAC. The committee tallied the votes and ensures a healthy distribution between topics and participants from across the globe and relevance to our theme. We received over 80 submissions and have an acceptance rate of about 10%. Our thanks to everyone who submitted a proposal and all the attendees who voted to make this a successful process. Here is the list of talks. More details can be found at the select talks page on the GTAC site.Category: TestingEarly Test Feedback by Test Prioritisation (Shin Yoo, University College London &  Robert Nilsson, Google Inc.)Crowd-source testing, Mozilla community style (Matt Evans, Mozilla)Measuring and Monitoring Experience in Interactive Streaming Multimedia Web Applications (Shreeshankar Chatterjee, Adobe Systems India)Category: TestabilityFlexible Design? Testable Design? You Don\u2019t Have To Choose! (Russ Rufer and Tracy Bialik, Google Inc.) Git Bisect and Testing (Christian Couder)Lessons Learned from Testability Failures (Esteban Manchado Velazquez, Opera Software ASA) Category: Test AutomationThe Future of Front-End Testing (Greg Dennis and Simon Stewart, Google Inc.)Twist, a next generation functional testing tool for building and evolving test suites (Vivek Prahlad, ThoughtWorks)For further information on the conference please visit its webpage at http://www.gtac.biz. Sujay Sahni for the GTAC 2010 Committee", "By James WhittakerFirst and foremost, apologies to all of those trying to get to our NY event who weren't able to do so. It was an absolutely packed house, frankly the popularity of it overwhelmed us! Clearly the mixture of a Google tour, Google goodies, food, drink and testing is an intoxicating cocktail. The event was not taped but GTAC will be and I'll likely not have been part of a two hour party before that talk! Some things, I think, are better off unrecorded and off the record...We will be having more of these events in the future. We'll learn from this and make sure you have plenty of warning. Thanks for understanding and if any rumors emerge from this event about things I may have said on stage...you can't prove anything!", "By James WhittakerEver look at a testing problem and wonder how to solve it? If so you know what it feels like to lack domain expertise. Sometimes this is user-oriented knowledge. Testing a flight simulator requires knowledge of how to fly a plane. Testing tax preparation software requires knowledge of accounting. Other times the knowledge is more problem-oriented. Testing a mobile operating system means understand how Wi-Fi and device drivers work. Whenever the bill of materials contains a testing problem that the risk analysis identifies as important, the expertise needed to test it needs to be on the testing team. Hire it, contract it, outsource it. Whatever it takes to ensure that people who know what they are doing and have experience doing it are on staff for the duration of the project. There is no technological substitution for expertise.It doesn't matter how good you think you are at exploratory testing, if you don't understand how something works find someone who does.", "", "", "", "Google is holding a testing event in our NY office Wednesday, September 15 at 5:30pm. This includes a tour of our local offices and a live talk on how Google does testing by our own James Whittaker. Rumor has it he's using an early version of his GTAC talk. Lots of food, drink and Google giveaways.", "By James WhittakerPossessing a bill of materials means that we understand the overall size of the testing problem. Unfortunately, the size of most testing problems far outstrips any reasonable level of effort to solve them. And not all of the testing surface is equally important. There are certain features that simple require more testing than others. Some prioritization must take place. What components must get tested? What features simply cannot fail? What features make up the user scenarios that simply must work?In our experience it is the unfortunate case that no one really agrees on the answers to these questions. Talk to product planners and you may get a different assessment than if you talk to developers, sales people or executive visionaries. Even users may differ among themselves. It falls on testers to act as the user advocates and find out how to take into account all these concerns to prioritize how testing resources will be distributed across the entire testing surface.The term commonly used for this practice is risk analysis and at Google we take information from all the projects stakeholders to come up with overall numerical risk scores for each feature. How do we get all the stakeholders involved? That's actually the easy part. All you need to do is assign numbers and then step back and have everyone tell you how wrong you are. We've found being visibly wrong is the best way to get people involved in the hopes they can influence getting the numbers right! Right now we are collecting this information in spreadsheets. By the time GTAC rolls around the tool we are using for this should be in a demonstrable form.", "By James WhittakerWhen are you finished testing? It\u2019s the age old quality question and one that has never been adequately answered (other than the unhelpful answer of never). I argue it never will be answered until we have a definition of the size of the testing problem. How can you know you are finished if you don\u2019t fully understand the task at hand? Answers that deal with coverage of inputs or coverage of code are unhelpful. Testers can apply every input and cover every line of code in test cases and still the software can have very serious bugs. In fact, it\u2019s actually likely to have serious bugs because inputs and code cannot be easily associated with what\u2019s important in the software. What we need is a way to identify what parts of the product can be tested, a bill of materials if you will, and then map our actual testing back to each part so that we can measure progress against the overall testing goal.This bill of materials represents everything that can be tested. We need it in a format that can be compared with actual testing so we know which parts have received enough testing and which parts are suspect. We have a candidate format for this bill of materials we are experimenting with at Google and will be unveiling at GTAC this year.", "By James WhittakerEach year, about this time, we say goodbye to our summer interns and bid them success in the upcoming school year. Every year they come knowing very little about testing and leave, hopefully, knowing much more. This is not yet-another-plea to universities to teach more testing, instead it is a reflection on how we teach ourselves.I like to experiment with metaphors that help people \"get it.\" From attacks to tools to tours to the apocalypse, I've seen my fair share. This summer, I got a lot of aha moments from various interns and new hires likening testing to cooking. We're chefs with no recipes, just a list of ingredients. We may all end up making a different version of Testing Cake, but we better at least be using the same set of ingredients. What are the ingredients? I'll list them here over the next couple of weeks. Please feel free to add your own and I'll hope you don't steal my thunder by getting them in faster than I. Right now I have a list of 7.Ingredient 1: Product expertiseDevelopers grow trees, testers manage forests. The level of focus of an individual developer should be on the low level concerns of building reliable and secure components. Developers must maintain intellectual mastery from the UI to low level APIs and memory usage of the features they code. We don\u2019t need them distracted and overwhelmed with system wide product expertise duties as well. Testers manage system wide issues and rarely have deep component knowledge. As a manager of the forest, we can treat any individual tree abstractly. Testers should know the entire landscape understanding the technologies and components involved but not actually taking part in their construction. This breadth of knowledge and independence of insight is a crucial complement to the developer\u2019s low level insights because testers must work across components and tie together the work of many developers when they assess overall system quality.Another way to think about this is that developers are the domain experts who understand the problem the software is solving and how it is being solved. Testers are the product experts who focus on the breadth of technologies used across the entire product.Testers should develop this product expertise to the extent that they cannot be stumped when asked questions like \"how would I do this?\" with their product. If I asked one of my Chrome testers any question about how to do anything with Chrome concerning installation, configuration, extensions, performance, rendering ... anything at all ... I expect an answer right away. An immediate, authoritative and correct answer. I would not expect the same of a developer. If I can stump a tester with such a question then I have cause for concern. If there is a feature none of us know about or don't know completely then we have a feature that might escape testing scrutiny. No, not on our watch!Product expertise is one ingredient that must be liberally used when mixing Testing Cake.", "By Philip Zembrod\n\n\n\nIn my quest to explore TDD I recently found another propery of TDD-written code that I hadn't expected: When reviewing or just reading such code, it's often best to first read the tests.\n\n\n\nWhen I look at new code or a code change, I ask: What is this about? What is it supposed to do? Questions that tests often have a good answer for. They expose interfaces and state use cases. This is cool, I thought, and decided to establish test-first reading as my code-reviewing routine. Of course this just applies the specification aspect of tests: Reading the specs before reading the code.\n\n\n\nOnly it didn't always work. From some tests I just failed to learn the point and intention of the tested code. Often, though not always, these were tests that were heavy with mocks and mock expectations.\n\n\n\nMocks aren't always a helpful tool, was my first conclusion. The phrase \"Good mocks, bad mocks\" popped up in my mind. I began to appreciate fakes again - and the people who write them. But soon I realized that this was about more than mocks vs. fakes vs. dummies vs. other Friends You Can Depend On. I was really looking at how well tests fulfill their role as specification.\n\n\n\nTDD teaches that tests are a better specification than prose. Tests are automatically enforced, and get stale less easily. But not all tests work equally well as specification! That's what test driven code reviewing taught me.\n\n\n\nI began to call them well-specifying tests and poorly-specifying tests. And the specification aspect isn't just some additional benefit, it's a really crucial property of tests. The more I thought about it, the more I saw: It is connected to a lot of things that first weren't obvious to me:\n\n\nIf tests are poorly-specifying, then possibly the tested product is poorly specified or documented. After all, it's the tests that really make sure how a product behaves. If they don't clearly state what they test, then it's less clear how the product works. That's a problem.\n\n\nWell-specifying tests are more robust. If a test just does and verifies things of which the architect or product manager will readily say \"yes, we need that\" then the test will survive refactorings or new features. Simply because \"yes, we need that.\" The test's use case is needed, its conditions must hold. It needn't be adapted to new code, new code must pass it. False positives are less likely.\n\n\n\nCorollary: Well-specifying tests have higher authority. If a test fails, a natural reaction is to ask \"is this serious?\" If a test is poorly-specifying, if you don't really understand what it is testing, then you may say \"well, maybe it's nothing\". And you may even be right! If a test is well-specifying, you'll easily see that its failing is serious. And you'll make sure the code gets fixed.\n\n\n\nI'm now thinking about an authority rank between 0 and 1 as a property of tests. It could be used to augment test coverage metrics. Code that is just covered by poorly-specifying tests would have poor authority coverage, even if the coverage is high. Quantifying an authority rank would be a conceptual challenge, of course, but part of it could be how well test driven code reviewing works with a given test.\n\n\n\nP.S. If anyone suspects that I'm having some fun inventing terms beginning with \"test driven,\" I'll plead guilty as charged. :-)", "by Alberto Savoia\nI first posted this article a few years ago on the Artima Developer website; but the question of what's adequate code coverage keeps coming up, so I thought it was time for a repost of Testivus wisdom on the subject.\n\nTestivus on Test Coverage\nEarly one morning, a young programmer asked the great master:\n\n\n\u201cI am ready to write some unit tests. What code coverage should I aim for?\u201d\nThe great master replied:\n\n\u201cDon\u2019t worry about coverage, just write some good tests.\u201d\nThe young programmer smiled, bowed, and left.\n\n\nLater that day, a second programmer asked the same question.\nThe great master pointed at a pot of boiling water and said:\n\n\u201cHow many grains of rice should I put in that pot?\u201d\nThe programmer, looking puzzled, replied:\n\n\u201cHow can I possibly tell you? It depends on how many people you need to feed, how hungry they are, what other food you are serving, how much rice you have available, and so on.\u201d\n\u201cExactly,\u201d said the great master.\nThe second programmer smiled, bowed, and left.\n\n\nToward the end of the day, a third programmer came and asked the same question about code coverage.\n\n\u201cEighty percent and no less!\u201d Replied the master in a stern voice, pounding his fist on the table.\nThe third programmer smiled, bowed, and left.\n\n\nAfter this last reply, a young apprentice approached the great master:\n\n\u201cGreat master, today I overheard you answer the same question about code coverage with three different answers. Why?\u201d\nThe great master stood up from his chair:\n\n\u201cCome get some fresh tea with me and let\u2019s talk about it.\u201d\nAfter they filled their cups with smoking hot green tea, the great master began:\n\n\u201cThe first programmer is new and just getting started with testing. Right now he has a lot of code and no tests. He has a long way to go; focusing on code coverage at this time would be depressing and quite useless. He\u2019s better off just getting used to writing and running some tests. He can worry about coverage later.\nThe second programmer, on the other hand, is quite experienced both at programming and testing. When I replied by asking her how many grains of rice I should put in a pot, I helped her realize that the amount of testing necessary depends on a number of factors, and she knows those factors better than I do \u2013 it\u2019s her code after all. There is no single, simple, answer, and she\u2019s smart enough to handle the truth and work with that.\u201d\n\u201cI see,\u201d said the young apprentice, \u201cbut if there is no single simple answer, then why did you tell the third programmer \u2018Eighty percent and no less\u2019?\u201d\nThe great master laughed so hard and loud that his belly, evidence that he drank more than just green tea, flopped up and down.\n\n\u201cThe third programmer wants only simple answers \u2013 even when there are no simple answers \u2026 and then does not follow them anyway.\u201d\nThe young apprentice and the grizzled great master finished drinking their tea in contemplative silence.", "By James A. WhittakerI've had more than a few emails about \"antenna-gate\" asking me to comment and suggesting clever, stabbing rebukes to a fallen competitor. I might aim a few of those at my own team in the future, some were genuinely funny, but none of them will appear here. Instead I offer first a word of caution and second a reflection that my Mom used to intone whenever disaster occurred around her. It's called \"counting your blessings.\" First, a caution that those of us who live in glass houses really should keep stones at arms length. The only way anyone can rebuke Apple, without risk of waking up one morning sucking on their own foot, is if they write no software or have no users. Apple does a lot of the former and they enjoy many of the latter. Bugs like this make me sick when they are mine and nervous when they aren't. If any tester in the industry isn't taking stock right now then they either aren't producing any software or aren't in possession of any users, at least ones they wish to keep. Second, taking stock has made me realize that I enjoy some important blessings that make the infinite task of testing so much more manageable. Indeed, the three blessings I count here are really the reason that testing doesn't fail more often than it does.  The Blessing of Unit TestingI am thankful for early cycle testing thinning out the bug herd. In late cycle testing major bugs are often masked by minor bugs and too many of the latter can hamper the search for the former. Every bug that requires a bug report means lost time. There is the time spent to find the bug; time spent to reproduce and report it; time to investigate its cause and ensure it is not a duplicate; time to fix it, or to argue about whether it should be fixed; time to build the new version and push it to the test lab; time to verify the fix; time to test that the fix introduced no additional bugs. Clearly the smaller the population to begin with, the easier the task becomes. Solid unit testing is a tester's best friend.The Blessing of RarityI am thankful that the vast majority of bugs that affect entire user populations are generally nuisance-class issues. These are typically bugs concerning awkward UI elements or the occasional misfiring of some feature or another where workarounds and alternatives will suffice until a minor update can be made. Serious bugs tend to have a more localized effect. True recall class bugs, serious failures that affect large populations of users, are far less common. Testers can take advantage of the fact that not all bugs are equally damaging and prioritize their effort to find bugs in the order of their seriousness. The futility of finding every bug can be replaced by an investigation based on risk.Risk analysis is so important that we've built an internal tool to help guide testers in performing it. Code-named \"Testify\" this tool streamlines the process of risk analysis, at least the way we do it at Google. We're working on open-sourcing an early prototype in time for GTAC 2010 (I can hear my team cringing now ... \"you promised it when?\"). The Blessing of RepetitionI am thankful that user behavior is highly repetitive. There are features that enjoy heavy usage across user populations and features that are far less popular. Mobile phones are a good example of this. The phone is constantly establishing connections to networks. Certain features like making and receiving calls, texting and so forth are used more often than taking pictures or searching maps. The popularity of user applications is a matter of hard data, not guesswork. Knowing what users do most often, less often and least often means testing resources can be applied with a commensurate amount of force and that testing itself can be patterned after actual usage profiles.Testers can gain a great deal from taking the user\u2019s point of view and weaving usage concerns into the software testing process. Focusing on the user ensures that high impact bugs are found early and software revisions that break key user scenarios are identified quickly and not allowed to persist.Apple may be the company in the news today, who knows who it will be tomorrow. Every company that produces software people care about has either been there or will be there. The job is simply too big for perfection to be an option. But there are key advantages we have that make the job manageable. Put down the stones and make sure that what few blessing we testers possess are being exploited for everything they are worth. Hopefully, your company will be spared and the next time a company suffers such a bug you won't be the one making excuses. Perhaps you'll be lucky enough to be the one saying, \"there but for the grace of testing go I.\"", "by Alberto Savoia\n\n\n\nNote: Apparently, there were lots of downloads of the Testivus booklet and I hit some kind of quota on my personal account.  If you have problems with reaching the original link below, please try this new download link or this one.\n\n\n\nA major topic at this year's GTAC conference is going to be testability: \"We also want to highlight methodologies and tools that can be used to build testability into our products.\"  That's great!\n\n\n\nTestability is one of the most important, yet overlooked, attributes of code \u2013 and one that is not discussed enough.  That's unfortunate, because by the time the issue of testability comes up in a project it's usually too late.  As preparation and seeding for GTAC, I though it would be fun and useful to get some discussions on testability going.  So here we go, feel free to chime in with your thoughts.\n\n\n\n\nA few years ago, after watching one too many episodes of Kung Fu, I was inspired to write a pretentious and cryptic little booklet about testing called \"The Way of Testivus\" (PDF).\n\n\n\n\n\n\n\nTestivus addresses the issue of testability in a few places, but I would like to start the discussion with this maxim:\n\n\n\n\n\n\n\n\nTo me, \"Think of code and tests as one\" is the very foundation of testability.  If you don't think about testing as you design and implement your code, you are very likely to make choices that will impair testability when the time comes.  This position seemed obvious and non-controversial to me at the time I wrote it, and I still stand by it.  Most people seem to agree with it as well, and more than one person told me that it's their favorite and most applicable maxim from all of Testivus.  There are however three groups of people who found issue with it.\n\n\n\nSome of the people, mostly from the TDD camp, think that my choice of words leaves too much wiggle room: \"Thinking about the tests is not enough, they should be writing and running those tests at the same time.\" \n\n\n\nOthers think that code and tests should not be thought of as one at all, but they should be treated independently \u2013 ideally as adversaries: \"I don't want code and tests to be too \"friendly\".  Production code should not be changed or compromised to make the testing easier, and tests should not trust the hooks put in the code to make it more testable.\"  Most of the people in this camp are not big fans of unit/developer testing in the first place, but not all.  One person, a believer in developer testing, told me that he gets the best results with a Dr. Jekyll and Mr. Hyde approach.  He assumes two different roles and personalities based on whether he's coding or testing his own code. When coding, he's the constructive Dr. Jekyll who focuses on elegant and efficient design and algorithms \u2013 and does not worry about testability.  When testing, he turns into the destructive Mr. Hyde; he tries to forget that it's his code or how he implemented it, and puts all his energy and anger into trying to break it.  Sounds like it could work quite well \u2013 though I don't think I'd want this person as an office mate during the Mr. Hyde phase.\n\n\n\nA third group, thought that the maxim was fine for unit tests, but not applicable to other types of tests that were best served by an adversarial black-box approach.  \n\n\n\nWhat are your thoughts?  Is it enough to think about testability when designing or writing the code, or must you actually write and run some tests in parallel with the code?  Does anyone agree with the position that code and tests should be designed and developed in isolation?  Are there other Dr. Jekylls and Mr. Hydes out there?\n\n\n\nAlberto", "We are thrilled to announce that GTAC 2010 keynotes are finalized.  We are very fortunate to have three world-renowned software testing icons: Robert Binder, Dr. Jeff Offutt and Dr. James Whittaker. Robert, Jeff and James bring to GTAC a powerful combination of practical and theoretical experience to help us address key aspect of this year's theme: Test to Testability. Robert will kick-off Day 1 with a keynote on the critical role of testability. On Day 2, Jeff will share his experiences on Automatic Test Generation from source code and about a technique he invented, Bypass Testing, for black-box testing of web applications. James will close the conference with a keynote focusing on the Test(ing) challenges and how to get ahead of them. More details on the specifics of their talk coming soon!Here are brief bios of our esteemed speakers: Robert BinderRobert V. Binder is a software entrepreneur and technologist with over 34 years of systems engineering experience. His 1994 analysis of software testability http://portal.acm.org/citation.cfm?id=184077 has had a continuing influence on research and practice in this area. As principal of System Verification Associates, he leads teams that deliver advanced IT assurance solutions. He was recently awarded a U.S. Patent for a unique approach to model-based testing of mobile systems. He is a member of the Editorial Board of Software Testing, Verification, and Review and internationally recognized as the author of the definitive Testing Object-Oriented Systems: Models, Patterns, and Tools. Robert holds an MS in Electrical Engineering and Computer Science from the University of Illinois at Chicago and a MBA from the University of Chicago. He is an IEEE Senior Member.Dr. Jeff OffuttDr. Jeff Offutt is Professor of Software Engineering at George Mason University. He has invented numerous test strategies, published over 120 refereed research papers, and is co-author of the textbook Introduction to Software Testing. He is editor-in-chief of Wiley's journal of Software Testing, Verification and Reliability; steering committee chair for the IEEE International Conference on Software Testing, Verification, and Validation; and program chair for ICST 2009. He has consulted with numerous companies on software testing, usability, and software intellectual property issues. Offutt is on the web at http://www.cs.gmu.edu/~offutt/Dr. James WhittakerDr. Whittaker is currently the Engineering Director over engineering tools and testing for Google's Seattle and Kirkland offices. He holds a PhD in computer science from the University of Tennessee and is the author or coauthor of four acclaimed textbooks. How to Break Software,  How to Break Software Security (with Hugh Thompson) and How to Break Web Software (with Mike Andrews). His latest is Exploratory Software Testing: Tips, Tricks, Tours and Techniques to Guide Test Design and he's authored over fifty peer-reviewed papers on software development and computer security. He holds patents on various inventions in software testing and defensive security applications and has attracted millions in funding, sponsorship, and license agreements while a professor at Florida Tech. He has also served as a testing and security consultant for dozens of companies and spent 3 years as an architect at Microsoft.Reminder: Call for proposalsIf you would like to present at this year\u2019s GTAC please remember to submit your proposal by the July 9th deadline. Please visit http://www.gtac.biz/call-for-proposals for details.Sujay Sahni for the GTAC 2010 Committee", "By Philip ZembrodIn an earlier post on trying out TDD I wrote how my mindset while coding changed from fear of bugs in the new code to eager anticipation to see the new code run through and eventually pass the already written tests. Today I want to tell about integrating components by writing integration tests first.In a new project we decided to follow TDD from the start. We happily created components, \u201ctesting feature after feature into existence\u201d (a phrase I love; I picked it up from a colleague), hitting a small test coverage of around 90% from the start. Obviously, when it came to integrating the components into a product, the obvious choice was to do that test-driven, too. So how did that go?What I would have done traditionally was select a large enough set of components that, once integrated, should make up something I could play with. Since at least a minimum UI would be needed, plus something that does visible or useful things, preferably both, this something would likely have been largish, integrating quite a few components. With the playing around and tryout, I\u2019d enter debugging, because of course it wouldn\u2019t work at first attempt. The not-too-small number of integrated components would make tracking the cause of failures hard, and anticipating all this while coding, I\u2019d have met the well-known fearful mindset again, slowing me down, as I described in my initial TDD post.How did TDI change this game for me? I realized: With my unit test toolbox that can test any single component, I can also test an integration of 2 components regardless of whether they have a UI or do something visible. That was the key to a truly incremental process of small steps.First, write the test for 2 components, run it and see it fail to make sure the integration code I\u2019m about to write is actually executed by the test. Write that bit of code, run the test and see it succeed. If it still fails, fix what's broken and repeat. Finding what's broken in this mode is usually easy enough because the increments are small. If the test failure doesn\u2019t make obvious what\u2019s wrong, adding some verifications or some logging does the trick. A debugger should never be needed; automated tests are, after all, a bit like recorded debugging sessions that you can replay any time in the future.Repeating this for the 3rd component added, the 4th, etc., I could watch my product grow, with new passing tests every day. Small steps, low risks in each, no fear of debugging; instead, continuous progress. Every day this roaring thrill: It works, it works! Something\u2019s running that didn\u2019t run yesterday. And tomorrow morning I\u2019ll start another test that will run tomorrow evening, most likely. Or already at lunchtime. Imagine what kind of motivation and acceleration this would give you. Better, try it out for yourself. I hope you\u2019ll be as amazed and excited as I am.What are the benefits? As with plain TDD, I find this fun-factor, this replacement of dread of debugging by eagerness for writing the next test to be able to write and run the next code the most striking effect of TDI.The process is also much more systematic. Once you have specified your expectations at each level of integration, you\u2019ll verify them continuously in the future, just by running the tests. Compare that to how reproducible, thorough and lasting your verification of your integration would be if you\u2019d done it manually.And if you wrote an integration test for every function or feature that you cared about during integration, then you can make sure each of them is in shape any time by just running the tests. I suspect one can\u2019t appreciate the level of confidence in the code that creates until one has experienced it. I find it amazing. I dare you to try it yourself!P.S. On top of this come all the other usual benefits of well-tested code that would probably be redundant to enumerate here, so I won\u2019t. ;-)", "Google Test Automation Conference (GTAC) 2010Call for Attendance & ProposalsWe are happy to announce that the application process is now open for Attendance and Proposals for the Fifth Google Test Automation Conference (GTAC), to be held in Hyderabad, India on October 28 - 29th.As in previous years, GTAC is an invitation only conference where we enable sharing of great ideas and active participation to challenge and refine our thoughts and experiences. As such the the application process expects you to share your ideas and insights that you would bring to the conference and how these would further the discussion about this year\u2019s theme of Test to Testability. This information will help the committee select a balanced audience of seasoned practitioners, students and academics. Also this year, we are introducing a participant-driven format that will give the power to the attendees to select and voice their opinion on the speakers and the content! To make these changes, we are opening up proposals and attendance applications simultaneously. Once the initial set of participants are finalized, we will conduct online viewing and voting by the participants for presentations. How to applyFor Attendance: Please visit http://www.gtac.biz/call-for-attendance For Proposals (to present): Please visit http://www.gtac.biz/call-for-proposalsDeadlineThe due date for both categories of applications is July 9th, 2010. Registration FeesThere are no registration fees. Please check the FAQ page for more information.Further informationGeneral website: http://www.gtac.biz/Call for proposals: http://www.gtac.biz/call-for-proposalsCall for attendance: http://www.gtac.biz/call-for-attendanceFAQ: http://www.gtac.biz/faqQuestions: Email us at gtac-2010@google.comWe look forward to your applications and a great GTAC! Finally we would appreciate your help in helping us spread the word about this event.RegardsSujay Sahni on behalf of the GTAC 2010 Committee", "Web Application Exploits and Defenses\n\nby Bruce Leban in Google Kirkland\n\nhttp://google-gruyere.appspot.com/\n\nIf you want your application to be as secure as possible, you need to learn how Evil People think. And you'll want to use that knowledge to do penetration testing: attacking your own application to try to find bugs.\n\nTo help you understand how applications can be attacked and how to protect them from attack, we've created the \u201cWeb Application Exploits and Defenses\u201d codelab. The codelab uses Gruyere, a small, cheesy, web application that is full of real world bugs.  \n\nIn the codelab, you'll learn how to:\n\n\nAttack  a web application  to find and exploit common web security vulnerabilities.\n\n\nAvoid and  fix these common bugs.\n\n\n\nGruyere is chock full of cool features, and the more features an application has the larger the attack surface. Your application probably has features just like these:\n\nCan you match each feature to the vulnerability that it exposes and the exploit it enables?\n\n\n\n\n  \n \nFeature\n \n \nNew     template languageHTML allowed in snippetsFile upload     capabilityAJAXWeb-based admin console\n \n\n\n\n\n  \n \nVulnerability\n \n \nCross     Site Scripting (XSS)Cross Site Request Forgery (XSRF)Cross     Site Script Inclusion (XSSI)Path traversalClient-state     manipulation\n \n\n\n\n\n  \n \nExploit\n \n \nInformation     disclosureElevation of privilegeDenial of Service     (DoS)SpoofingCode execution\n \n\n\n\nHa! Tricked you! Each of these features introduces multiple vulnerabilities. And each vulnerability can be exploited in multiple ways. The codelab walks you step by step through each vulnerability, with progressive hints guiding you on how to find them, how to exploit them and how to avoid them.\n\nHere are some examples of fictitious attacks against Google applications. Do you recognize them? (answers below)\n  \n \nhttp://www.gmail.com/?search=in:spam+%3Cscript%3EmoveToInbox(selectAll())%3C/script%3Ehttp://www.blogger.com/delete-blog.ghttp://www.picasa.com/../../../../../../../etc/passwdhttp://www.youtube.com/admin?v=Vr0oK3gMzK&action=rickrollhttp://checkout.google.com/buy?order=4815162342&total=0.01\n \n\n\n\nAre you sure that your application isn't vulnerable to similar attacks!?\nCheck out the Toilet-Friendly Version for the answers", "By James A. WhittakerYes I know, I've been quiet. Seriously heads down shipping products and developing what I think are some pretty cool new testing ideas and tools. Perhaps GTAC will be the chance for you to judge that for yourselves. Perhaps it will be worth the wait. I hope I am invited to speak at GTAC. (Is this an appropriate forum for such lobbying? Should I open it up to a vote on whether I should or should not be there? I am happy not going as India is a long trip, but this is GTAC we are talking about!) There are a number of things that will be ready to either debut or, if I am lucky, open source by then. Would you like to see a Chrome extension to display bug, test case, coverage and other information as an overlay on top of your web app UI? Imagine being able to see bugs at their exact location on the UI, report bugs by simply right-clicking the errant part of the web page, see footprints where test cases (automated and manual and across multiple testers) have been and lots more useful information. Are you tired of querying databases to see these things and just want your test data as a cellophane wrapper around your web app UI? If you were at STAR this week, you got a preview. But that presentation is already out of date. Would you like to be able to write automated test cases that can control your web app, your browser and the operating system they are running on? Well if that stack contains Chrome and Chrome OS, you can do it with a new web test framework we are developing. Would you like to do all of this with Java Script? Sound like magic? Well I think the Web Test Framework is appropriately named: WTF. Would you like to see a record and playback facility that records directly to Java Script and is actually built-in to your browser? A R/P tool that handles AJAX and won't get confused by self-updating web pages? That stores recordings directly into a test case management system that is accessible to the world? Would you like to hear about the extensive library of testing tours we have developed and how our manual testing strategy across the web-app/Chrome/Chrome-OS stack is shaping up? These are some of the things that have kept me from this blog. Forgive me. I will report on them here and, with a little pressure directed toward Sujay The Decider perhaps demo them at GTAC.", "Thanks for all the enquiries about this year's GTAC event. We have been busy planning and are now ready to share the details. GTAC 2010 will be hosted by Google office in Hyderabad, India on October 28th & 29th, 2010.As in previous years, the focus of the conference will be on solving software engineering challenges using tools and automation. This year the conference theme is \"Test to Testability\". With this theme our goal is to look beyond the challenges faced during testing and the complex products that are being developed today. We also want to highlight methodologies and tools that can be used to build testability into our products and to look at how developer tools can be a means towards effective and efficient testing. GTAC 2010 hopes to bring together a group that shares lessons learned and practical experiences regarding testing web apps, services, and systems.One of the strengths of the conference is that it's driven by a peer group and vocal participation. As in previous years, GTAC is an invitation only conference to share great ideas and to have your thoughts challenged and refined. When you apply, we will be asking you to share with us what ideas and insights you'll be bringing to the conference and how these can further the discussion. The application process for attending will be opening in mid-May 2010.Today we are also launching the official site for this event and will be updating it and this blog with relevant information in the coming weeks. The next announcement will be a call for attendance and proposals, so do watch these spaces. Please send suggestions, questions and recommendations to: gtac-2010@google.com or post your comments here to this blog.Looking forward to having a great set of participants and presenters to make this a fun and valuable learning event!Sujay Sahni on behalf of the GTAC 2010 Committees", "Now that ICST 2010 has concluded, I'm going ahead and posting the keynote paper and the slides. Enjoy! Patrick CopelandLet me know if you have comments or questions.Here's a link to the paper. They are semi-complementary.", "Drop me a resume and a brief note if you're interested in joining Google as an SET. email: test.eng.hiring@gmail.com. Thanks -- Patrick CopelandWhen we hire people we look for folks with a \"testing DNA.\" These are people who are great computer scientists at their core, but also are very curious, love software, and are passionate about test engineering. People who have those characteristics tend to pursue challenges and continue to learn.\ufeff Are you one of us? We have positions all over the US and the world.What is a SET?At Google, Software Engineers in Test (SET) develop test frameworks and build robust, scalable, and effective tests.  SETs spend a majority of their time coding in either C++, Java, or scripting in Python.  A SET is a software engineer, a core developer, who has a passion for test engineering.   How is testing done differently at Google?Literally within milliseconds of a code check-in, our build process will automatically select the appropriate tests to run based on dependency analysis, run those tests and report the results. By reducing the window of opportunity for bad code to go unnoticed, overall debugging and bug isolation time is radically reduced. The net result is that the engineering teams no longer sink hours into debugging build problems and test failures.Development teams write good tests because they care about the products, but also because they want more time to spend writing features and less on debugging.Testing teams focus on higher abstractions, like identifying latencies, system or customer focused testing, and enabling the process with tools.SETs avoid becoming codependents within this system and generally do not write unit tests or other activities that are best done by the developer.More about SETsOur SET\u2019s spend time developing code to prevent bugs. Google has a strong cultural emphasis on developers improving quality (i.e. unit tests, code reviews, design reviews, root cause analysis).  We want our engineers to spend their time innovating - not fixing bugs.SETs enable products to launch faster.  They have great influence over internal processes and how developers write code.    One of Google's less understood capabilities is our massive distributed computing environment. The testing groups exploit this infrastructure to do huge amounts of work very quickly and elegantly. For someone who wants to learn and grow as an engineer, the uninhibited access to the entire code base is a unique opportunity.", "I'll be a presenting a paper at ICST 2010 in Paris April 6-10 about how Google tests and builds software. Here's a pointer to the program if you are interested. Also here's a link to the abstract of the talk itself. I'll publish the paper after the talk here. Hopefully, I'll see some of you there!Posted by Patrick Copeland", "By James A. WhittakerFlashback. It's 1990. Chances are you do not own a cell phone. And if you do it weighs more than a full sized laptop does now. You certainly have no iPod. The music in your car comes from the one or two local radio stations that play songs you can tolerate and a glove box full of CDs and cassettes. Yes, I said cassettes...you know the ones next to those paper road maps. Music on the go? We carried our boom boxes on our shoulder back then. If you are a news junkie, you get your fix from the newspaper or you wait until 6 ... or 11. Sports? Same. Oh and I hope you don't like soccer or hockey because you can't watch that stuff in this country more often than every four years. Go find a phone book if you want to call someone and complain. I could go on, and on, and on, but you get the point. Oh wait, one more: how many of you had an email address in 1990? Be honest. And the people reading this blog are among the most likely to answer that affirmatively. The world is different. The last 20 years has changed the human condition in ways that no other 20 year period can match. Imagine taking a 16 year old from 1990 and transplanting him or her to a 2010 high school. Culture shock indeed. Imagine transporting a soccer mom, a politician, a university professor... Pick just about any profession and the contrast would be so stark that those 1990 skills would be a debilitating liability. Except one: that of a software tester. A circa 1990 tester would come from a mainframe/terminal world. Or if they were on the real cutting edge, a locally networked PC. They'd fit into the data center/slim client world with nary a hiccup. They'd know all about testing techniques because input partitioning, boundary cases, load and stress, etc, are still what we do today. Scripting? Yep, he'd be good there too. Syntax may have changed a bit, but that wouldn't take our time traveler long to pick up. That GEICO caveman may look funny at the disco, but he has the goods to get the job done. Don't get me wrong, software testing has been full of innovation. We've minted patents and PhD theses. We built tools and automated the crud out of certain types of interfaces. But those interfaces change and that automation, we find to our distress, is rarely reuseable. How much real innovation have we had in this discipline that has actually stood the test of time? I argue that we've thrown most of it away. A disposable two decades. It was too tied to the application, the domain, the technology. Each project we start out basically anew, reinventing the testing wheel over and over. Each year's innovation looks much the same as the year before. 1990 quickly turns into 2010 and we remain stuck in the same old rut. The challenge for the next twenty years will be to make a 2010 tester feel like a complete noob when transported to 2030. Indeed, I think this may be accomplished in far less than 20 years if we all work together. Imagine, for example, testing infrastructure built into the platform. Not enough for you? Imagine writing a single simple script that exercises your app, the browser and the OS at the same time and using the same language. Not enough for you? Imagine building an app and having it automatically download all applicable test suites and execute them on itself. Anyway, what are you working on?Interested? Progress reports will be given at the following locations this year:Swiss Testing Day, Zurich, March 17 2010STAR East, Orlando, May 2010GTAC, TBD, Fall 2010Here's to an interesting future.", "I recently did an interview with Matt Johnston of uTest (a community based testing company) that talks about our philosophy and approach to testing at Google. Let me know what you think.  Part 1, Part 2, Part 3Posted by Patrick Copeland", "By James A. WhittakerW. Edwards Deming helped to revolutionize the process of manufacturing automobiles in the 1970s and a decade later the software industry ran with the manufacturing analogy and the result was nearly every waterfall, spiral or agile method we have. Some like TQM, Cleanroom and Six Sigma are obvious descendants of Deming while others were just heavily influenced by his thinking. Deming was the man.I repeat, was. My time testing in Google's data center makes it clear that this analogy just doesn't fit anymore. I want a new one. And I want one that helps me as a tester. I want one that better guides my behavior.We just don't write or release software the way we used to. Software isn't so much built as it is grown. Software isn't shipped ... it's simply made available by, often literally, the flip of a switch. This is not your father's software. 21st century development is a seamless path from innovation to release where every phase of development, including release, is happening all the time. Users are on the inside of the firewall in that respect and feedback is constant. If a product isn't compelling we find out much earlier and it dies in the data center. I fancy these dead products serve to enrich the data center, a digital circle of life where new products are built on the bones of the ones that didn't make it.In our father's software and Deming's model we talk about quality control and quality assurance while we play the role of inspector. In contrast, my job seems much more like that of an attending physician. In fact, a medical analogy gives us some interesting parallels to think about software testing. A physician's hospital is our data center, there is always activity and many things are happening in parallel. Physicians have patients; we have applications and features. Their medical devices are our infrastructure and tools. I can picture my application's features strewn across the data center in little virtual hospital beds. Over here is the GMail ward, over there is Maps. Search, of course, has a wing of its own and Ads, well, they all have private rooms.In a hospital records are important. There are too many patients with specific medical conditions and treatment histories for any physician to keep straight. Imagine walking up to the operating table without examination notes and diagnoses? Imagine operating without a constant stream of real time health data?Yet as software testers we find ourselves in this situation often. That app lying in our data center has been tested before. It has been treated before. Where are our medical notes?So let's add little clipboards to the virtual data center beds in which our apps lay. Let's add equipment to take vitals and display them for any attending tester to see. Like human patients, apps have a pulse, data runs through code paths like blood through veins. There are important things happening, countable events that lead to statistics, indicators and create a medical history for an attending tester to use in whatever procedure they must now perform. The work of prior testers need not be ignored.It's an unsettling aspect of the analogy that I have put developers in the role of creator, but so be it. Like other metaphorical creators before them they have spawned intrinsically flawed creatures. Security is their cancer, privacy their aging. Software is born broken and only some things can be fixed. The cancer of security can only be managed. Like actual aging, privacy is a guarantee only young software enjoys. Such is the life of a data center app.But it is the monitors and clipboards that intrigue me. What do they say of our digital patients? As an app grows from concept into adolescence what part of their growth do we monitor? Where is the best place to place our probes? How do we document treatment and evaluations? Where do we store the notes about surgeries? What maladies have been treated? Are there problematic organs and recurrent illness? The documents and spreadsheets of the last century are inadequate. A patient's records are only useful if they are attached to the patient, up-to-date and in full living color to be read by whatever attending tester happens to be on call.This is the challenge of the new century of software. It's not a process of get-it-as-reliable-as-possible-before-we-ship. It's health care, cradle to grave health care ... prevention, diagnosis, treatment and cure.So slip into your scrubs, it's going to be a long night in the ER.", "By James A. WhittakerGoogle is hiring. We have openings for security testers, test tool developers, automation experts and manual testers. That's right, I said manual testers.As a result of all this interviewing I've been reading a lot of interview feedback and wanted to pass along some insights about how these applicants approach solving the testing problems we ask in our interviews. I think the patterns I note in this post are interesting insights into the mind of the software tester, at least the ones who want to work for Google.One of the things our interviewers like to ask is 'how would you test product xyz?' The answers help us judge a tester's instincts, but after reading many hundreds of these interviews I have noticed marked patterns in how testers approach solving such problems. It's as though testers have a default testing framework built into their thinking that guides them in choosing test cases and defines the way they approach test design.In fact, these built-in frameworks seem to drive a tester's thinking to the extent that when I manage to identify the framework a tester is using, I can predict with a high degree of accuracy how they will answer the interviewers' questions. The framework defines what kind of tester they are. I find this intriguing and wonder if others have similar or counter examples to cite.Here are the frameworks I have seen just in the last two weeks:The Input Domain Framework treats software as an input-output mechanism. Subscribers of this framework think in terms of sets of inputs, rules about which inputs are more important and relationships between inputs, input sequences and outputs. This is a common model in random testing, model-based testing and the testing of protocols and APIs. An applicant who uses this framework will talk about which inputs they would use to test a specific application and try to justify why those inputs are important.The Divide and Conquer Framework treats software as a set of features. Subscribers begin by decomposing an app into its features, prioritizing them and then working through that list in order. Often the decomposition is multi-layered creating a bunch of small testing problems out of one very large one. You don't test the feature so much as you test its constituent parts. An applicant who uses this framework is less concerned with actual test cases and more concerned with reducing the size of the problem to something manageable.The Fishbowl Framework is a big picture approach to testing in which we manipulate the application while watching and comparing the results. Put the app in a fishbowl, swirl it around in the water and watch what happens. The emphasis is more on the watching and analyzing than it is on exactly how we manipulate the features. An applicant who uses this framework chooses tests that cause visible output and large state changes.The Storybook Framework consists of developing specific scenarios and making sure the software does what is is supposed to do when presented with those scenarios. Stories start with the expected path and work outward. They don't always get beyond the expected. This framework tests coherence of behavior more than subtle errors. Applicants who employ this framework often take a user's point of view and talk about using the application to get real work done.The Pessimists Framework starts with edge cases. Subscribers test erroneous input, bad data, misconfigured environments and so on. This is a common strategy on mature products where the main paths are well trodden. Applicants who use this framework like to assume that the main paths will get tested naturally as part of normal dev use and dog-fooding and that the testing challenge is concentrated on lower probability scenarios. They are quick to take credit for prior testing, assume its rationality and pound on problematic scenarios.There are more and I am taking furious notes to try and make sense of them all. As I get to know the testers who work in my organization, it doesn't take long to see which frameworks they employ and in what order (many are driven by multiple frameworks). Indeed, after studying an applicant's first interview, I can almost always identify the framework they use to answer testing questions and can often predict how they are going to answer the questions other interviewers ask even before I read that far.Now some interesting questions come out of this that I am still looking into. Which of these frameworks is best? Which is best suited to certain types of functionality? Which is better for getting a job at Google? Already patterns are emerging.One thing is for sure, we're interviewing at a rate that will provide me with lots of data on this subject. Contact me if you'd like to participate in this little study!", "Google Testing Blog is now live on twitter. Follow us here: http://twitter.com/googletestingBy Patrick Copeland", "By James A. WhittakerMore thoughts:Understand your orgs release process and prioritiesLate cycle pre-release testing is the most nerve racking part of the entire development cycle. Test managers have to strike a balance between doing the right testing and ensuring a harmonious release. I suggest attending all the dev meetings, but certainly as release approaches you shouldn't miss a single one. Pay close attention to their worries and concerns. Nightmare scenarios have a tendency to surface late in the process. Add test cases to your verification suite to ensure these scenarios won't happen. The key here is to get late cycle pre-release testing right without any surprises. Developers can get skittish so make sure they understand your test plan going into the final push. The trick isn't to defer to development as to how to perform release testing but to make sure they are on-board with your plan. I find that at Google increasing the team's focus on manual testing is wholeheartedly welcomed by the dev team. Find your dev team's comfort zone and strike a balance between doing the right testing and making the final hours/days as wrinkle-free as possible.  Question your testing processStart by reading every test case and reviewing all automation. Can you map these test cases back to the test plan? How many tests do you have per component? Per feature? If a bug is found outside the testing process did you create a test case for it? Do you have a process to fix or deprecate broken or outdated test cases? As a test manager the completeness and thoroughness of the set of tests is your job. You may not be writing or running a lot of tests, but you should have them all in your head and be the first to spot gaps. It should be something a new manager tackles early and stays on top of at all times. Look for ways to innovateThe easiest way to look good in the eyes of developers is to maintain the status quo. Many development managers appreciate a docile and subservient test team. Many of them like a predictable and easily understood testing practice. It's one less thing to worry about (even in the face of obvious inefficiencies the familiar path is often the most well worn). As a new manager it is your job not to let them off so easy! You should make a list of the parts of the process that concern you and the parts that seem overly hard or inefficient. These are the places to apply innovation. Prepare for nervousness from the developer ranks, but do yourself and the industry a favor and place some bets for the long term. There is no advice I have found universally applicable concerning how to best foster innovation. What works for me is to find the stars on your team and make sure they are working on something they can be passionate about. As a manager this is the single most important thing you can do to increase productivity and foster innovation.", "By James A. WhittakerI got this question in email this morning from a reader:\"I am a test supervisor at --- and was promoted to a QA management position yesterday.  I'm excited and terrified, so I have been thinking about how to organize the thought in my mind. After attending StarWest and following your blog for a while now, I am very interested in your opinion.If you were a brand new QA Manager, and you knew what you know now, what are the top 5-10 things you would focus on?\"I am flattered by the confidence but in the event it is misplaced I wanted to answer this question publicly and invite readers to chime in with their own experiences. Besides, I am curious as to other opinions because I live with this same excitement and terror every day and could use a little advice myself. Here's my first couple and I'll add some more in future posts (unless of course you guys beat me to it).Start living with your product, get passionate about itDrink your product's kool-aid, memorize the sales pitch, understand it's competitive advantages but retain your skepticism. Test/QA managers should be as passionate about the product as dev managers but we need to temper our passion with proof. Make sure the test team never stops testing the functionality represented by the sales pitch. Furthermore, part of living with your product is being a user yourself. I now live without a laptop and exclusively use my Chrome OS Netbook for my day to day work. As people see me with it in the hallways, I get to recite its sales pitch many times every day. Great practice. I also get to live with its inadequacies and take note of the things it has yet to master. This is great discussion fodder with devs and other stakeholders and also forces me to consider competitive products. When I can't do something important on my Chrome OS Netbook, I have to use a competing product and this spawns healthy discussions about how users will perceive our product's downside and how we can truthfully communicate the pros and cons of our product to customers. Every day becomes a deep dive into my product as an actual user. This is a great way to start off on a new product. Really focus on the test plan, make it an early priorityIf you are taking over an existing role as test manager for an existing product chances are that a test plan already exists and chances are that test plan is inadequate. I'm not being unkind to your predecessor here, I am just being truthful. Most test plans are transitory docs. Now let me explain what I mean by that. Testers are quick to complain about inadequate design docs: that devs throw together a quick design doc or diagram but once they start coding, that design stagnates as the code takes on a life of its own. Soon the code does not match the design and the documentation is unreliable. If this is not your experience, congratulations but I find it far more the norm than design docs that are continually updated. Testers love to complain about this. \"How can I test a product without a full description of what the product does?\" But don't we often do the same thing with respect to our test plans? We throw together a quick test plan but as we start writing test cases (automated or manual) they take on a life of their own. Soon the test cases diverge from the test plan as we chase new development and our experience develops new testing insight. The test plan has just become like the design docs: a has-been document. You're a new test manager now, make fixing these documents one of your first priorities. You'll get to know your product's functionality and you'll see holes in the current test infrastructure that will need plugging. Plus, you'll have a basis to communicate with dev managers and show them you are taking quality seriously. Dev managers at Google love a good test plan, it gives them confidence you know what you are doing. Coming up next:Understand your orgs release process and prioritiesQuestion your testing processLook for ways to innovate", "By Julian Harty\n\n\n\nThe open-source launch of Chrome OS was announced today, and the source is available to download and build http://www.chromium.org/chromium-os. The entire project, including testing, is being open-sourced and made available for scrutiny and to help others to both contribute and learn from our experiences.\n\n\nThe test engineering team haven't been idle - we're a small, international team and as a result we're having to be innovative in terms of our testing so we maximize our contribution to the project. We had two goals: to take care of short-term release quality and to plan an automation infrastructure that will serve the operating system for many years in the future. \n\n\n\nCurrently we're combining manual and automated testing to achieve these goals. The manual testing provides fast feedback while we're extending the use of test automation to optimize future testing. In terms of test automation, we're using a collection of open-source tools such as:\n\n\nautotest http://autotest.kernel.org/ for complete end-to-end testing of builds \nwebdriver and selenium http://code.google.com/p/selenium/ to test through the Chrome web browser\n\n\nThere are some interesting plans and ideas afoot on how to significantly increase the testability and accessibility of Chrome OS - watch for future blog posts on these topics in the coming months!\n\n\n\nWe have used various approaches to design our tests, including 'tours' (mentioned in various posts on this blog). We are also applying the concept of 'attack surface' used in security testing more generally to determine what to test, from both technical and functional perspectives.\n\n\n\n\nFor the launch we devised the 'early-adopters tour'; where we validated the open source build and installation instructions on a collection of netbooks purchased from local stores (we expect many of you will want to build and run Chrome OS on similar machines).\n\n\n\n\nIf you're one of the early adopters - have fun building, installing and running Chrome OS and post your comments and ideas here. We hope you enjoy using Chrome OS as much as we're enjoying testing it!", "By Mi\u0161ko Hevery\n\nBest way to learn TDD is to have someone show you while pairing with you. Short of that, I have set up an eclipse project for you where you can give it a try:\n\nhg clone https://bitbucket.org/misko/misko-hevery-blog/\n\nOpen project blog/tdd/01_Calculator in Eclipse.\n\nIt should be set up to run all tests every time you modify a file.\nYou may have to change the path to java if you are not an Mac OS.\n\nProject -> Properties -> Builders -> Test -> Edit\n\nChange location to your java\n\n\n\nRight-click on Calculator.java -> Run As -> Java Application to run the calculator\n\n\nYour mission is to make the calculator work using TDD. This is the simplest form of TDD where you don't have to mock classes or create complex interactions, so it should be a good start for beginners.\n\nTDD means:\n\nwrite a simple test, and assert something interesting in it\n\n implement just enough to make that tests green (nothing more, or you will get ahead of your tests)\n\nthen write another test, rinse, and repeat.\n\n\nI have already done all of the work of separating the behavior from the UI, so that the code is testable and properly Dependency Injected, so you don't have to worry about running into testability issues.\n\nCalculator.java: This is the main method and it is where all of the wiring is happening.\n\nCalculatorView.java: This is a view and we don't usually bother unit testing it has cyclomatic complexity of one, hence there is no logic. It either works or does not. Views are usually good candidates for end-to-end testing, which is not part of this exercise.\n\nCalculatorModel.java: is just a PoJo which marshals data from the Controller to the View, not much to test here.\n\nCalculatorController.java: is where all of your if statements will reside, and we need good tests for it.\n\n\nI have started you off with first 'testItShouldInitializeToZero' test. Here are some ideas for next tests you may want to write.\n\ntestItShouldConcatinateNumberPresses\n\ntestItShouldSupportDecimalPoint\n\ntestItShouldIgnoreSecondDecimalPoint\n\ntestItShouldAddTwoIntegers\n\n\nI would love to see what you will come up with and what your thoughts are, after you get the whole calculator working. I would also encourage you to post interesting corner case tests here for others to incorporate. If you want to share your code with others, I would be happy to post your solutions.\n\nGood luck!\n\nPS: I know it is trivial example, but you need to start someplace.", "By Rajat Dewan\n\n\n\n\n\nI appreciate James' offer to talk about how I have used the FedEx tour in Mobile Ads. Good timing too as I just found two more priority 0 bugs with the automation that the FedEx tour inspired! It was fun presenting this at STAR and I am pleased so many people attended.\n\n\n\nMobile has been a hard problem space for testing: a humongous  browser, phone, capability combination which is changing fast as the underlying technology evolves. Add to this poor tool support for the mobile platform and the rapid evolution of the device and you'll understand why I am so interested in advice on how to do better test design. We've literally tried everything, from checking screenshots of Google's properties on mobile phones to treating the phone like a collection of client apps and automating them in the UI button-clicking traditional way.\n\nSoon after James joined Google in May 2009, he started introducing the concept of tours, essentially making a point of \"structured\" exploratory testing. Tours presented a way for me to look at the testing problem in a radical new way. Traditionally, the strategy is simple, focus on the end user interaction, and verify the expected outputs from the system under test. Tours (at least for me) change this formula. They force the tester to focus on what the software does, isolating the different moving parts of software in execution, and isolating the different parts of the software at the component (and composition) level. Tours tell me to focus on testing the parts that drive the car, rather than on whether or not the car drives. This is somewhat counter intuitive I admit, that's why it is so important. The real value add of the tours comes from the fact that they guide me in testing those different parts and help me analyze how different capabilities inter-operate. Cars will always drive you off the lot, which part will break first is the real question.\n\nI think testing a car is a good analogy. As a system it's devilishly complicated, hard to automate and hard to find the right combination of factors to make it fail. However, testing the dashboard can be automated; so can testing the flow of gasoline from the fuel tank to the engine and from there to the exhaust, so can lots of other capabilities. These automated point solutions can also be combined to test a bigger piece of the whole system. It's exactly what a mechanic does when trying to diagnose a problem: he employs different strategies for testing/checking each mechanical subsystem.\n\nAt STAR West, I spoke about evolving a good test strategy with the help of tours, specifically the FedEx tour. Briefly, the FedEx tour talks about tracking the movement of data and how it gets consumed and transformed by the system. It focuses on a very specific moving part, and as it turns out a crucial one for mobile. \n\n\n\nJames' FedEx tour tells me to identify and track data through my system. Identifying it is the easy part: the data comes from the Ads Database and is basically the information a user sees when the ad is rendered. When I followed it through the system, I noted three (and only three) places where the data is used (either manipulated or rendered for display). I found this to be true for all 10 local versions of the Mobile Ads application. The eureka moment for me was realizing that if I validated the data at those three points, I had little else to do in order to verify any specific localized version of an ad. Add all the languages you want, I'll be ready!\n\n\n\nI was able to hook verification modules at each one of these three data inflection points. This basically meant validating data for the new Click-to-Call Ad parameters and locale specific phone number format. I was tracking how code is affecting the data at each stage, which also helps in localizing a bug better than other conventional means...I knew exactly where the failure was! For overcoming the location dependency, I mocked the GPS location parameters of the phone. As soon as I finished with the automation, I ran each ad in our database through each of the language versions verifying the integrity of the data. The only thing that was left was to visually verify rendering of the ads on the three platforms, reducing the manual tests to three (one each for Android, iPhone and Palm Pre).\n\n\n\nThe FedEx tour guided me to build a succinct piece of automation and turned what could have been a huge and error prone manual test into a reusable piece of automation that will find and localize bugs quickly. We're now looking at applying the FedEx tour across ads and in other client and cloud areas in the company. Hopefully there will be more experience reports from others who have found it useful.\n\n\nExploratory Testing ... it's not just for manual testers anymore!", "by Zhanyong G. Mock Wan in Google KirklandIn the previous episode, we showed how Google C++ Mocking Framework matchers can make both your test code and your test output readable. What if you cannot find the right matcher for the task?Don't settle for anything less than perfect. It's easy to create a matcher that does exactly what you want, either by composing from existing matchers or by writing one from scratch.The simplest composite matcher is Not(m), which negates matcher m as you may have guessed. We also have AnyOf(m1, ..., mn) for OR-ing and AllOf(m1, ..., mn) for AND-ing. Combining them wisely and you can get a lot done. For example,EXPECT_THAT(new_code, AnyOf(StartsWith(\u201c// Tests\u201d)),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Not(ContainsRegex(\u201cTODO.*intern\u201d))));could generate a message like:Expected: (starts with \u201c// Tests\u201d) or\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(doesn't contain regular expression \u201cTODO.*intern\u201d)Actual: \u201c/* TODO: hire an intern. */ int main() {}\u201dIf the matcher expression gets too complex, or your matcher logic cannot be expressed in terms of existing matchers, you can use plain C++. The MATCHER macro lets you define a named matcher:MATCHER(IsEven, \u201c\u201d) { return (arg % 2) == 0; }allows you to write EXPECT_THAT(paren_num, IsEven()) to verify that paren_num is divisible by two. The special variable arg refers to the value being validated (paren_num in this case) \u2013 it is not a global variable.You can put any code between {} to validate arg, as long as it returns a bool value.The empty string \u201c\u201d tells Google C++ Mocking Framework to automatically generate the matcher's description from its name (therefore you'll see \u201cExpected: is even\u201d when the match fails). As long as you pick a descriptive name, you get a good description for free.You can also give multiple parameters to a matcher, or customize its description. The code:// P2 means the matcher has 2 parameters. Their names are low and high.MATCHER_P2(InClosedRange, low, high, \u201cis in range [%(low)s, %(high)s]\u201d) {\u00a0\u00a0return low }...EXPECT_THAT(my_age, InClosedRange(adult_min, penalty_to_withdraw_401k)); may print:Expected: is in range [18, 60]\u00a0\u00a0Actual: 2(No, that's not my real age.) Note how you can use Python-style interpolation in the description string to print the matcher parameters.You may wonder why we haven't seen any types in the examples. Rest assured that all the code we showed you is type-safe. Google C++ Mocking Framework uses compiler type inference to \u201cwrite\u201d the matcher parameter types for you, so that you can spend the time on actually writing tests \u2013 or finding your perfect match.Toilet-Friendly Version", "By Mi\u0161ko Hevery\n\nA lot of people have been asking me lately, what is the cost of testing, so I decided, that I will try to measure it, to dispel  the myth that testing takes twice as long.\n\nFor the last two weeks I have been keeping track of the amount of time I spent writing tests versus the time writing production code. The number surprised even me, but after I thought about it, it makes a lot of sense. The magic number is about 10% of time spent on writing tests. Now before, you think I am nuts, let me back it up with some real numbers from a personal project I have been working on.\n\n\nTotalProductionTestRatio\nCommits1,3471,3471,347\nLOC14,7098,7115,98840.78%\nJavaScript LOC10,0776,8193,25832.33%\nRuby LOC4,6321,8922,74059.15%\nLines/Commit10.926.474.4540.78%\nHours(estimate)1,2001,08012010.00%\nHours/Commit0.890.800.09\nMins/Commit53485\n\n\nCommits refers to the number of commits I have made to the repository. LOC is lines of code which is broken down by language. The ratio shows the typical breakdown between the production and test code when you test drive and it is about half, give or take a language. It is interesting to note that on average I commit about 11 lines out of which 6.5 are production and 4.5 are test. Now, keep in mind this is average, a lot of commits are large where you add a lot of code, but then there are a lot of commits where you are tweaking stuff, so the average is quite low.\n\nThe number of hours spent on the project is my best estimate, as I have not kept track of these numbers. Also, the 10% breakdown comes from keeping track of my coding habits for the last two weeks of coding. But, these are my best guesses.\n\nNow when I test drive, I start with writing a test which usually takes me few minutes (about 5 minutes) to write. The test represents my scenario. I then start implementing the code to make the scenario pass, and the implementation usually takes me a lot longer (about 50 minutes). The ratio is highly asymmetrical! Why does it take me so much less time to write the scenario than it does to write the implementation given that they are about the same length? Well look at a typical test and implementation:\n\nHere is a typical test for a feature:\nArrayTest.prototype.testFilter = function() {\n var items = [\"MIsKO\", {name:\"john\"}, [\"mary\"], 1234];\n assertEquals(4, items.filter(\"\").length);\n assertEquals(4, items.filter(undefined).length);\n\n assertEquals(1, items.filter('iSk').length);\n assertEquals(\"MIsKO\", items.filter('isk')[0]);\n\n assertEquals(1, items.filter('ohn').length);\n assertEquals(items[1], items.filter('ohn')[0]);\n\n assertEquals(1, items.filter('ar').length);\n assertEquals(items[2], items.filter('ar')[0]);\n\n assertEquals(1, items.filter('34').length);\n assertEquals(1234, items.filter('34')[0]);\n\n assertEquals(0, items.filter(\"I don't exist\").length);\n};\n\nArrayTest.prototype.testShouldNotFilterOnSystemData = function() {\n assertEquals(\"\", \"\".charAt(0)); // assumption\n var items = [{$name:\"misko\"}];\n assertEquals(0, items.filter(\"misko\").length);\n};\n\nArrayTest.prototype.testFilterOnSpecificProperty = function() {\n var items = [{ignore:\"a\", name:\"a\"}, {ignore:\"a\", name:\"abc\"}];\n assertEquals(2, items.filter({}).length);\n\n assertEquals(2, items.filter({name:'a'}).length);\n\n assertEquals(1, items.filter({name:'b'}).length);\n assertEquals(\"abc\", items.filter({name:'b'})[0].name);\n};\n\nArrayTest.prototype.testFilterOnFunction = function() {\n var items = [{name:\"a\"}, {name:\"abc\", done:true}];\n assertEquals(1, items.filter(function(i){return i.done;}).length);\n};\n\nArrayTest.prototype.testFilterIsAndFunction = function() {\n var items = [{first:\"misko\", last:\"hevery\"},\n              {first:\"mike\", last:\"smith\"}];\n\n assertEquals(2, items.filter({first:'', last:''}).length);\n assertEquals(1, items.filter({first:'', last:'hevery'}).length);\n assertEquals(0, items.filter({first:'mike', last:'hevery'}).length);\n assertEquals(1, items.filter({first:'misko', last:'hevery'}).length);\n assertEquals(items[0], items.filter({first:'misko', last:'hevery'})[0]);\n};\n\nArrayTest.prototype.testFilterNot = function() {\n var items = [\"misko\", \"mike\"];\n\n assertEquals(1, items.filter('!isk').length);\n assertEquals(items[1], items.filter('!isk')[0]);\n};\n\nNow here is code which implements this scenario tests above:\nArray.prototype.filter = function(expression) {\n var predicates = [];\n predicates.check = function(value) {\n   for (var j = 0; j < predicates.length; j++) {\n      if(!predicates[j](value)) {\n        return false;\n      }\n    }\n    return true;\n  };\n  var getter = Scope.getter;\n  var search = function(obj, text){\n    if (text.charAt(0) === '!') {\n      return !search(obj, text.substr(1));\n    }\n    switch (typeof obj) {\n    case \"bolean\":\n    case \"number\":\n    case \"string\":\n      return ('' + obj).toLowerCase().indexOf(text) > -1;\n   case \"object\":\n     for ( var objKey in obj) {\n       if (objKey.charAt(0) !== '$' && search(obj[objKey], text)) {\n         return true;\n       }\n     }\n     return false;\n   case \"array\":\n     for ( var i = 0; i < obj.length; i++) {\n       if (search(obj[i], text)) {\n         return true;\n       }\n     }\n     return false;\n   default:\n     return false;\n   }\n };\n switch (typeof expression) {\n   case \"bolean\":\n   case \"number\":\n   case \"string\":\n     expression = {$:expression};\n   case \"object\":\n     for (var key in expression) {\n       if (key == '$') {\n         (function(){\n           var text = (''+expression[key]).toLowerCase();\n           if (!text) return;\n           predicates.push(function(value) {\n             return search(value, text);\n           });\n         })();\n       } else {\n         (function(){\n           var path = key;\n           var text = (''+expression[key]).toLowerCase();\n           if (!text) return;\n           predicates.push(function(value) {\n             return search(getter(value, path), text);\n           });\n         })();\n       }\n     }\n     break;\n   case \"function\":\n     predicates.push(expression);\n     break;\n   default:\n     return this;\n }\n var filtered = [];\n for ( var j = 0; j < this.length; j++) {\n   var value = this[j];\n   if (predicates.check(value)) {\n     filtered.push(value);\n   }\n }\n return filtered;\n};\n\nNow, I think that if you look at these two chunks of code, it is easy to see that even though they are about the same length, one is much harder to write. The reason, why tests take so little time to write is that they are linear in nature. No loops, ifs or interdependencies with other tests. Production code is a different story, I have to create complex ifs, loops and have to make sure that the implementation works not just for one test, but all test. This is why it takes you so much longer to write production than test code. In this particular case, I remember rewriting this function three times, before I got it to work as expected. :-)\n\nSo a naive answer is that writing test carries a 10% tax. But, we pay taxes in order to get something in return. Here is what I get for 10% which pays me back:\n\nWhen I implement a feature I don't have to start up the whole application and click several pages until I get to page to verify that a feature works. In this case it means that I don't have to refreshing the browser, waiting for it to load a dataset and then typing some test data and manually asserting that I got what I expected. This is immediate payback in time saved!\n\nRegression is almost nil.  Whenever you are adding new feature you are running the risk of breaking something other then what you are working on immediately (since you are not working on it you are not actively testing it). At least once a day I have a what the @#$% moment when a change suddenly breaks a test at the opposite end of the codebase which I did not expect, and I count my lucky stars. This is worth a lot of time spent when you discover that a feature you thought was working no longer is, and by this time you have forgotten how the feature is implemented.\n\nCognitive load is greatly reduced since I don't have to keep all of the assumptions about the software in my head, this makes it really easy to switch tasks or to come back to a task after a meeting, good night sleep or a weekend.\n\nI can refactor the code at will, keeping it from becoming stagnant, and hard to understand. This is a huge problem on large projects, where the code works, but it is really ugly and everyone is afraid to touch it. This is worth money tomorrow to keep you going.\n\n\nThese benefits translate to real value today as well as tomorrow. I write tests, because the additional benefits I get more than offset the additional cost of 10%.  Even if I don't include the long term benefits, the value I get from test today are well worth it. I am faster in developing code with test. How much, well that depends on the complexity of the code. The more complex the thing you are trying to build is (more ifs/loops/dependencies) the greater the benefit of tests are.\n\nSo now you understand my puzzled look when people ask me how much slower/costlier the development with tests is.", "By Zhanyong G. Mock Wan in Google Kirkland\n\nAlright, it sounds like a good idea to verify that matchmakers can read and write. How does this concern us programmers, though?\nActually, we are talking about a way of writing tests here \u2013 a way that makes both the test code and its output read like English (hence \u201cliterate\u201d). The key to this technique is matchers, which are predicates that know how to describe themselves. For example, in Google C++ Mocking Framework, ContainsRegex(\"Ahcho+!\") is a matcher that matches any string that has the regular expression \"Ahcho+!\" in it. Therefore, it matches \"Ahchoo!\" and \"Ahchoooo! Sorry.\", but not \"Aha!\".\nWhat's this to do with test readability, anyway? It turns out that matchers, whose names are usually verb phrases, lend themselves easily to an assertion style that resembles natural languages. Namely, the assertion\n\n\nEXPECT_THAT(value, matcher);\n\nsucceeds if value matches matcher. For example,\n\n#include <gmock/gmock.h>using ::testing::Contains;...EXPECT_THAT(GetUserList(), Contains(admin_id));\n\nverifies that the result of GetUserList() contains the administrator.\n\nNow, pretend the punctuations aren't there in the last C++ statement and read it. See what I mean?\n\nBetter yet, when an EXPECT_THAT assertion fails, it will print an informative message that includes the expression being validated, its value, and the property we expect it to have \u2013 thanks to a matcher's ability to describe itself in human-friendly language. Therefore, not only is the test code readable, the test output it generates is readable too. For instance, the above example might produce:\n\nValue of: GetUserList()Expected: contains \"yoko\"\u00a0\u00a0Actual: { \"john\", \"paul\", \"george\", \"ringo\" }\n\nThis message contains relevant information for diagnosing the problem, often without having to use a debugger.\nTo get the same effect without using a matcher, you'd have to write something like:\n\nstd::vector<std::string> users = GetUserList();EXPECT_TRUE(VectorContains(users, admin_id))\u00a0\u00a0\u00a0\u00a0<< \" GetUserList() returns \" << users\u00a0\u00a0\u00a0\u00a0<< \" and admin_id is \" << admin_id;\n\nwhich is harder to write and less clear than the one-liner we saw earlier.\n\nGoogle C++ Mocking Framework (http://code.google.com/p/googlemock/) provides dozens of matchers for validating many kinds of values: numbers, strings, STL containers, structs, etc. They all produce friendly and informative messages. See http://code.google.com/p/googlemock/wiki/CheatSheet to learn more. If you cannot\nfind one that matches (pun intended) your need, you can either combine existing matchers, or define your own from scratch. Both are quite easy to do. We'll show you how in another episode. Stay tuned!\n\nToilet-friendly version", "By Mi\u0161ko Hevery\n\n\nOnce upon a time Java created an experiment called checked-exceptions, you know, you have to declare exceptions or catch them. Since that time, no other language (I know of) has decided to copy this idea, but somehow the Java developers are in love with checked exceptions. Here, I am going to \"try\" to convince you that checked-exceptions, even though look like a good idea at first glance, are actually not a good idea at all:\n\nEmpirical Evidence\n\nLet's start with an observation of your code base. Look through your code and tell me what percentage of catch blocks do rethrow or print error? My guess is that it is in high 90s. I would go as far as 98% of catch blocks are meaningless, since they just print an error or rethrow the exception which will later be printed as an error. The reason for this is very simple. Most exceptions such as FileNotFoundException, IOException, and so on are sign that we as developers have missed a corner case. The exceptions are used as away of informing us that we, as developers, have messed up. So if we did not have checked exceptions, the exception would be throw and the main method would print it and we would be done with it (optionally we would catch all exceptions in the main log them if we are a server).\n\nChecked exceptions force me to write catch blocks which are meaningless: more code, harder to read, and higher chance that I will mess up the rethrow logic and eat the exception.\n\nLost in Noise\n\nNow lets look at the 2-5% of the catch blocks which are not rethrow and real interesting logic happens there. Those interesting bits of useful and important information is lost in the noise, since my eye has been trained to skim over the catch blocks. I would much rather have code where a catch would indicate: \"pay, attention! here, something interesting is happening!\", rather than, \"it is just a rethrow.\" Now, if we did not have checked exceptions, you would write your code without catch blocks, test your code (you do test right?) and realize that under some circumstances an exception is throw and deal with it by writing the catch block. In such a case forgetting to write a catch block is no different than forgetting to write an else block of the if statement. We don't have checked ifs and yet no one misses them, so why do we need to tell developers that FileNotFound can happen. What if the developer knows for a fact that it can not happen since he has just placed the file there, and so such an exception would mean that your filesystem has just disappeared! (and your application is not place to handle that.)\n\nChecked exception make me skim the catch blocks as most are just rethrows, making it likely that you will miss something important.\n\nUnreachable Code\n\nI love to write tests first and implement as a consequence of tests. In such a situation you should always have 100% coverage since you are only writing what the tests are asking for. But you don't! It is less than 100% because checked exceptions force you to write catch blocks which are impossible to execute. Check this code out:\nbytesToString(byte[] bytes) {\n ByteArrayOutputStream out = new ByteArrayOutputStream();\n try {\n   out.write(bytes);\n   out.close()\n   return out.toSring();\n } catch (IOException e) {\n   // This can never happen!\n   // Should I rethrow? Eat it? Print Error?\n }\n}\n\nByteArrayOutputStream will never throw IOException! You can look through its implementation and see that it is true! So why are you making me catch a phantom exception which can never happen and which I can not write a test for? As a result I cannot claim 100% coverage because of things outside my control.\n\nChecked exceptions create dead code which will never  execute.\n\nClosures Don't Like You\n\nJava does not have closures but it has visitor pattern. Let me explain with concrete example. I was creating a custom class loader and need to override load() method on MyClassLoader which throws ClassNotFoundException under some circumstances. I use ASM library which allows me to inspect Java bytecodes. The way ASM works is that it is a visitor pattern, I write visitors and as ASM parses the bytecodes it calls specific methods on my visitor implementation. One of my visitors, as it is examining bytecodes, decides that things are not right and needs to throw a ClassNotFondException which the class loader contract says it should throw. But now we have a problem. What we have on a stack is MyClassLoader -> ASMLibrary -> MyVisitor. MyVisitor wants to throw an exception which MyClassLoader expects but it can not since ClassNotFoundException is checked and ASMLibrary does not declare it (nor should it). So I have to throw RuntimeClassNotFoundException from MyVisitor which can pass through ASMLibrary which MyClassLoader can then catch and rethrow as ClassNotFoundException.\n\nChecked exception get in the way of functional programing.\n\nLost Fidelity\n\nSuppose java.sql package would be implemented with useful exception such as SqlDuplicateKeyExceptions and SqlForeignKeyViolationException and so on (we can wish) and suppose these exceptions are checked (which they are). We say that the SQL package has high fidelity of exception since each exception is to a very specific problem. Now lets say we have the same set up as before where there is some other layer between us and the SQL package, that layer can either redeclare all of the exceptions, or more likely throw its own. Let's look at an example, Hibernate is object-relational-database-mapper, which means it converts your SQL rows into java objects. So on the stack you have MyApplication -> Hibernate -> SQL. Here Hibernate is trying hard to hide the fact that you are talking to SQL so it throws HibernateExceptions instead of SQLExceptions. And here lies the problem. Your code knows that there is SQL under Hibernate and so it could have handled SqlDuplicateKeyException in some useful way, such as showing an error to the user, but Hibernate was forced to catch the exception and rethrow it as generic HibernateException. We have gone from high fidelitySqlDuplicateKeyException to low fidelity HibernateException. An so MyApplication can not do anything. Now Hibernate could have throw HibernateDuplicateKeyException but that means that Hibernate now has the same exception hierarchy as SQL and we are duplicating effort and repeating ourselves.\n\nRethrowing checked exceptions causes you to lose fidelity and hence makes it less likely that you could do something useful with the exception later on.\n\nYou can't do Anything Anyway\n\nIn most cases when exception is throw there is no recovery. We show a generic error to the user and log an exception so that we con file a bug and make sure that that exception will not happen again. Since 90+% of the exception are bugs in our code and  all we do is log, why are we forced to rethrow it over and over again.\n\nIt is rare that anything useful can be done when checked exception happens, in most case we die with error! Therefor I want that to be the default behavior of my code with no additional typing.\n\nHow I deal with the code\n\nHere is my strategy to deal with checked exceptions in java:\n\nAlways catch all checked exceptions at source and rethrow them as LogRuntimeException.\nLogRuntimeException is my runtime un-checked exception which says I don't care just log it.\n\nHere I have lost Exception fidelity.\n\n\n\nAll of my methods do not declare any exceptions\n\nAs I discover that I need to deal with a specific exception I go back to the source where LogRuntimeException was thrown and I change it to <Specific>RuntimeException (This is rarer than you think)\nI am restoring the exception fidelity only where needed.\n\n\n\nNet effect is that when you come across a try-catch clause you better pay attention as interesting things are happening there.\nVery few try-catch calluses, code is much easier to read.\n\nVery close to 100% test coverage as there is no dead code in my catch blocks.", "By James WhittakerMathematically entropy is a measure of uncertainty. If there are, say, five events then maximum entropy occurs when those five events are equally likely and minimum entropy when one of those events is certain and the other four impossible. The more uncertain events you have to consider, the higher measured entropy climbs. People often think of entropy as a measure of randomness: the more (uncertain) events one must consider, the more random the outcome becomes. Testers introduce entropy into development by adding to the number of things a developer has to do. When developers are writing code, entropy is low. When we submit bugs, we increase entropy. Bugs divert their attention from coding. They must now progress in parallel on creating and fixing features. More bugs means more parallel tasks and raises entropy. This entropy is one reason that bugs foster more bugs ... the entropic principle ensures it. Entropy creates more entropy! Finally there is math to show what is intuitively appealing: that prevention beats a cure. However, there is nothing we can do to completely prevent the plague of entropy other than create developers who never err. Since this is unlikely any time soon we must recognize how and when we are introducing entropy and do what we can to manage it. The more we can do during development the better. Helping out in code reviews, educating our developers about test plans, user scenarios and execution environments so they can code against them will reduce the number of bugs we have to report. Smoking out bugs early, submitting them in batches and making sure we submit only high quality bugs by triaging them ourselves will keep their mind on development. Writing good bug reports and quickly regressing fixes will keep their attention where it needs to be. In effect, it maximizes the certainty of the 'development event' and minimizes the number and impact of bugs. Entropy thus tends toward it's minimum. We can't banish this plague but if we can recognize the introduction of entropy into development and understand its inevitable effect on code quality, we can keep it at bay.", "by Mi\u0161ko Hevery\n\n\nI would like to make an analogy between building software and building a car. I know it is imperfect one, as one is about design and the other is about manufacturing, but indulge me, the lessons are very similar.\n\nA piece of software is like a car. Lets say you would like to test a car, which you are in the process of designing, would you test is by driving it around and making modifications to it, or would you prove your design by testing each component separately? I think that testing all of the corner cases by driving the car around is very difficult, yes if the car drives you know that a lot of things must work (engine, transmission, electronics, etc), but if it does not work you have no idea where to look. However, there are some things which you will have very hard time reproducing in this end-to-end test. For example, it will be very hard for you to see if the car will be able to start in the extreme cold of the north pole, or if the engine will not overheat going full throttle up a sand dune in Sahara. I propose we take the engine out and simulate the load on it in a laboratory.\n\nWe call driving car around an end-to-end test and testing the engine in isolation a unit-test. With unit tests it is much easier to simulate failures and corner cases in a much more controlled environment. We need both tests, but I feel that most developers can only imagine the end-to-end tests.\n\nBut lets see how we could use the tests to design a transmission. But first, little terminology change, lets not call them test, but instead call them stories. They are stories because that is what they tell you about your design. My first story is that:\n\nthe transmission should allow the output shaft to be locked, move in same direction (D) as the input shaft, move in opposite (R) or move independently (N)\n\n\nGiven such a story I could easily create a test which would prove that the above story is true for any design submitted to me. What I would most likely get is a transmission which would only have a single gear in each direction. So lets write another story\n\nthe transmission should allow the ratio between input and output shaft to be [-1, 0, 1, 2, 3, 4]\n\n\nAgain I can write a test for such a transmission but i have not specified how the forward gear should be chosen, so such a transmission would most likely be permanently stuck in 1st gear and limit my speed, it will also over-rev the engine.\n\nthe transmission should start in 1st and than switch to higher gear before the engine reaches maximum revolutions.\n\n\nThis is better, but my transmission would most likely rev the engine to maximum before it would switch, and once it would switch to higher gear and I would slow down, it would not down-shift.\n\nthe transmission should down shift whenever the engine RPM fall bellow 1000 RPMs\n\n\nOK, now it is starting to drive like a car, but still the limits for shifting really are 1000-6000 RPMs which is not very fuel efficient way to drive your car.\n\nthe transmission should up-shift whenever the estimated fuel consumption at a higher gear ration is better than the current one.\n\n\nSo now our engine will not rev any more but it will be a lazy car since once the transmission is in the fuel efficient mode it will not want to down-shift\n\nthe transmission should down-shift whenever the gas pedal is depressed more than 50% and the RPM is lower than the engine's peak output RPM.\n\n\nI am not a transmission designer, but I think this is a decent start.\n\nNotice how I focused on the end result of the transmission rather than on testing specific internals of it. The transmission designer would have a lot of levy in choosing how it worked internally, Once we would have something and we would test it in the real world we could augment these list of stories with additional stories as we discovered additional properties which we would like the transmission to posses.\n\nIf we would decide to change the internal design of the transmission for whatever reason we would have these stories as guides to make sure that we did not forget about anything. The stories represent assumptions which need to be true at all times. Over the lifetime of the component we can collect hundreds of stories which represent equal number of assumption which is built into the system.\n\nNow imagine that a new designer comes on board and makes a design change which he believes will improve the responsiveness of the transmission, he can do so because the existing stories are not restrictive in how, only it what the outcome should be. The stories save the designer from breaking an existing assumption which was already designed into the transmission.\n\nNow lets contrast this with how we would test the transmission if it would already be build.\n\ntest to make sure all of the gears work\n\ntest to make sure that the engine is not allowed to over-rev\n\n\nIt is hard now to think about what other tests to write, since we are not using the tests to drive the design. Now, lets say that someone now insist that we get 100% coverage, we open the transmission up and we see all kinds of logic, and rules and we don't know why since we were not part of the design so we write a test\n\nat 3000 RPM input shaft, apply 100% throttle and assert that the transmission goes to 2nd gear.\n\n\nTests like that are not very useful when you want to change the design, since you are likely to break the test, without fully understanding why the test was testing that specific conditions, it is hard to know if anything was broken if the tests is red.. That is because the tests does not tell a story any more, it only asserts the current design. It is likely that such a test will be in the way when you will try to do design changes. The point I am trying to make is that there is huge difference between writing tests before or after. When we write tests before we are:\n\ncreating a story which is forcing a particular design decision.\n\ntests are a collection of assumptions which needs to be true at all times.\n\n\nwhen we write tests after the fact we:\n\nmiss a lot of reasons why things are done in particular way even if we have 100% coverage\n\ntest are often brittle because they are tied to particulars of the current implementation\n\ntests are just snapshots and don't tell a story of why the component does something, only that it does.\n\n\nFor this reason there are huge differences in quality when writing assumptions as stories before (which force design to emerge) or writing tests after which take a snapshot of a given design.", "By James WhittakerSorry I haven't followed up on this, let the excuse parade begin: A) My new book just came out and I have spent a lot of time corresponding with readers. B) I have taken on leadership of some new projects including the testing of Chrome and Chrome OS (yes you will hear more about these projects right here in the future). C) I've gotten just short of 100 emails suggesting the 7th plague and that takes time to sort through.This is clearly one plague-ridden industry (and, no, I am not talking about my book!)I've thrown out many of them that deal with a specific organization or person who just doesn't take testing seriously enough. Things like the Plague of Apathy (suggested exactly 17 times!) just doesn't fit. This isn't an industry plague, it's a personal/group plague. If you don't care about quality, please do us all a favor and get out of the software business. Go screw someone else's industry up, we have enough organic problems we have to deal with. I also didn't put down the Plague of the Deluded Developer (suggested by various names 22 times) because it dealt with developers that as a Googler I no longer have to deal with ... those who think they never write bugs. Our developers know better and if I find out exactly where they purchased that clue I will forward the link. Here's some of the best. As many of them have multiple suggesters I have credited the persons who were either first or gave the most thoughtful analysis. Feel free, if you are one of these people, to give further details or clarifications in the comments of this post as I am sure these summaries do not do them justice. The Plague of Metrics (Nicole Klein, Curtis Pettit plus 18 others): Metrics change behavior and once a tester knows how the measurement works, they test to make themselves look good or say what they want it to say ignoring other more important factors. The metric becomes the goal instead of measuring progress. The distaste for metrics in many of these emails was palpable!The Plague of Semantics (Chris LeMesurier plus 3 others): We misuse and overuse terms and people like to assign their own meaning to certain terms. It means that designs and specs are often misunderstood or misinterpreted. This was also called the plague of assumptions by other contributors. The Plague of Infinity (Jarod Salamond, Radislav Vasilev and 14 others): The testing problem is so huge it's overwhelming. We spend so much time trying to justify our coverage and explain what we are and are not testing that it takes away from our focus on testing. Every time we take a look at the testing problem we see new risks and new things that need our attention. It randomizes us and stalls our progress. This was also called the plague of endlessness and exhaustion. The Plague of Miscommunication (Scott White and 2 others): The language of creation (development) and the language of destruction (testing) are different. Testers write a bug report and the devs don't understand it and cycles have to be spent explaining and reexplaining. A related plague is the lack of communication that causes testers to redo work and tread over the same paths as unit tests, integration tests and even the tests that other testers on the team are performing. This was also called the plague of language (meaning lack of a common one). The Plague of Rigidness (Roussi Roussev, Steven Woody, Michele Smith and 5 others): Sticking to the plan/process/procedure no matter what. Test strategy cannot be bottled in such a manner yet process heavy teams often ignore creativity for the sake of process. We stick with the same stale testing ideas product after product, release after release. This was also called the plague of complacency. Roussi suggested a novel twist calling this the success plague where complacency is brought about through success of the product. How can we be wrong when our software was so successful in the market? And I have my own 7th Plague that I'll save for the next post. Unless anyone would like to write it for me? It's called the Plague of Entropy. A free book to the person who nails it.", "We are thrilled to announce the speakers and talks for the 4th Google Test Automation Conference (GTAC). Competition was fierce: we received over 100 submissions and have an acceptance rate of lower than 10%.Testing Applications on Mobile Devices (Doron Reuveni, uTest)JsTestDriver (Jeremie Lenfang-Engelmann, Misko Hevery, Google)Fighting Layout Bugs (Michael Tamm, optivo GmbH)Even better than the real thing - Lessons learned from testing GWT applications (Nicolas Wettstein, Google)Selenium: to 2.0 and Beyond! (Simon Stewart, Google)Automating Performance Test Data Collection and Reporting (David Burns, David Henderson, smartFOCUS DIGITAL)Achieving Web Test Automation with a Mixed-Skills Team (Mark Micallef, BBC Future Media and Technology)Score One for Quality! (Joshua Williams and Ross Smith, Microsoft)Automatic workarounds for web applications (Antonio Carzaniga, Alessandra Gorla, Nicol\u00f2 Perino, Mauro Pezz\u00e8, University of Lugano )Precondition Satisfaction by Smart Object Selection in Random Testing (Yi Wei, Serge Gebhardt, ETH Zurich)For further information on the conference please visit its wepage at http://www.gtac.biz.", "by Shyam Seshadri\n\nBefore I jump into how exactly you can perform super fast and easy JS testing, let me give you some background on the problem.\n\nJavascript is a finicky language (Some people even hesitate to call it a language). And it can easily grow and become a horrible and complicated beast, incapable of being tamed once let loose. And testing it is a nightmare. Once you have decided on a framework (of which there are a dime a dozen), you then have to set it up to run just right. You need to set it up to actually run your tests. Then you have to figure out how to run it in a continuous integration environment. Maybe even run it in headless mode. And everyone solves it in their own ways.\n\nBut the biggest problem I have with most of these frameworks is that executing the tests usually requires a context switch. By that, I mean to run a JSUnit test, you end up usually having to open the browser, browse to a particular url or html page which then runs the test. Then you have to look at the results there, and then come back to your editor to either proceed further or fix your tests. Works, but really slows down development.\n\nIn java, all it takes is to click the run button in your IDE to run your tests. You get instant feedback, a green / red bar and details on which tests passed and failed and at what line. No context switch, you can get it to run at every save, and proceed on your merry way. Till now, this was not possible with Javascript.\n\nBut now, we have JS Test Driver. My colleagues Jeremie and Misko ended up running into some of the issues I outlined above, and decided that going along with the flow was simply unacceptable. So they created a JS Testing framework which solves these very things. You can capture any browser on any machine, and when you tell it to run tests, it will go ahead and execute them on all these browsers and return you the results in your client. And its blazing fast. I am talking milliseconds to run 100 odd tests. And you can tell it to rerun your tests at each save. All within the comforts of your IDE. And over the last three weeks, I have been working on the eclipse plugin for JS Test Driver, and its now at the point where its in a decent working condition.\n\nThe plugin in action\n\nThe plugin allows you to, from within Eclipse, start the JS Test Driver server, capture some browsers, and then run your tests. You get pretty icons telling you what browsers were captured, the state of the server, the state of the tests. It allows you to filter and show only failures, rerun your last launch configuration, even setup the paths to your browsers so you can launch it all from the safety of eclipse. And as you can see, its super fast. Some 100 odd tests in less than 10 ms. If thats not fast, I don\u2019t know what is.\n\nFor more details on JS Test Driver, visit its Google Code website and see how you can use it in your next project and even integrate it into a continuous integration. Misko talks a little bit more about the motivations behind writing it on his Yet Another JS Testing Framework post. To try out the plugin for yourselves, go add the following update site to eclipse:\n\nhttp://js-test-driver.googlecode.com/svn/update/\n\n For all you IntelliJ fanatics, there is something similar in the works.", "By James A. WhittakerYes, I only posted 6 plagues. Congratulations for catching this purposeful omission! You wouldn't trust a developer who argues \"this doesn't need to be tested\" or \"that function works like so\" and you shouldn't trust me when I say there are 7 plagues. In the world of testing all assumptions must be scrutinized and it doesn't work until someone, namely a tester, verifies that it does!Clearly this is an alert and education readership. But why assume even this statement is true? How about another test? Anyone feel like contributing the 7th plague?I've actually received a few via email already and I have an idea of my own 7th. So email them to me at docjamesw@gmail.com and I'll post a few of the best, with attribution, on this blog. Maybe I can even scare up some Google SWAG or a copy of my latest book to the best one. First come, first published.", "Because GWT (Google Web Toolkit) is new and exciting it's easy to forget the lessons on clean GUI code structure that have been accumulated over nearly thirty years.GwtTestCase is good for testing UI-specific code in JavaScript. If you find yourself using GwtTestCase for testing non-ui client-side logic you may not have a clear View/Presenter separation. Separating the View and the Presenter allows for more modular, more easily tested code with shorter test times. Model View Presenter was introduced in another episode back in February. Here's how to apply it to a GWT app.Defining terms:Server \u2013 a completely standard backend with no dependency on GWT.Model \u2013 the data model. May be shared between the client and server side, or if appropriate you might have a different model for the client side. It has no dependency on GWT.View \u2013 the display. Classes in the view wrap GWT widgets, hiding them from the rest of your code. They contain no logic, no state, and are easy to mock.Presenter \u2013 all the client side logic and state; it talks to the server and tells the view what to do. It uses RPC mechanisms from GWT but no widgets.The Presenter, which contains all the interesting client-side code is fully testable in Java!public void testRefreshPersonListButtonWasClicked() {IMocksControl easyMockContext = EasyMock.createControl()mockServer = easyMockContext.createMock(Server.class);mockView = easyMockContext.createMock(View.class);List franz = Lists.newArrayList(new Person(\"Franz\", \"Mayer\"));mockServer.getPersonList(AsyncCallbackSuccessMatcher<list<person>>reportSuccess(franz)));mockView.clearPersonList());mockView.addPerson(\u201cFranz\u201d, \u201cMayer\u201d);easyMockContext.replay();presenter.refreshPersonListButtonClicked();easyMockContext.verify();}Testing failure cases is now as easy as changing expectations. By swapping in the following expectations, the above test goes from testing success to testing that after two server failures, we show an error message.mockServer.getPersonList(AsyncCallbackFailureMatcher<list<person>>reportFailure(failedExpn))expectLastCall().times(2); // Ensure the presenter tries twicemockView.showErrorMessage(\u201cSorry, please try again later\u201d));You'll still need an end-to-end test. But all your logic can be tested in small and fast tests. The Source Code for the Matchers is open-sourced and can be downloaded here: AsyncCallbackSuccessMatcher.java - AsyncCallbackFailureMatcher.java.Consider using Test Driven Development (TDD) to develop the presenter. It tends to result in higher test coverage, faster and more relevant tests, as well as a better code structure.This week's episode by David Morgan, Christopher Semturs and Nicolas Wettstein based in Z\u00fcrich, Switzerland \u2013 having a real Mountain ViewToilet-friendly versionAsyncCallbackSuccessMatcher.javaAsyncCallbackFailureMatcher.java.", "by Mi\u0161ko Hevery\n\nEveryone seems to think that they are writing OO after all they are using OO languages such as Java, Python or Ruby. But if you exam the code it is often procedural in nature.\n\nStatic Methods\n\nStatic methods are procedural in nature and they have no place in OO world. I can already hear the screams, so let me explain why, but first we need to agree that global variables and state is evil. If you agree with previous statement than for a static method to do something interesting it needs to have some arguments, otherwise it will always return a constant. Call to a staticMethod() must always return the same thing, if there is no global state. (Time and random, has global state, so that does not count and object instantiation may have different instance but the object graph will be wired the same way.)\n\nThis means that for a static method to do something interesting it needs to have arguments. But in that case I will argue that the method simply belongs on one of its arguments. Example: Math.abs(-3) should really be -3.abs(). Now that does not imply that -3 needs to be object, only that the compiler needs to do the magic on my behalf, which BTW, Ruby got right. If you have multiple arguments you should choose the argument with which method interacts the most.\n\nBut most justifications for static methods argue that they are \"utility methods\". Let's say that you want to have toCamelCase() method to convert string \"my_workspace\" to \"myWorkspace\". Most developers will solve this as StringUtil.toCamelCase(\"my_workspace\"). But, again, I am going to argue that the method simply belongs to the String class and should be \"my_workspace\".toCamelCase(). But we can't extend the String class in Java, so we are stuck, but in many other OO languages you can add methods to existing classes.\n\nIn the end I am sometimes (handful of times per year) forced to write static methods due to limitation of the language. But that is a rare event since static methods are death to testability. What I do find, is that in most projects static methods are rampant.\n\nInstance Methods\n\nSo you got rid of all of your static methods but your codes still is procedural. OO says that code and data live together. So when one looks at code one can judge how OO it is without understanding what the code does, simply by looking at the relationship of data and code.\nclass Database {\n // some fields declared here\n boolean isDirty(Cache cache, Object obj) {\n   for (Object cachedObj : cache.getObjects) {\n     if (cachedObj.equals(obj))\n       return false;\n   }\n   return true;\n }\n}\n\nThe problem here is the method may as well be static! It is in the wrong place, and you can tell this because it does not interact with any of the data in the Database, instead it interacts with the data in cache which it fetches by calling the getObjects() method. My guess is that this method belongs to one of its arguments most likely Cache. If you move it to Cache you well notice that the Cache will no longer need the getObjects() method since the for loop can access the internal state of the Cache directly. Hey, we simplified the code (moved one method, deleted one method) and we have made Demeter happy.\n\nThe funny thing about the getter methods is that it usually means that the code where the data is processed is outside of the class which has the data. In other words the code and data are not together.\nclass Authenticator {\n Ldap ldap;\n Cookie login(User user) {\n   if (user.isSuperUser()) {\n     if ( ldap.auth(user.getUser(),\n            user.getPassword()) )\n       return new Cookie(user.getActingAsUser());\n   } else (user.isAgent) {\n       return new Cookie(user.getActingAsUser());\n   } else {\n     if ( ldap.auth(user.getUser(),\n            user.getPassword()) )\n       return new Cookie(user.getUser());\n   }\n   return null;\n }\n}\n\nNow I don't know if this code is well written or not, but I do know that the login() method has a very high affinity to user. It interacts with the user a lot more than it interacts with its own state. Except it does not interact with user, it uses it as a dumb storage for data. Again, code lives with data is being violated. I believe that the method should be on the object with which it interacts the most, in this case on User. So lets have a look:\nclass User {\n String user;\n String password;\n boolean isAgent;\n boolean isSuperUser;\n String actingAsUser;\n\n Cookie login(Ldap ldap) {\n   if (isSuperUser) {\n     if ( ldap.auth(user, password) )\n       return new Cookie(actingAsUser);\n   } else (user.isAgent) {\n       return new Cookie(actingAsUser);\n   } else {\n     if ( ldap.auth(user, password) )\n       return new Cookie(user);\n   }\n   return null;\n }\n}\n\nOk we are making progress, notice how the need for all of the getters has disappeared, (and in this simplified example the need for the Authenticator class disappears) but there is still something wrong. The ifs branch on internal state of the object. My guess is that this code-base is riddled with if (user.isSuperUser()). The issue is that if you add a new flag you have to remember to change all of the ifs which are dispersed all over the code-base. Whenever I see If or switch on a flag I can almost always know that polymorphism is in order.\nclass User {\n String user;\n String password;\n\n Cookie login(Ldap ldap) {\n   if ( ldap.auth(user, password) )\n     return new Cookie(user);\n   return null;\n }\n}\n\nclass SuperUser extends User {\n String actingAsUser;\n\n Cookie login(Ldap ldap) {\n   if ( ldap.auth(user, password) )\n     return new Cookie(actingAsUser);\n   return null;\n }\n}\n\nclass AgentUser extends User {\n String actingAsUser;\n\n Cookie login(Ldap ldap) {\n   return new Cookie(actingAsUser);\n }\n}\n\nNow that we took advantage of polymorphism, each different kind of user knows how to log in and we can easily add new kind of user type to the system. Also notice how the user no longer has all of the flag fields which were controlling the ifs to give the user different behavior. The ifs and flags have disappeared.\n\nNow this begs the question: should the User know about the Ldap? There are actually two questions in there. 1) should User have a field reference to Ldap? and 2) should User have compile time dependency on Ldap?\n\nShould User have a field reference to Ldap? The answer is no, because you may want to serialize the user to database but you don't want to serialize the Ldap. See here.\n\nShould User have compile time dependency on Ldap? This is more complicated, but in general the answer depends on weather or not you are planning on reusing the User on a different project, since compile time dependencies are transitive in strongly typed languages. My experience is that everyone always writes code that one day they will reuse it, but that day never comes, and when it does, usually the code is entangled in other ways anyway, so code reuse after the fact just does not happen. (developing a library is different since code reuse is an explicit goal.) My point is that a lot of people pay the price of \"what if\" but never get any benefit out of it. Therefore don't worry abut it and make the User depend on Ldap.", "by Juergen Allgayer, Conference ChairTesting for the Web is the theme of the 4th Google Test Automation Conference (GTAC), to be held in Zurich, October 21-22. We are happy to announce that we are now accepting applications for attendance. The success of the conference depends on active participation of the attendees. Because the available spaces for the conference are limited, we ask each person to apply for attendance. Since we aim for a balanced audience of seasoned practitioners, students and academics, we ask the applicants to provide a brief background statement.How to applyPlease visit http://www.gtac.biz/call-for-attendance to apply for a attendance.DeadlinePlease submit your application until August 28, 2009 at the latest.Registration FeesThere are no registration fees. We will send out detailed registration instructions to each invited applicant. We will provide breakfast and lunch. There will be a reception on the evening of October 21.CancellationIf you applied but can no longer attend the conference please notify usimmediately by sending an email to gtac-2009-cfa@google.com sosomeone from the waiting list can get the opportunity instead.Further informationGeneral website: http://www.gtac.biz/Call for proposals: http://www.gtac.biz/call-for-proposalsCall for attendance: http://www.gtac.biz/call-for-attendanceAccommodations: http://www.gtac.biz/accomodations", "By James A. WhittakerAnd now for the last plague in this series. I hope you enjoyed them (the posts ...not the plagues!)Imagine playing a video game blindfolded or even with the heads up display turned off. You cannot monitor your character's health, your targeting system is gone. There is no look ahead radar and no advance warning of any kind. In gaming, the inability to access information about the campaign world is debilitating and a good way to get your character killed. There are many aspects of testing software that fall into this invisible spectrum. Software itself is invisible. We see it only through the UI with much of what is happening doing so under the covers and out of our line of sight. It\u2019s not like building a car in which you can clearly see missing pieces and many engineers can look at a car and get the exact same view of it. There is no arguing whether the car has a bumper installed, it is in plain sight for everyone involved to see. Not so with software which exists as magnetic fluctuations on storage media. It\u2019s not a helpful visual.Software testing is much like game playing while blindfolded. We can't see bugs; we can't see coverage; we can't see code changes. This information, so valuable to us as testers, is hidden in useless static reports. If someone outfitted us with an actual blindfold, we might not even notice. This blindness concerning our product and its behavior creates some very real problems for the software tester. Which parts of the software have enjoyed the most unit testing? Which parts have changed from one build to the next? Which parts have existing bugs posted against them? What part of the software does a specific test case cover? Which parts have been tested thoroughly and which parts have received no attention whatsoever?Our folk remedy for the blindness plague has always been to measure code coverage, API/method coverage or UI coverage. We pick the things we can see the best and measure them, but do they really tell us anything? We\u2019ve been doing it this way for years not because it is insightful, but simply because it is all our blindness will allow us to do. We\u2019re interacting with our application under test a great deal, but we must rely on other, less concrete senses for any feedback about our effort.Software testers could learn a lot from the world of gaming. Turn on your heads up display and see the information you've been blind to. There's power in information.", "by Juergen Allgayer, Conference ChairWe are thrilled to announce that Niklaus Wirth and Alberto di Meglio are this years keynote speakers at the 4th Google Test Automation Conference (GTAC).Niklaus WirthProf. Niklaus Wirth, is the designer of several programing languages and operating systems, including Pascal and Oberon. He received many awards including the Turing award, is author of many books and articles such as \"Program Development by Stepwise Refinement\" and \"Algorithms + Data Structures = Programs\". Prof. Niklaus Wirth served as professor at Stanford, University of Zurich, and ETH Zurich.Alberto di MeglioIn 2003, Dr. Alberto di Meglio was appointed by CERN as Software Integration Manager in the Middleware Reengineering Activity of the first Enabling Grids for E-science (EGEE) project. At the end of the EGEE project, thanks to the very successful results obtained with the integration and testing tools and procedures developed for the grid middleware developed by EGEE, Alberto set up Infrastructure for Testing, Integration and Configuration of Software, an international infrastructure co-funded by EC FP7 (European Commission: CORDIS - Seventh Framework Programme) for building and testing software on the grid, of which he is currently Project Director.Reminder: Call for proposalsIf you would like to give a talk at GTAC please remember to submit your proposal until August 1 at the latest. Please visit http://www.gtac.biz/call-for-proposals for details.", "By James A. Whittaker There are two communities who regularly find bugs, the testers who are paid to find them and the users who stumble upon them quite by accident. Clearly the users aren\u2019t doing so on purpose, but through the normal course of using the software to get work (or entertainment, socializing and so forth) done failures do occur. Often it is the magic combination of an application interacting with real user data on a real user\u2019s computing environment that causes software to fail. Isn\u2019t it obvious then that testers should endeavor to create such data and environmental conditions in the test lab in order to find these bugs before the software ships?Actually, the test community has been diligently making an attempt to do just that for decades. I call this bringing the user into the test lab, either in body or in spirit. My own PhD dissertation was on the topic of statistical usage testing and I was nowhere near the first person to think of the idea as my multi-page bibliography will attest. But there is a natural limit to the success of such efforts. Testers simply cannot be users or simulate their actions in a realistic enough way to find all the important bugs. Unless you actually live in the software you will miss important issues.It\u2019s like homeownership. It doesn\u2019t matter how well the house is built. It doesn\u2019t matter how diligent the builder and the subcontractors are during the construction process. The house can be thoroughly inspected during every phase of construction by the contractor, the homeowner and the state building inspector. There are just some problems that will only be found once the house is occupied for some period of time. It needs to be used, dined in, slept in, showered in, cooked in, partied in, relaxed in and all the other things homeowners do in their houses. It\u2019s not until the teenager takes an hour long shower while the sprinklers are running that the septic system is found deficient. It\u2019s not until a car is parked in the garage overnight that we find out the rebar was left out of the concrete slab. The builder won't and often can't do these things.And time matters as well. It takes a few months of blowing light bulbs at the rate of one every other week to discover the glitch in the wiring and a year has to pass before the nail heads begin protruding from the drywall. These are issues for the homeowner, not the builder. These are the software equivalents of memory leaks and data corruption, time is a necessary element in finding such problems.These are some number of bugs that simply cannot be found until the house is lived in and software is no different. It needs to be in the hands of real users doing real work with real data in real environments. Those bugs are as inaccessible to testers as nail pops and missing rebar are to home builders.Testers are homeless. We can do what we can do and nothing more. It\u2019s good to understand our limitations and plan for the inevitable \u201cpunch lists\u201d from our users. Pretending that once an application is released the project is over is simply wrong headed. There is a warranty period that we are overlooking and that period is still part of the testing phase.", "by Patrick CopelandWe get questions once in a while about our readership. Here's a brief summary of the last 30 days...Page view count: 34,140Time on each page: 2:52Most popular day to read: Tuesday's.Most traffic (top 5 in order): US, India, UK, Germany, CanadaNumber of countries with readers: 131Most popular posts.../2009/07/why-are-we-embarrassed-to-admit-that-we (2,227)/2009/06/7-plagues-of-software-testing (2,151)/2009/07/software-testing-categorization /2009/07/advantages-of-unit-testing /2009/06/plague-of-repetitiveness /2009/07/old-habits-die-hard /2009/07/separation-anxiety /2009/06/gtac-call-for-proposals /2009/07/plague-of-amnesia", "By James A. Whittaker \"Testing is boring.\" Don\u2019t pretend for a moment that you\u2019ve never heard a developer, designer or other non tester express that sentiment and take a moment to search your own soul for the truth. Even the most bigoted tester would have to admit to contracting the boredom plague at some point. The day-in, day-out execution of tests and the filing of bug reports simply doesn't hold the interest of most technical people who are drawn to computing for its creative and challenging reputation. Even if you find yourself immune to the boredom, you have to admit there are many monotonous and uncreative aspects of testing.It doesn\u2019t begin that way though. Early in a tester\u2019s career, the thrill of the bug hunt can keep a tester going for many months. It can be as intoxicating as playing a video game and trying to find some elusive prize. And lots of progress in terms of skill is made in those early years with testers going from novice to pretty good in no time flat. Who can argue with a career that offers such learning, advancement and intellectually interesting problems? But as the achievement curve levels out, the task of testing can get very repetitive and that quickly turns to monotony. I think, promotion concerns aside, this is why many testers switch to development after a few years. The challenge and creativity gets eclipsed by the monotony. I think bored testers are missing something. I submit that it is only the tactical aspects of software testing that become boring over time and many turn to automation to cure this. Automation as a potion against the tedium of executing test cases and filing bugs reports is one thing, but automation is no replacement for the strategic aspects of the testing process and it is in this strategy that we find salvation from this plague. The act of test case design, deciding what should and shouldn\u2019t be tested and in what proportion, is not something automation is good at and yet it is an interesting and intellectually challenging task. Neither is the strategic problem of monitoring tests and determining when to stop. These are hard and interesting strategic problems that will banish the plague of boredom. Testers can succumb to the plague of boredom or they can shift their focus from mostly tactical to a nice mix of tactical work and strategic thinking. Make sure that in your rush to perform the small tactical testing tasks you aren't dropping the strategic aspects of your work because therein are the interesting technical challenges and high level thinking that will hold your interest and keep this plague at bay.", "by Patrick CopelandYou might not know this but when we started this blog back in January 2007 we were the first at Google to allow readers to comment openly on our posts. We take pride in our goal to participate in the testing community.This openness is sometimes abused. Lately the volume of  comment spam has been on the raise. We have to spend a lot of time playing \"whack-a-mole\" cleaning it up. It's annoying to our readers and clutters the legitimate discussion.We've taken the step to moderate comments starting today and will delete spam before you see it. Comments that are clearly spam or are purely promotional in nature will be filtered. The only down side is that comments will have a slight latency getting posted. We will continue to encourage debate and won't censor conflicting or alternative ideas.Thanks for understanding. And keep up the rich discussion.", "by Shyam SeshadriNowadays, when I talk with (read: rant at) anyone about why they should do test driven development or write unit tests, my spiel has gotten extremely similar and redundant to the point that I don't have to think about it anymore. But even when I do pairing with skeptics, even as I cajole and coax testable code or some specific refactorings out of them, I wonder, why is it that I have to convince you of the worth of testing ? Shouldn't it be obvious ?And sadly, it isn't. Not to many people. To many people, I come advocating the rise of the devil itself. To others, it is this redundant, totally useless thing that is covered by the manual testers anyway. The general opinion seems to be, \"I'm a software engineer. It is my job to write software. Nowhere in the job description does it say that I have to write these unit tests.\" Well, to be fair, I haven't heard that too many times, but they might as well be thinking it, given their investment in writing unit tests. And last time I checked, an engineer's role is to deliver a working software. How do you even prove that your software works without having some unit tests to back you up ? Do you pull it up and go through it step by step, and start cursing when it breaks ? Because without unit tests, the odds are that it will.But writing unit tests as you develop isn't just to prove that your code works (though that is a great portion of it). There are so many more benefits to writing unit tests. Lets talk in depth about a few of these below.Instantaneous GratificationThe biggest and most obvious reason for writing unit tests (either as you go along, or before you even write code) is instantaneous gratification. When I write code (write, not spike. That is a whole different ball game that I won't get into now), I love to know that it works and does what it should do. If you are writing a smaller component of a bigger app (especially one that isn't complete yet), how are you even supposed to know if what you just painstakingly wrote even works or not ? Even the best engineers make mistakes.Whereas with unit tests, I can write my code. Then just hit my shortcut keys to run my tests, and voila, within a second or two, I have the results, telling me that everything passed (in the ideal case) or what failed and at which line, so I know exactly what I need to work on. It just gives you a safety net to fall back on, so you don't have to remember all the ways it is supposed to work in. Something tells you if it is or not.Also, doing Test Driven Development when developing is one of the best ways to keep track of what you are working on. I have times when I am churning out code and tests, one after the other, before I need to take a break. The concept of TDD is that I write a failing test, and then I write just enough code to pass that test. So when I take a break, I make it a point to leave at a failing test, so that when I come back, I can jump right back into writing the code to get it to pass. I don't have to spend 15 - 20 minutes reading through the code to figure out where I left off. My asserts usually tell me exactly what I need to do.Imposing Modularity / ReusabilityThe very first rule of reusable code is that you have to be able to instantiate an instance of the class before you can use it. And guess what ? With unit tests, you almost always have to instantiate an instance of the class under test. Therefore, writing a unit test is always a first great step in making code reusable. And the minute you start writing unit tests, most likely, you will start running into the common pain points of not having injectable dependencies (Unless of course, you are one of the converts, in which case, good for you!).Which brings me to the next point. Once you start having to jump through fiery hoops to set up your class just right to test it, you will start to realize when a class is getting bloated, or when a certain component belongs in its own class. For instance, why test the House when what you really want to test is the Kitchen it contains. So if the Kitchen class was initially part of the House, when you start writing unit tests, it becomes obvious enough that it belongs separately. Before long, you have modular classes which are small and self contained and can be tested independently without effort. And it definitely helps keep the code base cleaner and more comprehensible.Refactoring Safety NetAny project, no matter what you do, usually ends up at a juncture where the requirements change on you. And you are left with the option of refactoring your codebase to add / change it, or rewrite from scratch. One, never rewrite from scratch, always refactor. Its always faster when you refactor, no matter what you may think. Two, what do you do when you have to refactor and you don't have unit tests ? How do you know you haven't horribly broken something in that refactor ? Granted, IDE's such as Eclipse and IntelliJ have made refactoring much more convenient, but adding new functionality or editing existing features is never simple.More often than not, we end up changing some undocumented way the existing code behaved, and blow up 10 different things (it takes skill to blow up more, believe me, I have tried). And its often something as simple as changing the way a variable is set or unset. In those cases, having unittests (remember those things you were supposed to have written?) to confirm that your refactoring broke nothing is godsend. I can't tell you the amount of times I have had to refactor a legacy code base without this safety net. The only way to ensure I did it correct was to write these large integration tests (because again, no unit tests usually tends to increase the coupling and reduce modularity, even in the most well designed code bases) which verified things at a higher level and pray fervently that I broke nothing. Then I would spend a few minutes bringing up the app everytime, and clicking on random things to make sure nothing blew up. A complete waste of my time when I could have known the same thing by just running my unit tests.DocumentationFinally, one of my favorite advantages to doing TDD or writing unit tests as I code. I have a short memory for code I have written. I could look back at the code I wrote two days ago, and have no clue what I was thinking. In those cases, all I have to do is go look at the test for a particular method, and that almost always will tell me what that method takes in as parameters, and what all it should be doing. A well constructed set of tests tell you about valid and invalid inputs, state that it should modify and output that it may return.Now this is useful for people like me with short memory spans. But it is also useful, say, when you have a new person joining the team. We had this cushion the last time someone joined our team for a short period of time, and when we asked him to add a particular check to a method, we just pointed him to the tests for that method, which basically told him what the method does. He was able to understand the requirements, and go ahead and add the check with minimal handholding. And the tests give a safety net so he doesn't break anything else while he was at it.Also useful is the fact that later, when someone comes marching through your door, demanding you fix this bug, you can always make sure whether it was a a bug (in which case, you are obviously missing a test case) or if it was a feature that they have now changed the requirements on (in which case you already have a test which proves it was your intent to do it, and thus not a bug).", "by Mi\u0161ko Hevery\n\n\nYou hear people talking about small/medium/large/unit/integration/functional/scenario tests but do most of us really know what is meant by that? Here is how I think about tests.\n\nUnit/Small\n\nLets start with unit test. The best definition I can find is that it is a test which runs super-fast (under 1 ms) and when it fails you don't need debugger to figure out what is wrong. Now this has some implications. Under 1 ms means that your test cannot do any I/O. The reason this is important is that you want to run ALL (thousands) of your unit-tests every time you modify anything, preferably on each save. My patience is two seconds max. In two seconds I want to make sure that all of my unit tests ran and nothing broke. This is a great world to be in, since if tests go red you just hit Ctrl-Z few times to undo what you have done and try again. The immediate feedback is addictive. Not needing a debugger implies that the test is localized (hence the word unit, as in single class).\n\nThe purpose of the unit-test is to check the conditional logic in your code, your 'ifs' and 'loops'. This is where the majority of your bugs come from (see theory of bugs). Which is why if you do no other testing, unit tests are the best bang for your buck! Unit tests, also make sure that you have testable code. If you have unit-testable code than all other testing levels will be testable as well.\n\nA KeyedMultiStackTest.java is what I would consider great unit test example from Testability Explorer. Notice how each test tells a story. It is not testMethodA, testMethodB, etc, rather each test is a scenario. Notice how at the beginning the test are normal operations you would expect but as you get to the bottom of the file the test become little stranger. It is because those are weird corner cases which I have discovered later. Now the funny thing about KeyedMultiStack.java is that I had to rewrite this class three times. Since I could not get it to work under all of the test cases. One of the test was always failing, until I realized that my algorithm was fundamentally flawed. By this time I had most of the project working and this is a key class for byte-code analysis process. How would you feel about ripping out something so fundamental out of your system and rewriting it from scratch? It took me two days to rewrite it until all of my test passed again. After the rewrite the overall application still worked. This is where you have an AHa! moment, when you realize just how amazing unit-tests are.\n\nDoes each class need a unit test? A qualified no. Many classes get tested indirectly when testing something else. Usually simple value objects do not have tests of their own. But don't confuse not having tests and not having test coverage. All classes/methods should have test coverage. If you TDD, than this is automatic.\n\nMedium/Functional\n\nSo you proved that each class works individually, but how do you know that they work together? For this we need to wire related classes together just as they would be in production and exercise some basic execution paths through it. The question here we are trying to answer is not if the 'ifs' and 'loops' work, (we have already answered that,) but whether the interfaces between classes abide by their contracts. Great example of functional test is MetricComputerTest.java. Notice how the input of each test is an inner class in the test file and the output is ClassCost.java. To get the output several classes have to collaborate together to: parse byte-codes, analyze code paths, and compute costs until the final cost numbers are asserted.\n\nMany of the classes are tested twice. Once directly throughout unit-test as described above, and once indirectly through the functional-tests. If you would remove the unit tests I would still have high confidence that the functional tests would catch most changes which would break things, but I would have no idea where to go to look for a fix, since the mistake can be in any class involved in the execution path. The no debugger needed rule is broken here. When a functional test fails, (and there are no unit tests failing) I am forced to take out my debugger. When I find the problem, I add a unit test retroactively to my unit test to 1) prove to myself that I understand the bug and 2) prevent this bug from happening again. The retroactive unit test is the reason why the unit tests at the end of KeyedMultiStackTest.java file are \"strange\" for a lack of a better world. They are things which I did not think of when i wrote the unit-test, but discovered when I wrote functional tests, and through lot of hours behind debugger track down to KeyedMultiStack.java class as the culprit.\n\nNow computing metrics is just a small part of what testability explorer does, (it also does reports, and suggestions) but those are not tested in this functional test (there are other functional tests for that). You can think of functional-tests as a set of related classes which form a cohesive functional unit for the overall application. Here are some of the functional areas in testability explorer: java byte-code parsing, java source parsing, c++ parsing, cost analysis, 3 different kinds of reports, and suggestion engine. All of these have unique set of related classes which work together and need to be tested together, but for the most part are independent.\n\nLarge/End-to-End/Scenario\n\nWe have proved that: 'ifs' and 'loops' work; and that the contracts are compatible, what else can we test? There is still one class of mistake we can make. You can wire the whole thing wrong. For example, passing in null instead of report, not configuring the location of the jar file for parsing, and so on. These are not logical bugs, but wiring bugs. Luckily, wiring bugs have this nice property that they fail consistently and usually spectacularly with an exception. Here is an example of end-to-end test: TestabilityRunnerTest.java. Notice how these tests exercises the whole application, and do not assert much. What is there to assert? We have already proven that everything works, we just want to make sure that it is wired properly.", "By James A. Whittaker Memory is supposed to be the first thing to go as one ages but in the grand scheme of engineering, software development can hardly be called elderly. Indeed, we\u2019re downright young compared to civil, mechanical, electrical and other engineering disciplines. We cannot use age as an excuse for our amnesia.There are two types of amnesia that plague software testers. We have team amnesia that causes us to forget our prior projects, our prior bugs, tests, failures and so forth. It\u2019s time we developed a collective memory that will help us to stop repeating our mistakes. Every project is not a fresh start, it's only a new target for what is now a more experienced team. The star ship Enterprise keeps a ship's log. A diary that documents its crews\u2019 adventures and that can be consulted for details that might help them out of some current jam. I'm not advocating a diary for test teams, but I do want a mechanism for knowledge retention. The point is that as a team we build on our collective knowledge and success. The longer the Enterprise crews' memory, the better their execution. Quick, tell me about the last major failure of your teams\u2019 product? Does your team have a collective memory of common bugs? Do you share good tests? If one person writes a test that verifies some functionality does everyone else know this so they can spend their time testing elsewhere? Do problems that break automation get documented so the painstaking analysis to fix them doesn\u2019t have to be repeated? Does the team know what each other is doing so that their testing overlaps as little as possible? Is this accomplished organically with dashboards and ongoing communication or are the only sync points work-stopping, time-wasting meetings? Answer honestly. The first step to recovery is to admit you have a problem.The second type of memory problem is industry amnesia. When I mentioned Boris Beizer and the pesticide paradox in my last post, how many of you had to look it up? And those who did know about it, how's your AJAX skills? Be honest...there are folks with both a historical perspective and modern skills but they are far too rare. Our knowledge, it seems, isn\u2019t collective, it\u2019s situational. Those who remember Beizer's insights worked in a world where AJAX didn\u2019t exist, those who webspeak without trouble missed a lot of foundational thinking and wisdom. Remembering only what is in front of us is not really memory.Industry amnesia is a real problem. Think about it this way: That testing problem in front of you right now (insert a problem that you are working on here) has been solved before. Are you testing an operating system? Someone has, many of them. Web app? Yep, that\u2019s been done. AJAX? Client-server? Cloud service? Yes, yes and yes. Chances are that what you are currently doing has been done before. There are some new testing problems but chances are the one in front of you now isn\u2019t one of them. Too bad the collective memory of the industry is so bad or it would be easy to reach out for help. Let me end this column by pointing my finger inward: How will we (Google) test the newly announced Chrome operating system? How much collective knowledge have we developed from Chrome and Android? How much of what we learned testing Android will help? How much of it will be reusable? How easily will the Chrome and Andriod test teams adapt to this new challenge? Certainly many test problems are ones we've faced before.Will we remember?", "by Juergen Allgayer, Conference ChairThe organization of the 4th Google Conference on Test Automation (GTAC 2009), held in Zurich October 21-22, is well underway and we are looking forward to an exciting event. We are pleased to announce the GTAC website and that presenters will be able to apply for sponsorship.Sponsorship availableOne presenter per accepted proposal will be able to apply for sponsorship from Google. For sponsored presenters Google will arrange and pay travel and lodging (within reason). Further details will be announced to speakers of accepted proposals.Website on-line (http://www.gtac.biz/)The conference website is now on-line with lots of information about the event, on how to submit proposals, about the venue, and much more.", "by Stephen Ng and Noel Yap\n\n\n\n\n\nOld habits die hard. Particularly when it comes to testing--once you get used to working in a project blanketed with lots of fast-running unit tests, it's hard to go back.\nSo when some of us at Google learned that we weren't able to use our favorite mocking libraries when writing for our favorite mobile platform (Android, naturally), we decided to do something about it.\n\n\nAs many of you already know (since you read this blog), mocking combined with dependency injection is a valuable technique for helping you write small, focused unit tests.  And those tests in turn can help you structure your code so that it's loosely coupled, making it more readable and maintainable.\nWith a bit of setup, this can be done in Android, too.  We've put together a tutorial describing our approach.  It's the first installment in what we hope will be a series of articles on android app development and testing, from the perspective of Googlers who are not actually on the Android team.  We'd love to hear whether you find it useful.", "Take your average developer and ask \"do you know language/technology X?\" None of us will feel any shame in admitting that we do not know X. After all there are so many languages, frameworks and technologies, how could you know them all? But what if X is writing testable code? Somehow we have trouble answering the question \"do you know how to write tests?\" Everyone says yes, whether or not we actually know it. It is as if there is some shame in admitting that you don't know how to write tests.Now I am not suggesting that people knowingly lie here, it is just that they think there is nothing to it. We think: I know how to write code, I think my code is pretty good, therefore my code is testable!I personally think that we would do a lot better if we would recognize testability as a skill in its own right. And as such skills are not innate and take years of practice to develop. We could than treat it as any other skill and freely admit that we don't know it. We could than do something about it. We could offer classes, or other materials to grow our developers, but instead we treat it like breathing. We think that any developer can write testable code.It took me two years of writing tests first, where I had as much tests as production code, before I started to understand what is the difference between testable and hard to test code. Ask yourself, how long have you been writing tests? What percentage of the code you write is tests?Here is a question which you can ask to prove my point: \"How do you write hard to test code?\" I like to ask this question in interviews and most of the time I get silence. Sometimes I get people to say, make things private. Well if visibility is your only problem, I have a RegExp for you which will solve all of your problems. The truth is a lot more complicated, the code is hard to test doe to its structure, not doe to its naming conventions or visibility. Do you know the answer?We all start at the same place. When I first heard about testing I immediately thought about writing a framework which will pretend to be a user so that I can put the app through its paces. It is only natural to thing this way. This kind of tests are called end-to-end-tests (or scenario or large tests), and they should be the last kind of tests which you write not the first thing you think of. End-to-end-tests are great for locating wiring bugs but are pretty bad at locating logical bugs. And most of your mistakes are in logical bugs, those are the hard ones to find. I find it a bit amusing that to fight buggy code we write even more complex framework which will pretends to be the user, so now we have even more code to test.Everyone is in search of some magic test framework, technology, the know-how, which will solve the testing woes. Well I have news for you: there is no such thing. The secret in tests is in writing testable code, not in knowing some magic on testing side. And it certainly is not in some company which will sell you some test automation framework. Let me make this super clear: The secret in testing is in writing testable-code! You need to go after your developers not your test-organization.Now lets think about this. Most organizations have developers which write code and than a test organization to test it. So let me make sure I understand. There is a group of people which write untestable code and a group which desperately tries to put tests around the untestable code. (Oh and test-group is not allowed to change the production code.) The developers are where the mistakes are made, and testers are the ones who feel the pain. Do you think that the developers have any incentive to change their behavior if they don't feel the pain of their mistakes? Can the test-organization be effective if they can't change the production code?It is so easy to hide behind a \"framework\" which needs to be built/bought and things will be better. But the root cause is the untestable code, and until we learn to admit that we don't know how to write testable code, nothing is going to change...", "by Shyam SeshadriWe all have separation anxiety. Its a human tendency. We love inertia, and we don\u2019t like change. But why should your code have separation anxiety ? Its not human (even though it might try and grow a brain of its own at times)!I bring this up because I got the chance to work with someone who had some questions on how to test UI code. Fairly innocuous question, and in most cases, my response would have been, keep the UI code simple and free of any logic, and don\u2019t worry too much about it. Or you could write some nice end to end / integration tests / browser based tests. So with that response set in mind, I set off into the unknown. Little was I to know was that was the least of my concerns. As I sat down to look at the code, I saw that there were already tests for the code. I was optimistic now, I mean, how bad could things be if there are already tests for it ?Well, I should remember next time to actually look at the tests first. But anyways, there were tests, so I was curious what kinds of tests they wanted to write and what kind of help I could provide, if any. So it turns out, the class was some sort of GUI Component, which basically had some fields, and depending on whether they were set of not, displayed them as widgets inside of it. So there were, say, 5 fields, and differing combinations of what was set would produce different output. The nice thing was that the rendered data was returned as a nice Java object, so you could easily assert on what was set, and get a handle on the widgets inside of it, etc. So the method was something along the following lines :public RenderedWidgetGroup render() {    RenderedWidgetGroup group =       createWidgetGroup(this.componentId,                         this.parent);    if (this.name = null) {        return group;    }     group.addWidget(new TextWidget(this.name));    group.addWidget(       new DateWidget(           this.updatedTimestamp == null ?                this.createdTimestamp : this.updatedTimestamp));    return group;}So far, so good, right ? Nice, clean, should be easy to test if we can set up this component with the right fields. After that, it should just be a few tests based on the different code paths defined by the conditionals. Yeah, thats what I thought too. So, me, being the naive guy that I was, said, yeah, that  looks nice, should be easy to test. I don\u2019t see the problem.Well, then I was taken to the tests. The first thing I see is a huge test. Or atleast thats what I think it is. Then I read it more closely, and see its a private method. It seems to take in a bunch of fields (Fields with names that I distinctly remembered from just a while ago) and churn out a POJO (Plain Old Java Object). Except this POJO was not the GUI Component object I expected. So obviously, I was curious (and my testing sensors were starting to tingle).  So I followed through to where it was called (which wasn\u2019t very hard, it was a private method) and lo and behold, my worst fears confirmed.The POJO object generated by the private method was passed in to the constructor of the GUI component, which (on following it further down the rabbit hole) in its constructor did something like the following :public MyGUIComponent(ComponentId id,                     Component parent,                     MyDataContainingPOJO pojo) {   super(id, parent);   readData(pojo);} And of course, readData, as you would expect, is a :Private methodLooks through the POJOIf it finds a field which is not null then it sets it in the Gui ComponentAnd of course, without writing the exact opposite of this method in the unit test, it just wasn\u2019t possible to write unit tests. And sudddenly, it became clear why they were having trouble unit testing their Gui Components. Because if they wanted to test render (Which was really their aim), they would have to set up this POJO with the correct fields (which of course, to make things more interesting / miserable, had sub POJOs with sub POJOs of their own. Yay!) to be exercised in their test.The problem with this approach is two fold :I need tests to ensure that the parsing and reading from the POJO logic is sound, and that it correctly sets up the GUI Component.Every time I want to test the render logic, I end up testing (unintentionally, and definitely unwantedly) the parsing logic.This also adds, as I saw, obviously complicated pre test setup logic which should not be required to test something completely different. This is a HUGE code smell. Unit testing a class should not be an arduous, painful task. It should be easy as setting up a POJO and testing a method. The minute you have to perform complicated setup to Unit test a class (Note, the keyword is unit test. You can have integration tests which need some non trivial setup.), stop! There is something wrong.The problem here is that there is a mixing of concerns in the MyGuiComponent class. As it turns out, MyGuiComponent breaks a few fundamental rules of testability. One, it does work in the constructor. Two, it violates the law of demeter, which states that the class should ask for what it needs, not what it doesn\u2019t need to get what it needs (Does that even make sense ?). Thirdly, it is mixing concerns. That is, it does too much. It knows both how to create itself as well as do the rendering logic. Let me break this down further :Work in the constructorIf you scroll back up and look at the constructor for MyGuiComponent, you will see it calling readData(pojo). Now, the basic concept to test any class is that you have to create an instance of the class under test (unless it has static methods. Don\u2019t get me started on that\u2026). So now, every time you create an instance of MyGuiComponent, guess what ? readData(pojo) is going to get called. Every. Single. Time ! And it cannot be mocked out. Its a private method. And god forbid if you really didn\u2019t care about the pojo and passed in a null. Guess what ? It most probably will blow up with a NullPointerException. So suddenly, that innocuous method in the constructor comes back to haunt you when you write yours tests (You are, aren\u2019t you ?).Law of DemeterFurthermore, if you look at what readData(pojo) does, you would be even more concerned. At its base, MyGuiComponent only cares about 6 fields which it needs to render. Depending on the state of each of these fields, it adds widget. So why does the constructor ask for something totally unrelated ? This is a fundamental violation of the Law of Demeter. The Law of Demeter can be summed up as \u201cAsk for what you need. If you need to go through one object to get what you need, you are breaking it.\u201d. A much more fancier definition can be found on the web if you care, but the minute you see something like A.B().C() or something along those lines, there is a potential violation.In this case, MyGuiComponent doesn\u2019t really care about the POJO. It only cares about some fields in the POJO, which it then assigns to its own fields. But the constructor still asks for the POJO instead of directly asking for the fields. What this means is that instead of just creating an instance of MyGuiComponent with the required fields in my test, I now have to create the POJO with the required fields and pass that in instead of just setting it directly. Convoluted, anyone ?Mixing ConcernsFinally, what could be considered an extension of the previous one, but deserves a rant of its own. What the fundamental problem with the above class is that it is mixing concerns. It knows both how to create itself as well as how to render itself once it has been created. And the way it has been coded enforces that the creation codepath is executed every single time. To provide an analogy for how ridiculous this is, it is like a a Car asking for an Engine Number and then using that number to create its own engine. Why the heck should a car have to know how to create its engine ? Or even a car itself ? Similarly, why should MyGuiComponent know how to create itself ? Which is exactly what is happening here.SolutionNow that we had arrived at the root of the problem, we immediately went about trying to fix it. My immediate instinct was to pull out MyGuiComponent into the two classes that I was seeing. So we pulled out a MyGuiComponentFactory, which took up the responsibility of taking in the POJO and creating a GuiComponent out of it. Now this was independently testable. We also added a builder pattern to the MyGuiComponent, which the factory leveraged.class MyGuiComponentFactory {    MyGuiComponent createFromPojo(ComponentId id,                                 Component parent,                                 MyDataContainingPOJO pojo) {     // Actual logic of converting from pojo to     // MyGuiComponent using the builder pattern    }}class MyGuiComponent {    public MyGuiComponent(ComponentId id,                         Component parent) {       super(id, parent);   }    public MyGuiComponent setName(String name) {       this.name = name;       return this;   }} And now, suddenly (and expectedly, I would like to add), the constructor for MyGuiComponent becomse simple. There is no work in the constructor. The fields are set up through setters and the builder pattern. So now, we started writing the unit tests for the render methods. It took about a single line of setup to instantiate MyGuiComponent, and we could test the render method in isolation now. Hallelujah!!Disclaimer :No code was harmed / abused in the course of the above blog post. There were no separation issues whatsoever in the end, it was a clean mutual break!", "By James A. Whittaker Let me be clear that these plague posts are specifically aimed at pointing out problems in testing. I'll get around to the lore and the solutions afterward. But keep up the debate in the comments, that's what this blog is all about.The plague of repetitivenessIf aimlessness is the result of \u2018just doing it\u2019 then repetitiveness is the result of \u2018just doing it some more.\u2019 Pass after pass, build after build, sprint after sprint, version after version we test our product. Developers perform reviews, write unit tests and run static analyzers. But we have little insight into this effort and can't take credit for it. Developers test but then we retest. We can\u2019t assume anything about what they did so we retest everything. As our product grows in features and bug fixes get applied, we continue our testing. It isn\u2019t long until new tests become old tests and all of them eventually become stale.This is Boris Beizer\u2019s pesticide paradox. Pesticide will kill bugs, but spray the same field enough times with the same poison and the remaining bugs will grow immune. Rinse and repeat is a process for cleaning one\u2019s hair, not testing software. The last thing we need is a build full of super-bugs that resist our \u2018testicide.\u2019 Even worse, all that so-called successful testing will give us a false sense of thoroughness and make our completeness metrics a pack of very dangerous lies. When you aren't finding bugs it's not because there are no bugs, it's the repetition that's creating the pesticide paradox. Farmers know to adjust their pesticide formula from time to time and also to adjust the formula for the specific type of pest they expect in their fields. They do this because they understand the history of pesticide they used and know they can't get by with brute force repetition of they same old poison. Testers must pay attention to their test results too and watch for automation that isn\u2019t adding value. A healthy injection of variation into automation is called for. Change the order of tests, change their data source, find new environments, modify input values do something the bugs aren\u2019t prepared for.", "By James A. Whittaker Yes I am going to be speaking at GTAC, thanks for asking. Frankly, I can't wait. I spoke at a Swiss testing conference and at the University of Zurich a couple years ago and I enjoyed the area and the people a lot. Excellent food, good beer and lots of cool European history and quaint back streets to wallow in. I hope to see you there.Speaking of speaking, I just finished giving my first internal tech talk this past week. I spoke about the 7 Plagues of Software Testing and received a lot of input from Googlers about them. I'm encouraged enough that Googlers found them thought provoking that I've decided to broaden the conversation by posting them here as well. My plan at GTAC is to give you details on how Google is addressing these plagues in our own testing and hopefully you'll be willing to share yours as well. One plague per post lest this blog be quarantined ...The Plague of AimlessnessLore. It\u2019s more than just a cool word. It conjures up a sorcerous image in one\u2019s mind of ancient spell books and learned wizards with arcane and perilously attained knowledge.And it\u2019s exactly what we lack in software testing. Testing lore? Are you kidding me? Where is it? Who\u2019s bogarting it? Can I have a hit?The software testing industry is infected with the plague of aimlessness. We lack lore; we lack a body of knowledge that is passed from wizard to apprentice and written down in spell books for the industrious to study. Our apprentices are without their masters. We must all reinvent the wheel in the privacy of our offices only to have other testers the world over reinvent it in theirs.I suggest we stop this nonsense. Testing is far too aimless. We test because we must or our manager tells us to do so. We automate because we can or because we know how to, not because it is part of some specific and proven strategy and certainly not because our lore dictates it. Is there a plan or some documented wisdom that guides our testing or are we just banging on the keyboard hoping something will fail? Where are the testing spell books? Surely the perilously attained knowledge of our tester forebears is something that we can access in this age of readily available information?When a hunter makes a kill, they remember the terrain and circumstances. They pass this knowledge on to their successors. Over time they understand the habits of their prey and the collective knowledge of many hunters makes the job of future hunters far easier. When you see this terrain, you can expect game to behave in this manner. Can we say the same of testing? How well do we learn from each other? Do our \u2018eureka moments\u2019 get codified so that future testers will not have to suffer the aimless thrashing that we suffered? Can we say when you see functionality like that, the best way to test it is like this?The plague of aimlessness is widespread. The need for testing lore is acute. Nike tells us to \u2018just do it\u2019 but what applies to exercise is not good advice for software testing. The next time you find yourself \u2018just doing\u2019 testing, pause for a moment and ask yourself \u2018what is my goal?\u2019 and \u2018is there a purpose to this test?\u2019 If the answer doesn\u2019t immediately come to mind, you\u2019re aimless, just doing it, and relying on luck and the sheer force of effort to find your quarry.Luck has no place in sorcery or hunting and it has no place in testing. Luck is a nice happenstance, but it cannot be our plan A. Watch for the plague of aimlessness. Document your successes, scrutinize your failures and make sure you pass on what you learn from this introspection to your colleagues.Be their wizard. Build a testing spell book and share it with others on your team. Over time you\u2019ll banish the plague of aimlessness.", "Posted by Juergen Allgayer - GTAC Conference Chair GTAC 2009: Testing for the webThe 4th Google Test Automation Conference brings together a selected set of industry practitioners around the topic of software testing and automation. This annual conference provides a forum for presentations and connects professionals with each other. To increase outreach, presentations are published online for everybody to see.This years theme is Testing for the Web, topics may include:Testing the UI of modern web applications (HTML5, Ajax)Testing applications on mobile devicesTesting in the cloudWeb testing tools (Selenium, Webdriver and co)Testing distributed asynchronous applicationsTesting for web browser compatibilityTesting large storage systemsLoad and performance testingFinding and reproducing failures that matterIt seemed like a good idea (things you expected to work, but that didn't)Presentations are targeted at experienced engineers actively working on problems of quality, test automation and techniques, but also include students and academics. We encourage innovative ideas, controversial experiences, problems, and solutions that further the discussion of software engineering and testing. Presentations are 45 min in length and speakers should be prepared for an active question and answer session following their presentation. While ideas are good, ideas refined by experience are even more interesting to participants at GTAC.The conference is a two day event comprised of a single track of talks. Our philosophy is to engage a small set of active participants who all experience the same topics carrying the discussions into lightning talks, speaker Q&A, and topical discussion groups. Each year we have worked to identify a location that has a unique profile of technology professionals. This year the conference will be held at the Google office in Zurich, Switzerland on October 21 and 22, 2009.Submission of ProposalsPlease email a detailed and extended abstract (one page at most) to gtac-2009-cfp@google.com. Your submission must include the name of topic, author(s), affiliation, and an outline of the proposed talk. We strongly recommend you to also submit one or two highlight slides of the talk. Submit your proposal before August 1, 2009. We will acknowledge reception within one business day. Where employer or disclosure authorization is needed, authors need to obtain it prior to submitting. The program committee will evaluate proposals based on quality and relevance. All submissions will be held confidentially prior to contacting the selected presenters.Notification of AcceptanceNotification of acceptance will be sent out on or before August 8, 2009. Authors of accepted proposals will present at the conference and their talk will be made available to the public on YouTube.CopyrightGTAC requires authors to present at the conference and permit their presentation to be made available on YouTube.AttendeesTo ensure active participation and provide a variety of technical perspectives, we select applying attendees. Further information will be published via a call for participation at a later time.Important DatesAugust 1 - Deadline for presentation proposalsAugust 8 - Notification of acceptanceOctober 21+22 - GTAC conference (Zurich, Switzerland)QuestionsIf you have questions regarding the submission process or potential topics please email us at: gtac-2009@google.comWe will add more information to the Google Testing Blog as we get closer to the dates.", "We have already received several inquiries about this year's GTAC - thanks for your enthusiasm, here's the news you've been waiting for - we will host the GTAC 2009 October 21 and 22 at the Google offices in Zurich, Switzerland.As with previous years, the focus of the conference will be on solving software engineering challenges using tools and automation. This year will have a special focus on \"Testing for the Web\". We are looking forward to getting together to sharing lessons learned and practical experience testing web apps, services, and systems. We are also encouraging a discussion on effectively testing apps and services for mobile devices.We will have a call for proposals coming out very soon - watch this space!One of the strengths of the conference is that it's driven by a peer group and vocal participation. As in previous years, GTAC is an invitation only conference to share great ideas and to have your thoughts challenged and refined. When you apply, we want you to tell us what ideas and questions you'll bring to the conference, and how you can further the discussion. We will open the application process in late July 2009.Please send suggestions, questions and recommendations to: gtac-2009@google.com or post your comments here to this blog.", "By James Whittaker One of the best parts about change is meeting new people and I've met a lot of Googlers in Mountain View and Kirkland over the past two weeks. There are many burning questions we've discussed but one has surfaced so much that it has to take top billing: manual vs. automated testing. Google, I've learned, has come full circle on this issue. When the company was new and Search was king most testing was manual and like a lot of startups there was little focus on actual QA. In recent years the pendulum has swung to automation with developers writing a lot of unit tests and testers creating automation frameworks prolifically. And now with my recent work on manual testing making the rounds what will throwing me into this mix produce? Actually, I'd like to put the manual vs. automation debate to bed. Instead, I think the issue is test design versus doing something because we can. How much automation is written not because there is a clear need but because we can? Or, perhaps, because we think we must? Hmm. Before you cry bias, how much manual testing is seat of the pants versus intentional, purposeful testing?See the point? Whether you are talking about manual or automated testing, it's test design ... identifiying what needs testing and what is the best way to test it ... that has to take center stage. Too often we are solution focused and marry ourselves to our automation rather than the product we are testing. Whenever I see testers that seem more vested in their automation than in the product they are shipping I worry the pendulum has swung too far. Automation suffers from this because there is a product - the tool - that can become the object of obession. Manual testers don't really produce such a physical baby they can fuss over or they probably would fuss just as much as the automaters. Ignore the tool, focus on the problem it should solve!Besides, I think fundamentally that manual and automated testing are irreversibly linked. All good automation begins it's life as manual test cases. And to create really good automation, you have to be good at manual testing. It's during manual testing that a tester gets good at test design, identifying important testing problems and crafting the solution approach. Any tester who isn't good at test design should think twice before they automate. Let's shift the debate to good test design. Whether those tests ultimately end up being executed manually or via a tool is a moot point, let's just make them good ones that solve real testing problems. How many octomoms does the testing world need anyway?", "By James Whittaker Here I am. Thanks for all the inquiries. Why the change to Google? I\u2019ve been asked that over and over again. As I reflect on the whole process, I have to admit that I like change. I like the challenge it brings, the creativity it sparks and the potential that I might fail at some new endeavor is simply intoxicating. Change, I think, is good. After all, if Robert Plant and Jimmy Page had never ventured beyond their comfortable British borders, they would have never written Kashmir and the planet is far better off for having that song. My first week at Google has been a whirlwind of activity. I had the distinction of being (at a ripe of old age of 43) the oldest person at new employee orientation. I passed a billionaire in the hallway. I sat in a room with some of the best testing minds in Silicon Valley and walked across campus with a young engineer whose biggest problem is that she can\u2019t learn enough fast enough. I\u2019ve signed dozens of books.There\u2019s much to learn and much to do. I\u2019ll catalog the results here if anyone is interested in following it. Coming from a company like Microsoft, I am used to mind-bogglingly complex problems and comfortable with partial solutions that point toward a better but still imperfect future. My role at Google will be to continue to thwart the impossible. Innovation as a main course is what brought me here. I hope to continue my work on testing tours and envisioning the future, but I am even more excited about the things I can\u2019t yet see. Given the team that I am working with here, I think it is safe to expect big things.In case you are interested, I am located in the Kirkland WA office and not yet assigned to a project. If I am lucky I will manage to get my hands into everything. I\u2019ll try and be careful not to spill the secret sauce over my nicest shirt\u2026", "By Patrick CopelandI'm excited to announce that James Whittaker has joined us as our newest Test Director at Google. James comes to us most recently from Microsoft. He has spent his career focusing on testing, building high quality products, and designing tools and process at the industrial scale. In the not so distant past, he was a professor of computer science at Florida Tech where he taught an entire software testing curriculum and issued computer science degrees with a minor in testing (something we need more schools to do). Following that , he started a consulting practice that spanned 33 countries. Apparently, fashion is not high on his list as he he has collected soccer jerseys from many of these countries and wears those during major tournaments. At Microsoft he wrote a popular blog, and in the near future you can expect him to start contributing here. He has trained thousands of testers worldwide. He's also written set of books in the How to Break Software series. They have won awards and achieved best seller status. His most recent book is on exploratory testing is coming out this summer. It is not a stretch to say that he is one of the most recognizable names in the industry and has had a deep impact on the field of testing. If you have a chance, strike up a conversation with James about the future of testing. His vision for what we'll be doing and how our profession will change is interesting, compelling and not just a little bit scary.Join me in welcoming James to Google!", "By Simon Stewart\n\nIt's a complaint that I've heard too many times to ignore: \"My Selenium tests are unstable!\" The tests are flaky. Sometimes they work, sometimes they don't. How deeply, deeply frustrating! After the tests have been like this for a while, people start to ignore them, meaning all the hard work and potential value that they could offer a team in catching bugs and regressions is lost. It's a shameful waste, but it doesn't have to be.\n\nFirstly, let's state clearly: Selenium is not unstable, and your Selenium tests don't need to be flaky. The same applies for your WebDriver tests.\n\nOf course, this raises the obvious question as to why so many selenium tests fail to do what you intended them to. There are a number of common causes for flaky tests that I've observed, and I'd like to share these with you. If your (least) favourite bugbear isn't here, please tell us about it, and how you would like to approach fixing it, in a comment to this post!\n\nProblem: Poor test isolation.\nExample: Tests log in as the same user and make use of the same fixed set of data.\nSymptoms: The tests work fine when run alone, but fail \"randomly\" during the build.\nSolution: Isolate resources as much as makes sense. Set up data within the tests to avoid relying on a \"set up the datastores\" step in your build (possibly using the Builder pattern). You may want to think about setting up a database per developer (or using something like Hypersonic or SQLite as a light-weight, in-memory, private database) If your application requires users to log in, create several user accounts that are reserved for just your tests, and provide a locking mechanism to ensure that only one test at a time is using a particular user account.\n\nProblem: Relying on flaky external services.\nExample: Using production backends, or relying on infrastructure outside of your team's control\nSymptom: All tests fail due to the same underlying cause.\nSolution: Don't rely on external services that your team don't control. This may be easier said than done, because of the risk of blowing out build times and the difficulty of setting up an environment that models reality closely enough to make the tests worthwhile. Sometimes it makes sense to start servers in-process, using something like Jetty in the Java world, or webrick in Ruby.\n\nWatching the tests run is a great way to spot these external services. For example, on one project the tests were periodically timing out, though the content was being served to the browser. Watching the tests run showed the underlying problem: we were serving \"fluff\" --- extra content from an external service in an iframe. This content was sometimes not loading in time, and even though it wasn't necessary for our tests the fact it hadn't finished loading was causing the problem. The solution was to simply block the unnecessary fluff by modifying the firewall rules on the Continuous Build machine. Suddenly, everything ran that little bit more smoothly!\n\nAnother way to minimize the flakiness of these tests is to perform a \"health check\" before running the tests. Are all the services your tests rely on running properly? Given that end-to-end tests tend to run for a long time, and may place an unusual load on a system, this isn't a fool-proof approach, but it's better to not run the tests at all rather than give a team \"false negatives\".\n\nProblem: Timeouts are not long enough\nExample: You wait 10 seconds for an AJAX request that takes 15 to complete\nSymptom: Most of the time the tests run fine, but under load or exceptional circumstances they fail.\nSolution: The underlying problem here is that we're attempting to determine how long something that lasts a non-deterministic amount of time will take. It's just not possible to know this in advance. The most sensible thing to do is not to use timeouts. Or rather, do use them, but set them generously and use them in conjunction with a notification from the UI under test that actions have finished so that the test can continue as soon as possible.\n\nFor example, it's not hard to change the production code to set a flag on the global Javascript \"window\" object when an XmlHttpRequest returns, and that could form the basis of a simple latch. Rather than polling the UI, you can then just wait for the flag to be set. Alternatively, if your UI gives an unambiguous \"I'm done\" signal, poll for that. Frameworks such as Selenium RC and WebDriver provide helper classes that make this significantly easier.\n\nProblem: Timeouts are too long\nExample: Waiting for a page to load by polling for a piece of text, only to have the server throw an exception and give a 500 or 404 error and for the text to never appear.\nSymptom: Your tests keep timing out, probably taking your Continuous Build with them.\nSolution: Don't just poll for your desired end-condition, also think of polling for well-known error conditions. Fail the test with an informative error message when you see the error condition. WebDriver's SlowLoadableComponent has an \"isError\" method for exactly this reason. You can push the additional checks into a normal Wait for Selenium RC too.\n\nThe underlying message: When your tests are flaky, do some root cause analysis to understand why they're flaky. It's very seldom because you're uncovered a bug in the test framework. In order for this sort of analysis and test-stability improvement work to be done effectively, you may well need support and help from your team. If you're working on your own, or in a small team, this may not be too hard. On a large project, it may be harder. I've had some success when a person or two is set aside from delivering functionality to work on making the tests more stable. The short-term pain of not having that extra pair of hands focusing on writing production code is more than made up for by the long-term benefit of a stable and effective suite of end-to-end tests that only fail when there's a real issue to be addressed.", "by Mi\u0161ko Hevery & Jeremie Lenfant-engelmann\n\nDid you notice that there are a lot of JavaScript testing frameworks out there? Why has the JavaScript community not consolidated on a single JavaScript framework the way Java has on JUnit. My feeling is that all of these frameworks are good at something but none solve the complete package. Here is what I want out of JavaScript unit-test framework:\n\nI want to edit my JavaScript code in my favorite IDE, and when I hit Ctrl-S, I want all of the tests to execute across all browsers and return the results immediately.\n\nI don't know of any JavaScript framework out there which will let me do what I want. In order to achieve my goal I need a JavaScript framework with these three features:\n\nCommand Line Control\n\nMost JavaScript test runners consist of JavaScript application which runs completely in the browser. What this means in practice is that I have to go to the browser and refresh the page to get my results. But browsers need an HTML file to display, which means that I have to write HTML file which loads all of my production code and my tests before I can run my tests. Now since browsers are sandboxes, the JavaScript tests runner can only report the result of the test run inside the browser window for human consumption only. This implies that 1) I cannot trigger running of the tests by hitting Ctrl-S in my IDE, I have to Alt-tab to Browser, hit refresh and Alt-tab back to the IDE and 2) I cannot display my test result in my IDE, the results are in the browser in human readable form only.\n\nOn my continuous build machine I need to be able to run the same tests and somehow get the failures out of the browser and on to the status page. Most JavaScript test runners have a very poor story here, which makes integrating them into a continuous build very difficult.\n\nWhat we need, is the ability to control test execution from command line so that I can trigger it from my IDE, or my continuous build machine. And I need test failures to be reported on the command line (not inside the browser where they are unreachable) so that I can display them in IDE or in continuous build status page.\n\nParallel Execution\n\nSince most JavaScript test runners run fully in the browser I can only run my test on one browser at a time during my development process. In practice this means that you don't find out about failures in other browser until you have checked in the code to your continuous build machine (if you were able to set it up) and your code executes on all browsers. By that point you have completely forgotten about what you have written and debugging becomes a pain. When I run my test I want to run them on all browser platforms in parallel.\n\nInstant Feedback in IDE\n\nAfter I hit Ctrl-S on my IDE, my patience for test results is about two seconds before I start to get annoyed. What this means in practice is that you can not wait until the browser launches and runs the tests. The browser needs to be already running. Hitting refresh on your browser manually is very expensive since the browser needs to reload all of the JavaScript code an re-parse it. If you have one HTML file for each TestCase and you have hundred of these TestCases, The browser may be busy for several minutes until it reloads and re-parses the same exact production code once for each TestCase. There is no way you can fit that into the patience of average developer after hitting Ctrl-S.\n\nIntroducing JsTestDriver\n\nJeremie Lenfant-engelmann and I have set out to build a JavaScript test runner which solves exactly these issues so that Ctrl-S causes all of my JavaScript tests to execute in under a second on all browsers. Here is how Jeremie has made this seemingly impossible dream a reality. On startup JsTestDriver captures any number of browsers from any number of platforms and turns them into slaves. As slave the browser has your production code loaded along with all of your test code. As you edit your code and hit Ctrl-S the JsTestDriver reloads only the files which you have modified into the captured browsers slaves, this greatly reduces the amount of network traffic and amount of JavaScript re-parsing which the browser has to do and therefore greatly improves the test execution time. The JsTestDriver than runs all of your test in parallel on all captured browsers. Because JavaScript APIs are non-blocking it is almost impossible for your tests to run slow, since there is nothing to block on, no network traffic and no re-parsing of the JavaScript code. As a result JsTestDriver can easily run hundreds of TestCases per second. Once the tests execute the results are sent over the network to the command which executed the tests either on the command line ready to be show in you IDE or in your continuous build.\n\nDemo", "By Julian HartyPart 1 and Part 2 of this series provided how-tos and usefulness tips for creating acceptance tests for Web apps. This final post reflects on some of the broader topics for our acceptance tests.Aims and drivers of our testsIn my experience and that of my colleagues, there are drivers and aims for acceptance tests. They should act as \u2018safety rails\u2019, by analogy similar to crash barriers at the sides of roads, that keep us from straying too far from the right direction. Our tests need to ensure development doesn\u2019t break essential functionality. The tests must also provide early warning, preferably minutes after relevant changes have been made to the code.My advice for developing acceptance tests for Web applications: start simple, keep them simple, and find ways to build and establish trust in your automation code. One of the maxims I use when assessing the value of a test is to think of ways to fool my test into giving erroneous results. Then I decide whether the test is good enough or whether we need to add safeguards to the test code to make it harder to fool. I\u2019m pragmatic and realise that all my tests are imperfect; I prefer to make tests \u2018good enough\u2019 to be useful where essential preconditions are embedded into the test. Preconditions should include checking for things that invalidate assumptions for that test (for example, the logged-in account is assumed to have administrative rights) and checking for the appropriate system state (for example, to confirm the user is starting from the correct homepage and has several items in the shopping basket).The value of the tests, and their ability to act as safety rails, is directly related to how often failing tests are a \"false positive.\" Too many false positives, and a team loses trust in their acceptance tests entirely.Acceptance tests aren\u2019t a \u2018silver bullet.\u2019 They don\u2019t solve all our problems or provide complete confidence in the system being tested (real life usage generates plenty of humbling experiences). They should be backed up by comprehensive automated unit tests and tests for quality attributes such as performance and security. Typically, unit tests should comprise 70% of our functional tests, integration tests 20%, and acceptance tests the remaining 10%.We need to be able to justify the benefits of the automated tests and understand both the return on investment (ROI) and Opportunity Cost \u2013 the time we spend on creating the automated tests is not available to do other things, so we need to ask whether we could spend our time better. Here, the intent is to consider the effects and costs rather than provide detailed calculations; I typically spend a few minutes thinking about these factors as I\u2019m deciding whether to create or modify an automated test. As code spends the vast majority of time in maintenance mode, living on for a long time after active work has ceased, I recommend assessing most costs and benefits over the life of the software. However, opportunity cost must be considered within the period I\u2019m actively working on the project, as that\u2019s all the time I have available.Test automation challengesUnlike testing of traditional web sites, where the contents tend not to change once they have been loaded, tests for web applications need to cope with highly dynamic contents that may change several times a second, sometimes in hard-to-predict ways, caused by factors outside our control.As web applications are highly dynamic, the tests need to detect relevant changes,  wait until the desired behaviour has occurred, and interrogate the application state before the system state changes again. There is a window of opportunity for each test where the system is in an appropriate state to query. The changes can be triggered by many sources, including input, such as a test script clicking a button; clock based, such as a calendar reminder is displayed for 1 minute; and server initiated changes, such as when a new chat message is received.The tests can simply poll the application, trying to detect relevant changes or  timeouts. If the test only looks for expected behaviour, it might spend a long time waiting in the event of problems. We can improve the speed and reliability of the tests by checking for problems, such as error messages.Browser-based UI tests are relatively heavy-weight, particularly if each test has to start from a clean state, such as the login screen. Individual tests can take seconds to execute. While this is much faster than a human could execute a test, it\u2019s much slower than a unit test (which takes milliseconds). There is a trade-off between optimizing tests by reducing the preliminary steps (such as bypassing the need to log in by using an authentication cookie) and maintaining the independence of the tests \u2013 the system or the browser may be affected by earlier tests. Fast tests make for happier developers, unless the test results prove to be erroneous.As with other software, automated tests need ongoing nurturing to retain their utility, especially when the application code is changed. If each test contains information on how to obtain information, such as an xpath expression to get the count of unread email, then a change to the UI can affect many tests and require each of those tests to be changed and retested. By applying good software design practices, we can encapsulate the \u2018how\u2019 from the rest of our tests. That way, if the application changes, we only need to change how we get the email count in one piece of code, instead of having to change it in every piece of code.Practical testsLots of bugs are discovered by means other than automated testing \u2013 they might be reported by users, for example. Once these bugs are fixed, the fixes must be tested. The tests must establish whether the problem has been fixed and, where practical, show that the root cause has been addressed. Since we want to make sure the bug doesn\u2019t resurface unnoticed in future releases, having automated tests for the bug seems sensible. Create the acceptance tests first, and make sure they expose the problem; then fix the bug and run the tests again to ensure the fix works. Antony Marcano is one of the pioneers of acceptance tests for bugs.Although this article focuses on acceptance tests, I\u2019d like to encourage you to consider creating smaller tests when practical. Smaller tests are more focused, run significantly faster, and are more likely to be run sooner and more often. We sweep through our acceptance tests from time to time and replace as many as we can with small or medium tests. The remaining acceptance tests are more likely to be maintained because we know they\u2019re essential, and the overall execution time is reduced \u2013 keeping everyone happy!Further informationA useful tutorial on xpathhttp://www.zvon.org/xxl/XPathTutorial/General/examples.htmlGoogle Test Automation Conference (GTAC) 2008: The value of  small testshttp://www.youtube.com/watch?v=MpG2i_6nkUgGTAC 2008: Taming the Beast - How to Test an AJAX Applicationhttp://www.youtube.com/watch?v=5jjrTBFZWgkPart 1 of this article contains an additional long list of excellent resources.", "By Julian Harty Part 1 of this series provided practical how-tos to create acceptance tests. Read on to learn how to make your tests more useful. Increasing the velocity Once we have some automated acceptance tests, they must be run, without delay, as often as appropriate to answer the concerns of the team. We may want to run a subset of the tests after each change of the code. The process of running tests can be automated and integrated with a source control system that continuously builds the code and runs various automated tests. If the acceptance tests are sufficiently fast and can run unattended, they should be included in the tests run by the continuous build system. One challenge for our acceptance tests at Google is to enable the web browser to run without appearing on screen; our machines don\u2019t typically have a physical or logical GUI. Utilities such as vnc and xvfb can host the web browser and enable us to run our acceptance tests. A useful guide on test automation is the book Pragmatic Project Automation  by Mike Clark. Fluid writing of test automation code smooths over obstacles, coping with the interface twixt web application and your code. When the application has been designed with testing in mind, hooks exist; keyboard shortcuts are proffered; and debug data is available for the asking. Hooks include IDs on key elements such as the search field, enabling tests to identify the correct element quickly, unambiguously, and correctly even as the layout of the UI changes. Increasing variety and fidelityVarying the tests Tests that repeat exactly the same steps using the same parameters tread a well-worn path through the application and may side-step some nearby bugs which we could find by changing a couple of parameters in the tests. Ways to change the tests include using external data sources and using random values for number of repetitions, sleeps, number of items to order, etc. You need to be able to distinguish between tests that fail because they are flaky and those that report valid failures in the software being tested, so make sure the tests record the parameters they used in sufficient detail to enable the test to be re-run consistently and predictably. Using a variety of web browsers Browsers differ between one provider and another and between versions. Your application may be trouble-free in one browser, yet entirely unusable in another. Make sure your automated tests execute in each of the major browsers used by your users; our list typically includes Internet Explorer, Firefox, Safari, Opera, and Chrome. Tools such as Selenium RC (http://seleniumhq.org/) and WebDriver (http://code.google.com/p/webdriver/) support most of the browsers, and if your tests are designed to run in parallel, you may be able to take advantage of parallel test execution frameworks such as Selenium Grid. Emulating mobile devices Many web applications are now used on mobile phones such as the iPhone or G1. While there are some early versions of WebDriver for these devices, you may find emulating these devices in a desktop browser is sufficient to give you the confidence you need. Firefox\u2019s excellent extensions and profiles make such testing easy to implement. Safari\u2019s development tools can be used to specify the parameters you need, such as which device to emulate. Here\u2019s an example of how to configure Firefox in WebDriver to emulate a version 1.1 iPhone. private static final String IPHONE_USER_AGENT_V1_1 =\u00a0\u00a0\u00a0\u00a0\"Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en) AppleWebKit/420.1 \"\u00a0\u00a0\u00a0\u00a0+ \"(KHTML; like Gecko) Version/3.0 Mobile/3B48b Safari/419.3\";\u00a0\u00a0/**\u00a0\u00a0\u00a0* Returns the WebDriver instance with settings to emulate\u00a0\u00a0\u00a0* an iPhone V1.1\u00a0\u00a0\u00a0*/\u00a0\u00a0public static WebDriver createWebDriverForIPhoneV1_1() {\u00a0\u00a0\u00a0\u00a0final String emptyString = \"\";\u00a0\u00a0\u00a0\u00a0FirefoxProfile profile = new FirefoxProfile();\u00a0\u00a0\u00a0\u00a0// Blank out headers that would otherwise confuse the web server.\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.appversion.override\", \"\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.description.override\", \"\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.platform.override\", \"\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.vendor.override\",\"\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.vendorsub.override\",\"\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\"general.appname.override\", \"iPhone\");\u00a0\u00a0\u00a0\u00a0profile.setPreference(\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"general.useragent.override\", IPHONE_USER_AGENT_V1_1);\u00a0\u00a0\u00a0\u00a0WebDriver webDriver = new FirefoxDriver(profile);\u00a0\u00a0\u00a0\u00a0return webDriver;\u00a0\u00a0} The user-agent string can be found online in many cases or captured from a tame web server that records the HTTP headers. I use http://www.pycopia.net/webtools/headers, which even emails the values to me in a format I can easily adapt to use in my test code. Robust tests Robust tests can continue to operate correctly even when things change in the application being tested or in the environment. Web applications use HTML, so try to add IDs and CSS classes to relevant elements of the application. Although these additions potentially increase the size of the page, they enable easier and more consistent identification, navigation, and selection of the user interface. Try to avoid brittle identifiers, such as xpath expressions that rely on positional data. For example, /div[3]/div[1] becomes unreliable as soon as any of the positions change \u2013 and problems may be hard to identify unless the change is easy to identify. Add guard conditions that assert your assumptions are still accurate. Design the tests to fail if any of the assumptions prove false. If possible, make the tests fail at compile time to provide the earliest possible feedback. Try to only make positive assertions. For example, if you expect an action to cause an item to be added to a list, assert that after the action the list contains the expected value, not that the list has changed size (because other functionality may affect the size). Also, if it's not something your test is concerned about, don't make assertions about it. Informative tests Help your tests to help others by being informative. Use a combination of meaningful error messages and more detailed logs to help people to tell whether the tests are working as intended and, if problems occur, to figure out what\u2019s going wrong. Recording evidence Taking screenshots of the UI when a problem occurs can help to debug the issue and disambiguate between mismatches in our assumptions vs. problems in the application. It\u2019s not an exact science: screenshots are seldom recorded at exactly the same time as the interaction with the application; typically they\u2019re recorded afterwards, and the application may have changed in the interim period, no matter how short that period is. Debug traces are useful for diagnosing acute problems, and range from simple debug statements like \u2018I made it to this point\u2019 to dumps of the entire state of values returned from the application by our automation tool. In comparison, logging is intended for longer-term tracking of behaviour which enables larger-scale thinking, such as enabling a test to be reproduced reliably over time. Good error messages should say what\u2019s expected and include the actual values being compared. Here are two examples of combinations of tests and assert messages, the second more helpful than the first: 1. Int actualResult = addTwoRandomOddNumbers(); assertTrue(\"Something wrong with calculation\", actualResult % 2 == 0); 2.    Int actualResult = addTwoRandomOddNumbers(number1, number2); assertEquals(String.format(\"Adding two odd numbers [%d] and [%d] should return an even result. Calculated result = %d\", number1, number2, actualResult) actualResult % 2 == 0); Bit-rot, the half-life of tests Vint Cerf coined the phrase bit-rot to reflect the decay of usefulness or availability of software and data stored on computers. In science, half-life is a measurement of the decay of radioactivity over time, and is the period taken for the radioactivity to reduce by 50%. Similarly, our tests are likely to suffer from bit-rot and will become less useful over time as the system and its use change. The only cure for bit-rot is prevention. Encourage the developers to adopt and own the tests. Tests for our tests? As our tests get bigger and more complex, let\u2019s add unit tests to help ensure our acceptance tests behave as expected. Mock objects are one practical way to reliably automate the tests, with several good and free frameworks available for common programming languages. I suggest you create unit tests for more involved support functions and for \u2018driver\u2019 code, rather than for the tests themselves. Peripheral topics If you think creating automated tests for a web application is hard, try using the web site with accessibility software such as a screen reader to learn just how inaccessible some of our web applications are! Screen readers, like automated tests, need ways to interrogate, interact with, and interpret the contents of web applications. In general, increasing the accessibility of a site can improve testability and vice-versa. So while you\u2019re working hard with the team to improve the testability, try to use the site with a screen reader. Here's one example: Fire Vox, a screen reader for Firefox. http://firevox.clcworld.net/about.html The third and final post of this series will reflect on the aims and challenges of acceptance tests.", "By Julian HartyAutomated tests are often touted as a solution for software testing, and effective automated tests certainly have their place and can deliver vital confidence in the software being tested. However, many tests fail to deliver value, either now or in the future, and there are plenty of projects whose automated tests are broken, forlorn, and unloved \u2013 cluttering up projects and wasting time and resources.How to create acceptance testsTests need to do something useful to survive. Automated tests should help the team to make the next move by providing justified confidence a bug has been fixed, confirming refactored code still works as intended, or demonstrating that new features have been successfully implemented. (See Alistair Cockburn\u2019s discussion, referenced in the \u201cMore Information\u201d section below, on intermediate work products \u2013 do they remind? inform? or inspire?) There should be sufficient tests \u2013 neither more nor less: more increase the support burden, fewer leave us open to unpleasant surprises in production.Acceptance tests must meet the needs of several groups, including the users and the developers. Long-lived tests must be written in the language of each group, using terms users will recognize and a programming language and style in which the developers are competent.We create tests by modelling the purpose of a test from the user\u2019s perspective: send a message, order a book, etc. Each test is decomposed into individual actions: to send a message, a user must be logged in, select the compose message icon, specify one or more recipients, type a minimum of either a subject or a message, then select Send. From this list of actions, create a skeleton in the programming language of choice and create a method name that reflects each action. Show these to both the users and programmers and ask them to tell you what they think each step represents. Now is a great time to refine the names and decide which methods are appropriate: before you\u2019ve invested too much time in the work. If you wait until later, your natural protective instincts will make it harder for you to accept good suggestions and make useful changes.For each method, we need to work out how to implement it in code. How could an automated test select the compose message icon? Do alternative ways exist? An understanding of HTML, CSS, and JavaScript will help you if you plan to use browser automation tools. All the visible elements of a web application are reflected in the Document Object Model (DOM) in HTML, and they can be addressed in various ways: the directions from the root of the document to the element using xpath; unique identifiers; or characteristics possessed by the elements, such as class names, attributes, or link text. Some examples of these addressing options are shown in the Navigation Options illustration below. (Notes: navigation using xpath is much slower than using IDs; and IDs should be unique.)Some actions can be initiated using JavaScript running in the browser. For devices such as the iPhone, changes in orientation when the phone is rotated are triggered this way (see Handling Orientation Events in the Safari Reference Library).Typically, automated web application tests use JavaScript, either directly or indirectly, to interact with the web application being tested.Utilities such as recording tools can help reduce the effort required to discover how to interact with the web application. The open-source test automation tool Selenium (http://seleniumhq.org/) includes a simple IDE record and playback tool that runs in the Firefox browser. Recorded scripts can help bootstrap your automated tests. However, don\u2019t be tempted to consider the recorded scripts as automated tests: they\u2019re unlikely to be useful for long. Instead, plan to design and implement your test code properly, using good software design techniques. Read on to learn how to use the PageObject design pattern to design your test code.Two of the tools I find most useful are Firebug (http://getfirebug.com/), a Swiss Army knife for the Web Browser, and Wireshark (http://www.wireshark.org/), a network protocol analysis tool with a distinguished pedigree. Firebug is extremely useful when learning how to interact with a web application or debug mysterious problems with your tests when they seem to be misbehaving. I encourage you to persist when learning to use these tools \u2013 it took me a while to get used to their foibles, but I wouldn\u2019t be without either of them these days.Homogenous languages and toolsSeveral years of experience across multiple project teams have taught us that the tests are more likely to survive when they\u2019re familiar and close to the developers. Use their programming language, put them in their codebase, use their test automation framework (and even their operating system). We need to reduce the effort of maintaining the tests to a minimum. Get the developers to review the automated tests (whether they write them or you do) and actively involve them when designing and implementing the tests.Typically, our acceptance tests use the xUnit framework; for example, JUnit for Java projects (see http://www.junit.org/). A good source of inspiration for creating effective tests is Gerard Meszaros\u2019 work (see http://www.xunitpatterns.com).Effective test designsBy using effective test designs, we can make tests easier to implement and maintain. The initial investment is minor compared to the benefits. One of my favourite designs is called Page Objects (see PageObjects on the Google Code site). A PageObject represents part or all of a page in a web application \u2013 something a user would interact with. A PageObject provides services to your test automation scripts and encapsulates the nitty-gritty details of how these services are performed. By encapsulating the nitty-gritty stuff, many changes to the web application, such as the reordering or renaming of elements, can be reflected in one place in your tests. A well-designed PageObject separates the \u2018what\u2019 from the \u2018how\u2019.Another effective test design is based on three simple words: \u2018given\u2019, \u2018when\u2019, and \u2018then\u2019. As a trio they reflect the essential elements of many tests:  given various preconditions and expectations, when such-and-such happens, then I expect a certain result.// Given I have a valid user account and am at the login page,// When I enter the account details and select the Enter button,// Then I expect the inbox to be displayed with the most recent email selected.The previous code consists of three programming comments that are easy for users to read. The actual programming code is entered immediately below each comment. Programming concepts such as literate programming are intended to make the code almost as readable as the textual comments.Isolate things that change from those that don\u2019t. For example, separate user account data from your test code. The separation makes changes easier, faster, and safer to implement, compared to making updates in the code for each test.Gaining SkillsWriting automated tests may be easy for some of you. In my case, I started with some simple example tests and tweaked them to suit my needs. I received boosts from working with more experienced practitioners who were able to correct my course and educate me in how to use various tools effectively. I recommend pairing with one of the developers of the software to be tested when you face a new testing requirement. Their intimate knowledge of the code and your understanding of the tests can form a potent combination. For instance, by working with one of the developers on a recent project, we were able to implement bi-directional injection of JSON messages and capture the responses from the server to test a key interaction between the server and client that was causing problems in production.I encourage you to try out examples, tweak them, experiment, and plunge in to writing your first automated tests. Learn about AJAX \u2013 it underpins the web applications. And learn from more experienced practitioners \u2013 I\u2019ve added some links at the end of the article to some of the people I respect who write great acceptance tests, including Antony Marcano and Alan Richardson.Part 2 of this series helps you create more specialized tests (for example, to emulate mobile web browsers) and gives advice on how to increase the utility and effectiveness of your tests.Further InformationIntermediate work products\u2018The intermediate work products have only one real purpose in life: \u2018\u2018to help the team make their next move\u2019\u2019.\u2019 \u2018An intermediate work product might be measured for \u2018\u2018sufficiency\u201d \u2014 was it sufficient to remind, inform or inspire? Any amount of effort or detail beyond sufficiency is extraneous to the purpose of the team and the purpose of the work product.\u2019  Cooperative game manifesto for software development (Alistair Cockburn)Cooperative game manifesto for software development at http://alistair.cockburn.us.JUnit infoJUnit in Action, available from Manning Publications Co. (2nd edition, early access or  1st edition)JUnit Recipes, by J. B. Rainsberger with Scott Stirling, available from Manning Publications Co.Firebug infoIntroduction to Firebug on Estelle Weyl\u2019s blog, \"CSS, JavaScript and XHTML Explained\"Firebug tutorials in the Firebug Archive at Michael Sync's blogFun with Firebug Tutorial on the Google Code siteWebDriver infowebdriver on the Google Code siteAJAX resourcesBulletproof Ajax\u2014An incredibly good book on how to write good AJAX code. It starts with the basics and builds reliably and clearly from good foundations. The DOM manipulation code is relevant for implementing your acceptance tests in tools such as WebDriver.Building a web site with Ajax \u2014Again, a book that starts simple and builds a simple application step by step.Acceptance tests are more A+S than T+G (Antony Marcano, in his blog at testingReflections.com)A+S => Activities + SpecificT+G => Tasks + GeneralAlan Richardson: any and everything. For example, see:A generalised model for User Acceptance Testing andA little abstraction when testing software with Selenium-RC and Java, both at the  Evil Tester blog", "As you may have noticed, we've taken a short hiatus from posting. But all is not lost...we have been building a group of robots that will automatically write witty testing articles for us. We are still tweaking our algorithms for verbosity, humor, and technical detail. Once we are done, the posting will continue. In the mean time, we are sending one of our\u00a0cyborgs\u00a0to do a keynote at\u00a0Star East 2009.\u00a0Here's the topic of the keynote if you are interested...Testing, once a marginalized function at Google, is now an integral part of Google\u2019s innovation machine. Patrick Copeland describes how this vital transformation took place. As he was analyzing how to be more efficient and better align his testing team with the needs of the company, Patrick realized they had to move beyond \u201cjust testing\u201d and become a force for change. His approach was based on five powerful principles: (1) Building feature factories rather than products, (2) Embedding testing at the grass roots level, (3) Solving problems of scale with technology, (4) Automating at the right level of abstraction, (5) Only doing what the team can do well. Learn how Google test teams used these principles to shift from a \u201cservice group\u201d composed predominantly of exploratory testers to an \u201cengineering group\u201d with technical skills. Their focus became \u201cdelivering innovation\u201d rather than \u201ctesting product.\u201d Learn how Patrick led a cultural shift where product teams saw testing and continuous improvement, not as alien concepts driven by someone else, but as a tool for them to meet their own goals of delivering features quickly and with fewer problems. Discover how you can incorporate the lessons of Google to make your test team a vital force for change.", "by H\u00e5vard Rast BlokWhile working with a search quality development team, I was asked to collect information from their result pages across all the supported languages. The aim was to quickly get an overview, and then manually look through them for irregularities. One option would have been to grab the pages using tools like Selenium or WebDriver. However, this would have been complex and expensive. Instead, I opted for a much simpler solution: Display each language variation in a separate IFrame within the same HTML page.An example of how this looks, can be seen here, where the Google Search page is shown in 43 different languages. There are also some links for other Google sites, or you can type in your own link, and the query attribute for language, \"hl=\", will be appended at the end.(Warning: Do not try this with YouTube, as 43 Flash windows on the same pages will crash your browser. Also, Firefox 2 is known to be slow, while Firefox 3 works fine.)The JavaScript CodeCreating the IFrames is easy using JavaScript, as can be seen in the example below. I assume that the array of languages to iterate over is retrieved by the function getLanguages(). Then a simple loop uses document.write(...) to dynamically add the IFrames. It is worth mentioning that this method seemed to be the best way of dynamically creating them; using the document.createElement(...) resulted in some complex race condition issues when adding the IFrames and their content at the same time.var languages = getLanguages();for (var lang, i = 0; lang = languages[i]; i++) {\u00a0\u00a0document.write('<hr><a name=\"' + lang + '\"/>' +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'<h2>' + lang + '</h2><center>' +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'<iframe src=\"' + url + lang +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'\" width=\"' + queryMap['width'] +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'\" height=\"' + queryMap['height'] +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'\"></iframe>' +\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</center><br/><br/>');}The rest of the source code, can be seen in this example. Nothing else is needed to get the overview.ConclusionThe example in this article shows that a very simple and inexpensive solution can be useful for exploratory testing of web pages; especially when quickly looking over the same pages in multiple languages. The small amount of code required, makes it easy to customize for any project or page, and the fact that the requests are done dynamically, gives a view which is always up to date.Of course, this type of overview lends itself best to very simple stateless pages, which do not require complex navigation. It would for example be more difficult to get the same list of IFrames for Gmail, or other complex applications. Also, as the IFrames are loaded dynamically, no history is kept, so tracking when a potential bug was introduced in the page under test might prove more tedious.Furthermore, it should be noted that the overview only simplifies a manual process of looking at the pages. In some situations, this might be very beneficial, and enough for the developer, while in other projects more automated tests might be designed. E.g., it could be difficult to automate tests for aesthetic issues, but easy to spot them manually, while it may prove more beneficial to automate checks for English terms in other languages.Finally, a word on the issue of translations and language skills. The overview in this example, quickly highlights issues like incorrect line wrapping, missing strings, etc. in all variations of the page. Also, it was easy to spot strings not already translated in some of the languages, like Japanese, and in fact I reported a bug against the Search front page for this. However, for other issues, more language specific skills are necessary to spot and file bugs: E.g. should the Arabic page show Eastern or Western Arabic numerals? And have the Danes picked the English term for \"Blogs\", while the Norwegians and Swedish prefer a localized term? I don't know.", "A Partial Mock is a mock that uses some behavior from a real object and some from a mock object. It is useful when you need bits of both. One way to implement this is often a Forwarding Object (or wrapper) which forwards calls to a delegate.For example, when writing an Olympic swimming event for ducks, you could create a simple forwarding object to be used by multiple tests:interface Duck {Point getLocation();void quack();void swimTo(Point p);}class ForwardingDuck implements Duck {private final Duck d;ForwardingDuck(Duck delegate) {this.d = delegate;}public Point getLocation() {return d.getLocation();}public void quack() {d.quack();}public void swimTo(Point p) {d.swimTo(p);}}And then create a test that uses all of the real OlympicDuck class's behavior except quacking.public void testDuckCrossesPoolAndQuacks() {final Duck mock = EasyMock.createStrictMock(Duck.class); mock.swimTo(FAR_SIDE);mock.quack(); // quack after the raceEasyMock.replay(mock);Duck duck = OlympicDuck.createInstance();Duck partialDuck = new ForwardingDuck(duck) { @Override public void quack() {mock.quack();}@Override public void swimTo(Point p) {mock.swimTo(p);super.swimTo(p);}// no need to @Override \u201cPoint getLocation()\u201d}OlympicSwimmingEvent.createEventForDucks().withDistance(ONE_LENGTH).sponsoredBy(QUACKERS_CRACKERS).addParticipant(partialDuck).doRace();MatcherAssert.assertThat(duck.getLocation(), is(FAR_SIDE));EasyMock.verify(mock);partialDuck is a complex example of a partial mock \u2013 it combines real and mock objects in three different ways: quack() calls the mock object. It verifies that the duck doesn't promote the sponsor (by quacking) until after the race. (We skip the real quack() method so that our continuous build doesn't drive us crazy.)getLocation() calls the real object. It allows us to use the OlympicDuck's location logic instead of rewriting/simulating the logic from that implementation.swimTo(point) calls both objects. It allows us to verify the call to the real duck before executing it.There is some debate about whether you should forward to the real or mock Duck by default. If you use the mock duck by default, any new calls to the mock will break the test, making them brittle. If you use the real duck, some very sensitive calls like submitToDrugTest() might get called by your test if your duck happens to win.Consider using a Partial Mock in tests when you need to leverage the implementation of the real object, but want to limit, simulate or verify method calls using the power of a mock object.Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko Hevery\n\nThere seems to be two camps in dependency-injection: (1) The constructor-injection camp and (2) the setter-injection camp. Historically the setter-injection camp come from spring, whereas constructor-injection camp are from pico-container and GUICE. But lets leave the history behind and explore the differences in the strategies.\n\nSetter-Injection\n\nThe basic-ideas is that you have a no argument-constructor which creates the object with \"reasonable-defaults\" . The user of the object can then call setters on the object to override the collaborators of the object in order to wire the object graph together or to replace the key collaborators with test-doubles.\n\nConstructor-Injection\n\nThe basic idea with constructor-injection is that the object has no defaults and instead you have a single constructor where all of the collaborators and values need to be supplied before you can instantiate the object.\n\nAt first it may seem that setter injection is preferred since you have no argument constructors which will make it easy for you to create the object in production and test. However, there is one non-obvious benefit with constructor injection, which in my opinion makes it a winner. Constructor-injection enforces the order of initialization and prevents circular dependencies. With setter-injection it is not clear in which order things need to be instantiated and when the wiring is done. In a typical application there may be hundreds of collaborators with at least as many setter calls to wire them together. It is easy to miss a few setter calls when wiring the application together. On the other hand constructor-injection automatically enforces the order and completeness of the instantiated. Furthermore, when the last object is instantiated the wiring phase of your application is completed. This further allows me to set the collaborators as final which makes the code easier to comprehend if you know a given field will not change during the lifetime of the application.\n\nLet's look at an example as to how we would instantiate a CreditCardProcessor.\nCreditCardProcessor processor = new CreditCardProcessor();\n\nGreat I have instantiated CreditCardProcessor, but is that enough? No, I somehow need to know to call, setOfflineQueue(). This information is not necessarily obvious.\nOfflineQueue queue = new OfflineQueue();\nCreditCardProcessor processor = new CreditCardProcessor();\nprocessor.setOfflineQueue(queue);\n\nOk I have instantiated the OfflineQueue and remember to set the queue as a collaborator of the processor, but am I done? No, you need to set the database to both the queue and the processor.\nDatabase db = new Database();\nOfflineQueue queue = new OfflineQueue();\nqueue.setDatabase(db);\nCreditCardProcessor processor = new CreditCardProcessor();\nprocessor.setOfflineQueue(queue);\nprocessor.setDatabase(db);\n\nBut wait, you are not done you need to set the Username, password and the URL on the database.\nDatabase db = new Database();\ndb.setUsername(\"username\");\ndb.setPassword(\"password\");\ndb.setUrl(\"jdbc:....\");\nOfflineQueue queue = new OfflineQueue();\nqueue.setDatabase(db);\nCreditCardProcessor processor = new CreditCardProcessor();\nprocessor.setOfflineQueue(queue);\nprocessor.setDatabase(db);\n\nOk, am I done now? I think so, but how do I know for sure? I know a framework will take care of it, but what if I am in a language where there is no framework, then what?\n\nOk, now let's see how much easier this will be in the constructor-injection. Lets instantiate CreditCardPrecossor.\nCreditCardProcessor processor = new CreditCardProcessor(?queue?, ?db?);\n\nNotice we are not done yet since CreditCardProcessor needs a queue and a database, so lets make those.\nDatabase db = new Database(\"username\", \"password\", \"jdbc:....\");\nOfflineQueue queue = new OfflineQueue(db);\nCreditCardProcessor processor = new CreditCardProcessor(queue, db);\n\nOk, every constructor parameter is accounted for, therefore we are done. No framework needed, to tell us that we are done. As an added bonus the code will not even compile if all of the constructor arguments are not satisfied. It is also not possible to instantiate things in the wrong order. You must instantiate Database before the OfflineQueue, since otherwise you could not make the compiler happy. I personally find the constructor-injection much easier to use and the code is much easier to read and understand.\n\nRecently, I was building a Flex application and using the Model-View-Controller. Flex XML markup requires that components must have no argument constructors, therefore I was left with setter-injection as the only way to do dependency injection. After several views I was having hard time to keep all of the pieces wired together properly, I was constantly forgetting to wire things together. This made the debugging hard since the application appeared to be wired together (as there are reasonable defaults for your collaborators) but the collaborators were of wrong instances and therefor the application was not behaving just right. To solve the issue, I was forced to abandon the Flex XML as a way to instantiate the application so that I can start using the constructor-injection and these issues went away.", "by Mi\u0161ko Hevery\n\nSome of the strongest objections I get from people is on my stance on what I call \"defensive programming\". You know all those asserts you sprinkle your code with. I have a special hate relationship against null checking. But let me explain.\n\nAt first, people wrote code, and spend a lot of time debugging. Than someone came up with the idea of asserting that some set of things should never happen. Now there are two kinds of assertions, the ones where you assert that an object will never get into on inconsistent state and the ones where you assert that objects never gets passed a incorrect value. The most common of which is the null check.\n\nThan some time later people started doing automated unit-testing, and a weird thing happened, those assertions are actually in the way of good unit testing, especially the null check on the arguments. Let me demonstrate with on example.\nclass House {\n  Door door;\n  Window window;\n  Roof roof;\n  Kitchen kitchen;\n  LivingRoom livingRoom;\n  BedRoom bedRoom;\n\n  House(Door door, Window window,\n            Roof roof, Kitchen kitchen,\n            LivingRoom livingRoom,\n            BedRoom bedRoom){\n    this.door = Assert.notNull(door);\n    this.window = Assert.notNull(window);\n    this.roof = Assert.notNull(roof);\n    this.kitchen = Assert.notNull(kitchen);\n    this.livingRoom = Assert.notNull(livingRoom);\n    this.bedRoom = Assert.notNull(bedRoom);\n  }\n\n  void secure() {\n    door.lock();\n    window.close();\n  }\n}\n\nNow let's say that i want to test the secure() method. The secure method needs door and window. Therefore my ideal would look like this.\ntestSecureHouse() {\n  Door door = new Door();\n  Window window = new Window();\n  House house = new House(door, window,\n             null, null, null, null);\n\n  house.secure();\n\n  assertTrue(door.isLocked());\n  assertTrue(window.isClosed());\n}\n\nSince the secure() method only needs to operate on door, and window, those are the only objects which I should have to create. For the rest of them I should be able to pass in null. null is a great way to tell the reader, \"these are not the objects you are looking for\". Compare the readability with this:\ntestSecureHouse() {\n  Door door = new Door();\n  Window window = new Window();\n  House house = new House(door, window,\n    new Roof(),\n    new Kitchen(),\n    new LivingRoom(),\n    new BedRoom());\n\n  house.secure();\n\n  assertTrue(door.isLocked());\n  assertTrue(window.isClosed());\n}\n\nIf the test fails here you are now sure where to look for the problem since so many objects are involved. It is not clear from the test that that many of the collaborators are not needed.\n\nHowever this test assumes that all of the collaborators have no argument constructors, which is most likely not the case. So if the Kitchen class needs dependencies in its constructor, we can only assume that the same person who put the asserts in the House also placed them in the Kitchen, LivingRoom, and BedRoom constructor as well. This means that we have to create instances of those to pass the null check, so our real test will look like this:\ntestSecureHouse() {\n  Door door = new Door();\n  Window window = new Window();\n  House house = new House(door, window,\n    new Roof(),\n    new Kitchen(new Sink(new Pipes()),\n           new Refrigerator()),\n    new LivingRoom(new Table(), new TV(), new Sofa()),\n    new BedRoom(new Bed(), new Closet()));\n\n  house.secure();\n\n  assertTrue(door.isLocked());\n  assertTrue(window.isClosed());\n}\n\nYour asserts are forcing you to create so many objects which have nothing to do with the test and only confuse the reader and make the tests hard to write. Now I know that a house with a null roof, livingRoom, kitchen and bedRoom is an inconsistent object which would be an error in production, but I can write another test of my HouseFactory class which will assert that it will never happen.\n\nNow there is a difference if the API is meant for my internal consumption or is part of an external API. For external API I will often times write tests to assert that appropriate error conditions are handled, but for the internal APIs my tests are sufficient.\n\nI am not against asserts, I often use them in my code as well, but most of my asserts check the internal state of an object not wether or not I am passing in a null value. Checking for nulls usually goes against testability, and given a choice between well tested code and untested code with asserts, there is no debate for me which one I chose.", "With all the sport drug scandals of late, it's difficult to find good role models these days. However, when your role model is a Domain Model (object model of the business entities), you don't need to cheat to be an MVP--Use Model-View-Presenter!MVP is very similar to MVC (Model-View-Controller). In MVC, the presentation logic is shared by Controller and View, as shown in the diagram below. The View is usually derived directly from visible GUI framework component, observing the Model and presenting it visually to the user. The Controller is responsible for deciding how to translate user events into Model changes. In MVP, presentation logic is taken over entirely by a Supervising Controller, also known as a Presenter.MVCMVPThe View becomes passive, delegating to the Presenter.public CongressionalHearingView() {  testimonyWidget.addModifyListener(    new ModifyListener() {      public void modifyText(ModifyEvent e) {        presenter.onModifyTestimony(); // presenter decides action to take      }});}The Presenter fetches data from the Model and updates the View.public class CongressionalHearingPresenter {  public void onModifyTestimony() {    model.parseTestimony(view.getTestimonyText()); // manipulate model  }  public void setWitness(Witness w) {    view.setTestimonyText(w.getTestimony()); // update view  }}This separation of duties allows for more modular code, and also enables easy unit testing of the Presenter and the View.public void testSetWitness() {  spyView = new SpyCongressionalHearingView();  presenter = new CongressionalHearingPresenter(spyView);  presenter.setWitness(new Witness(\u201cMark McGwire\u201d, \u201cI didn't do it\u201d));  assertEquals( \u201cI didn't do it\u201d, spyView.getTestimonyText());}Note that this makes use of a perfectly legal injection -- Dependency Injection.Remember to download this episode of Testing on the Toilet and post it in your office.", "When scientists in California tried to raise condors in captivity, they ran into a problem.  The chicks wouldn't eat from the researchers' hands; they wanted a mother condor to feed them. So the scientists got a puppet.  To the chicks, it looked like their mother's head was feeding them\u2014but inside was the same scientist's hand.\n\nConsider a contrived example based on that:\n\n\nTEST_F(BabyCondorTest, EatsCarrion) {\u00a0\u00a0FakeCondor mother;\u00a0\u00a0scoped_ptr carrion;\u00a0\u00a0BabyCondor* pchick = &chick_;\u00a0\u00a0mother.Imprint(vector(&pchick, &pchick + 1));  // just one chick\u00a0\u00a0while(!chick_.HasFood()) {\u00a0\u00a0\u00a0\u00a0mother.Eat();  // disposes of any food the mother kept for herself\u00a0\u00a0\u00a0\u00a0mother.Scavenge(carrion.reset(new FakeCarrion));  // finds new food\u00a0\u00a0\u00a0\u00a0mother.RandomlyDistributeFoodAmongYoungAndSelf();  // feeds baby or mom\u00a0\u00a0}\u00a0\u00a0chick_.Eat();\u00a0\u00a0EXPECT_TRUE(carrion->WasEaten());}\n\n\nSomething is wrong here\u2014that was a lot of setup!  The general-purpose FakeCondor replicates too much functionality from the full class.  The researchers' puppet didn't scavenge its own carrion, so why should ours? We just want to test that the baby Eats.  We condense various motherhood behaviors, such as giving food, into single method calls by extracting a role interface.  (If we couldn't change Condor, we would also write an adapter.)\n\n\nclass CondorMotherhoodRoleInterface {\u00a0public:\u00a0\u00a0virtual Carrion* GiveFood() = 0;\u00a0\u00a0virtual SomeReturnTypes* OtherMomBehaviors() = 0;};\n\n\nThen we write a single-use fake which provides only behaviors we need for this particular test.\n\n\nclass CondorFeedingPuppet: public CondorMotherhoodRoleInterface {\u00a0public:\u00a0\u00a0virtual Carrion* GiveFood() { return test_carrion_; }\u00a0\u00a0virtual SomeReturnTypes* OtherMomBehaviors() { return NULL; } \u00a0\u00a0Carrion* test_carrion_;  // public var is tolerable in a one-off object};TEST_F(BabyCondorTest, EatsCarrion) {\u00a0\u00a0CondorFeedingPuppet mother;  FakeCarrion test_carrion;\u00a0\u00a0mother.test_carrion_ = &test_carrion;\u00a0\u00a0chick_.ReceiveFood(&mother);\u00a0\u00a0chick_.Eat();\u00a0\u00a0EXPECT_TRUE(test_carrion.WasEaten());}\n\n\nThis highly-focused fake is easy and quick to write, and makes the test much simpler and more readable.   Don't overestimate the complexity of your dependencies!  Often a very simple fake is the best.\n\nRemember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko Hevery\n\nA great question from the reader...\n\nThe only thing that does not fully convince me in your articles is usage of Guice. I'm currently unable to see clearly its advantages over plain factories, crafted by hand. Do you recommend using of Guice in every single case? I strongly suspect, there are cases, where hand-crafted factories make a better fit than Guice. Could you comment on that (possibly at your website)?\n\nI think this is multi-part question:\n\nShould I be using dependency-injection?\n\nShould I be using manual dependency-injection or automatic dependency-injection framework?\n\nWhich automatic dependency-injection framework should I use?\n\n\nShould I be using dependency-injection?\n\nThe answer to this question should be a resounding yes! We covered this many times how to think about the new-operator, singletons are liars, and of course the talk on dependency-injection.\n\nDependency injection is simply a good idea and it helps with: testability; maintenance; and bringing new people up to speed on new code-base. Dependency-injection helps you with writing good software whether it is a small project of one or large project with a team of collaborators.\n\nShould I be using manual dependency-injection or automatic dependency-injection framework?\n\nWhether or not to use a framework for dependency injection depends a lot on your preferences and the size of your project. You don't get any additional magical powers by using a framework. I personally like to use frameworks on medium to large projects but stick to manual DI with small projects. Here are some arguments both ways to help you make a decision.\n\nIn favor of manual DI:\n\nSimple: Nothing to learn, no dependencies.\n\nNo reflection magic: In IDE it is easy to find out who calls the constructors.\n\nEven developers who do not understand DI can follow and contribute to projects.\n\n\nIn favor of automatic DI framework:\n\nConsistency: On a large team a lot can be said in doing things in consistent manner. Frameworks help a lot here.\n\nDeclarative: The wiring, scopes and rules of instantiation are declarative. This makes it easier to understand how the application is wired together and easier to change.\n\nLess typing: No need to create the factory classes by hand.\n\nHelps with end-to-end tests: For end-to-end tests we often need to replace key components of the application with fake implementations, an automated framework can be of great help.\n\n\nWhich automatic dependency-injection framework should I use?\n\nThere are three main DI frameworks which I am aware off: GUICE, Pico Container and Spring.\n\nI work for Google, I have used GUICE extensively therefor my default recommendation will be GUICE. :-) However I am going to attempt to be objective about the differences. Keep in mind that I have not actually used the other ones on real projects.\n\nSpring was first. As a result it goes far beyond DI and has everything and kitchen sink integrated into it which is very impressive. The DI part of Spring has some differences worth pointing out. Unlike GUICE or Pico, Spring uses XML files for configuration. Both are declarative but GUICE is compiled and as a result GUICE can take advantage of compiler type safety and generics, which I think is a great plus for GUICE.\n\nHistorically, Spring started with setter injection. Pico introduced constructor injection. Today, all frameworks can do both setter and constructor injection, but the developers using these frameworks still have their preferences. GUICE and Pico strongly prefer constructor injection while Spring is in the setter injection camp. I prefer constructor injection but the reasons are better left for another post.\n\nPersonally, I think all of the three have been around for a while and have proven themselves extensively, so no matter which one you chose you will benefit greatly from your decision. All three frameworks have been heavily influenced by each other and on a macro level are very similar.\n\nYour milage may very.", "Welcome back!  We trust you all had a good holiday season and are ready for more TotTs -- Dave\n\n\nMost of us are aware that mock and stub objects can make testing easier by isolating the class under test from external dependencies.  This goes hand-in-hand with dependency injection.  Writing all these classes can be a pain though. \n\nEasyMock provides an alternative. It dynamically implements an interface which records and replays your desired behavior. Let's say you want to model an ATM interface: \n\n\npublic interface Atm { \n\u00a0\u00a0boolean enterAccount(String accountNumber); \n\u00a0\u00a0boolean enterPin(String pin); \n\u00a0\u00a0boolean enterWithdrawalAmount(int dollars); \n}\n\n\nIt is pretty easy to mock this interface.  Still, every mock has to implement all three methods, even if you only need one.  You also need a separate mock for each set of inputs.  With EasyMock, you can create  mocks as you need them, recording and replaying your expectations:\n\n\npublic void testAtmLogin() {\n\u00a0\u00a0Atm mockAtm = createMock(Atm.class);       // 1\n\u00a0\u00a0EasyMock.expect(mockAtm.enterAccount(\"MyAccount\")).andReturn(true);  // 2\n\u00a0\u00a0EasyMock.expect(mockAtm.enterPin(\"1234\")).andReturn(true);  // 3\n\u00a0\u00a0EasyMock.replay(mockAtm);         // 4\n\u00a0\u00a0Account account = new Account();\n\u00a0\u00a0account.login(mockAtm);        // 5\n\u00a0\u00a0assertTrue(account.isLoggedIn());\n\u00a0\u00a0EasyMock.verify(mockAtm);        // 6\n} \n\n\nWe tell EasyMock to create a dynamic proxy implementing Atm (1), which starts in record mode.  Then we record two method calls along with the expected results (2 and 3).  The replay() call tells EasyMock to stop recording (4).  After that, calls on the object return the set values.  If it gets a call it does not expect, it throws an Exception to fail fast.  Account now uses the mock as if it were the real thing (5).  The verify() method checks to see if the mock actually received all the calls you expect (6).  It really is that simple.  If we want to simulate failure, we can set up another test to return false from one of the method calls.\n\nEasyMock has lots more capabilities as well.  It can throw exceptions.   It also can record multiple calls to the same method returning the same or different results.  You also can create stub expectations and nice mocks so you don't have to record every expected call.  You also can create several mocks, and even nest them to test classes with complex dependencies.  Beware, though, this often creates brittle tests, and is a sign the class under test needs refactoring. \n\nBasic EasyMock only mocks interfaces, but there is an EasyMockClassExtension that mocks non-final classes when you really must.  See the EasyMock documentation at the link below for details.\n\nRemember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko Hevery\n\nShahar asks an excellent question about how to deal with frameworks which we use in our projects, but which were not written with testability in mind.\n\nHi Misko, First I would like to thank you for the \u201cGuide to Writing Testable Code\u201d, which really helped me to think about better ways to organize my code and architecture. Trying to apply the guide to the code I\u2019m working on, I came up with some difficulties. Our code is based on external frameworks and libraries. Being dependent on external frameworks makes it harder to write tests, since test setup is much more complex. It\u2019s not just a single class we\u2019re using, but rather a whole bunch of classes, base classes, definitions and configuration files. Can you provide some tips about using external libraries or frameworks, in a manner that will allow easy testing of the code?\n\n-- Thanks, Shahar\n\n\nThere are two different kind of situations you can get yourself into:\n\n Either your code calls a third-party library (such as you calling into LDAP authentication, or JDBC driver)\n\nOr a third party library calls you and forces you to implement an interface or extend a base class (such as when using servlets).\n\n\nUnless these APIs are written with testability in mind, they will hamper your ability to write tests.\n\nCalling Third-Party Libraries\n\nI always try to separate myself from third party library with a Facade and an Adapter. Facade is an interface which has a simplified view of the third-party API. Let me give you an example. Have a look at javax.naming.ldap. It is a collection of several interfaces and classes, with a complex way in which you have to call them. If your code depends on this interface you will drown in mocking hell. Now I don't know why the API is so complex, but I do know that my application only needs a fraction of these calls. I also know that many of these calls are configuration specific and outside of bootstrapping code these APIs are cluttering what I have to mock out.\n\nI start from the other end. I ask myself this question. 'What would an ideal API look like for my application?' The key here is 'my application' An application which only needs to authenticate will have a very different 'ideal API' than an application which needs to manage the LDAP. Because we are focusing on our application the resulting API is significantly simplified. It is very possible that for most applications the ideal interface may be something along these lines.\ninterface Authenticator {\n boolean authenticate(String username,\n                      String password);\n}\n\nAs you can see this interface is a lot simpler to mock and work with than the original one as a result it is a lot more testable. In essence the ideal interfaces are what separates the testable world from the legacy world.\n\nOnce we have an ideal interface all we have to do is implement the adapter which bridges our ideal interface with the actual one. This adapter may be a pain to test, but at least the pain is in a single location.\n\nThe benefit of this is that:\n\nWe can easily implement an InMemoryAuthenticator for running our application in the QA environment.\n\nIf the third-party APIs change than those changes only affect our adapter code.\n\nIf we now have to authenticate against a Kerberos or Windows registry the implementation is straight forward.\n\nWe are less likely to introduce a usage bug since calling the ideal API is simpler than calling the original API.\n\n\nPlugging into an Existing Framework\n\nLet's take servlets as an example of hard to test framework. Why are servlets hard to test?\n\nServlets require a no argument constructor which prevents us from using dependency injection. See how to think about the new operator.\n\nServlets pass around HttpServletRequest and HttpServletResponse which are very hard to instantiate or mock.\n\n\nAt a high level I use the same strategy of separating myself from the servlet APIs. I implement my actions in a separate class\nclass LoginPage {\n Authenticator authenticator;\n boolean success;\n String errorMessage;\n LoginPage(Authenticator authenticator) {\n   this.authenticator = authenticator;\n }\n\n String execute(Map<String, String> parameters,\n                String cookie) {\n   // do some work\n   success = ...;\n   errorMessage = ...;\n }\n\n String render(Writer writer) {\n   if (success)\n     return \"redirect URL\";\n   else\n     writer.write(...);\n }\n}\n\nThe code above is easy to test because:\n\nIt does not inherit from any base class.\n\nDependency injection allows us to inject mock authenticator (Unlike the no argument constructor in servlets).\n\nThe work phase is separated from the rendering phase. It is really hard to assert anything useful on the Writer but we can assert on the state of the LoginPage, such as success and errorMessage.\n\nThe input parameters to the LoginPage are very easy to instantiate. (Map<String, String>, String for cookie, or a StringWriter for the writer).\n\n\nWhat we have achieved is that all of our application logic is in the LoginPage and all of the untestable mess is in the LoginServlet which acts like an adapter. We can than test the LoginPage in depth. The LoginSevlet is not so simple, and in most cases I just don't bother testing it since there can only be wiring bug in that code. There should be no application logic in the LoginServlet since we have moved all of the application logic to LoginPage.\n\nLet's look at the adapter class:\nclass LoginServlet extends HttpServlet {\n Provider<LoginPage> loginPageProvider;\n\n // no arg constructor required by\n // Servlet Framework\n LoginServlet() {\n   this(Global.injector\n          .getProvider(LoginPage.class));\n }\n\n // Dependency injected constructor used for testing\n LoginServlet(Provider<LoginPage> loginPageProvider) {\n   this.loginPageProvider = loginPageProvider;\n }\n\n service(HttpServletRequest req,\n         HttpServletResponse resp) {\n   LoginPage page = loginPageProvider.get();\n   page.execute(req.getParameterMap(),\n        req.getCookies());\n   String redirect = page.render(resp.getWriter())\n   if (redirect != null)\n     resp.sendRedirect(redirect);\n }\n}\n\nNotice the use of two constructors. One fully dependency injected and the other no argument. If I write a test I will use the dependency injected constructor which will than allow me to mock out all of my dependencies.\n\nAlso notice that the no argument constructor is forcing me to use global state, which is very bad, but in the case of servlets I have no choice.  However, I make sure that only servlets access the global state and the rest of my application is unaware of this global variable and uses proper dependency injection techniques.\n\nBTW there are many frameworks out there which sit on top of servlets and which provide you a very testable APIs. They all achieve this by separating you from the servlet implementation and from HttpServletRequest and HttpServletResponse. For example Waffle and WebWork", "by Mi\u0161ko HeveryRecently many of you, after reading Guide to Testability, wrote to telling me there is nothing wrong with static methods. After all what can be easier to test than Math.abs()! And Math.abs() is static method! If abs() was on instance method, one would have to instantiate the object first, and that may prove to be a problem. (See how to think about the new operator, and class does real work)The basic issue with static methods is they are procedural code. I have no idea how to unit-test procedural code. Unit-testing assumes that I can instantiate a piece of my application in isolation. During the instantiation I wire the dependencies with mocks/friendlies which replace the real dependencies. With procedural programing there is nothing to \"wire\" since there are no objects, the code and data are separate.Here is another way of thinking about it. Unit-testing needs seams, seams is where we prevent the execution of normal code path and is how we achieve isolation of the class under test. seams work through polymorphism, we override/implement class/interface  and than wire the class under test differently in order to take control of the execution flow. With static methods there is nothing to override. Yes, static methods are easy to call, but if the static method calls another static method there is no way to overrider the called method dependency.Lets do a mental exercise. Suppose your application has nothing but static methods. (Yes, code like that is possible to write, it is called procedural programming.) Now imagine the call graph of that application. If you try to execute a leaf method, you will have no issue setting up its state, and asserting all of the corner cases. The reason is that a leaf method makes no further calls. As you move further away from the leaves and closer to the root main() method it will be harder and harder to set up the state in your test and harder to assert things. Many things will become impossible to assert. Your tests will get progressively larger. Once you reach the main() method you  no longer have a unit-test (as your unit is the whole application) you now have a scenario test. Imagine that the application you are trying to test is a word processor. There is not much you can assert from the main method. We have already covered that global state is bad and how it makes your application hard to understand. If your application has no global state than all of the input for your static method must come from its arguments. Chances are very good that you can move the method as an instance method to one of the method's arguments. (As in method(a,b) becomes a.method(b).) Once you move it you realized that that is where the method should have been to begin with. The use of static methods becomes even worse problem when the static methods start accessing the global state of the application. What about methods which take no arguments? Well, either methodX() returns a constant in which case there is nothing to test; it accesses global state, which is bad; or it is a factory.Sometimes a static methods is a factory for other objects. This further exuberates the testing problem. In tests we rely on the fact that we can wire objects differently replacing important dependencies with mocks. Once a new operator is called we can not override the method with a sub-class. A caller of such a static factory is permanently bound to the concrete classes which the static factory method produced. In other words the damage of the static method is far beyond the static method itself. Butting object graph wiring and construction code into static method is extra bad, since object graph wiring is how we isolate things for testing.\"So leaf methods are ok to be static but other methods should not be?\" I like to go a step further and simply say, static methods are not OK. The issue is that a methods starts off being a leaf and over time more and more code is added to them and they lose their positions as a leafs. It is way to easy to turn a leaf method into none-leaf method, the other way around is not so easy. Therefore a static leaf method is a slippery slope which is waiting to grow and become a problem. Static methods are procedural! In OO language stick to OO. And as far as Math.abs(-5) goes, I think Java got it wrong. I really want to write -5.abs(). Ruby got that one right.", "Posted by Lydia Ash, GTAC Conference ChairThe Google Test Automation Conference 2008 was a smashing success, and in no small part due to all of our presenters and participants. A wonderful thank you from all of us at Google to everyone that participated! The tone of the conference really struck me as I watched how everyone came together around various topics. I don't think we would have had trouble keeping the conversations going if there was an entire day dedicated to the moderated discussions.We were able to get the slide decks from our presenters. These are listed below with their video link.We will be evaluating what location the next GTAC should be held, and your comments will help shape the next conference. Building on the successes in the past, next year should be even better! Stay tuned for a Save the Date notice sometime in the spring.Opening Remarks - Lydia AshVideo: http://www.youtube.com/watch?v=l5QmHXcNk4gThe Future of Testing - James A. WhittakerVideo coming soon...Advances in Automated Software Testing Technologies - Elfriede Dustin and Marcus BorchVideo: http://www.youtube.com/watch?v=HEpSdSyU03IBoosting Your Testing Productivity with Groovy - Andres AlmirayVideo: http://www.youtube.com/watch?v=UvWTfVCWKJYSlides: http://www.slideshare.net/aalmiray/gtac-boosting-your-testing-productivity-with-groovy/Taming the Beast: How to Test an AJAX Application - Markus Clermont and John ThomasVideo: http://www.youtube.com/watch?v=5jjrTBFZWgkSlides: http://docs.google.com/Presentation?id=dczwht9g_62gccsc9ggThe New Genomics: Software Development at Petabyte Scale - Matt WoodVideo: http://www.youtube.com/watch?v=A64WKH9gNI8Slides part 1: http://docs.google.com/Presentation?id=dczwht9g_3318qqfb6f5Slides part 2: http://docs.google.com/Presentation?id=dczwht9g_393d7zg4xcmUsing Cloud Computing to Automate Full-Scale System Tests - Marc-Elian B\u00e9gin and Charles LoomisVideo: http://www.youtube.com/watch?v=atyq-41GnjcSlides: http://docs.google.com/Presentation?id=dczwht9g_251gcv8cbfvPracticing Testability in the Real World - Vishal ChowdharyVideo: http://www.youtube.com/watch?v=hL829wNaF78Slides: http://docs.google.com/Presentation?id=dczwht9g_0hgd2w5rzContext-Driven Test Automation: How to Build the System you Really Need - Pete SchneiderVideo: http://www.youtube.com/watch?v=N9sm_zcpUEwSlides: http://docs.google.com/Presentation?id=dczwht9g_236ccxj32fdAutomated Model-Based Testing of Web Applications - Atif M. Memon and Oluwaseun AkinmadeVideo: http://www.youtube.com/watch?v=6LdsIVvxISUSlides: http://www.cs.umd.edu/~atif/GTAC08/The Value of Small Tests - Christopher SemtursVideo: http://www.youtube.com/watch?v=MpG2i_6nkUgSlides: http://docs.google.com/Presentation?id=dckk962d_332cxtcsmhgJInjector: A Coverage and End-To-End Testing Framework for J2ME and RIM - Julian Harty, Olivier Gaillard, and Michele SamaVideo: http://www.youtube.com/watch?v=B2v5jQ9NLVgSlides: http://docs.google.com/Presentation?id=dczwht9g_82d7w8bqd9Atom Publishing Protocol: Testing your Server Implementation - David CalaveraVideo: http://www.youtube.com/watch?v=uRmWTfT91uQSlides: http://thinkincode.net/gtac_atomPub_testing_your_server_implementation.pdfSimple Tools to Fight the Bigger Quality Battle: Continuous Integration Using Batch Files and Task Scheduler - Komal Joshi and Patrick MartinVideo: http://www.youtube.com/watch?v=wgP7ejMBCCUSlides: http://docs.google.com/Presentation?id=dczwht9g_141czcvc7md", "Sorry folks, this was a duplicate post. Please see the original here:\u00a0http://googletesting.blogspot.com/2008/12/mockers-of-c-world-delight.html", "Posted by Zhanyong Wan, Software EngineerFive months ago we open-sourced Google C++ Testing Framework to help C++ developers write better tests. Enthusiastic users have embraced it and sent in numerous encouraging comments and suggestions, as well as patches to make it more useful. It was a truly gratifying experience for us.  Today, we are excited to release Google C++ Mocking Framework\u00a0(Google Mock for short)\u00a0under the\u00a0new BSD license. When used with Google Test, it lets you easily create and use mock objects\u00a0in C++ tests and rapid prototypes. If you aren't sure what mocks are or why you'll need them, our Why Google Mock? article will help explain why this is so exciting, and the Testing on the Toilet episode posted nearby on this blog gives a more light-hearted overview. In short, this technique can greatly improve the design and testability of software systems, as shown in this OOPSLA paper.  We are happily using Google Mock in more than 100 projects at Google. It works on Linux, Windows, and Mac OS X. Its benefits include: Simple, declarative syntax for defining mocks  Rich set of matchers for validating function arguments    Intuitive syntax for controlling the behavior of a mock    Automatic verification of expectations    Easy extensibility through new user-defined matchers and actions    Our users inside Google have appreciated that Google Mock is easy and even fun to use, and is an effective tool for improving software quality. We hope you'll like it too. Interested? Please take a few minutes to read the documentation and download Google Mock. Be warned, though: mocking is addictive, so proceed at your own risk.  And... we'd love to hear from you!\u00a0 If you have any questions or feedback, please meet us on the Google Mock Discussion Group. Happy mocking!", "by Zhanyong Wan, Software Engineer     Life is unfair. You work every bit as hard as Joe the Java programmer next to you. Yet as a C++ programmer, you don't get to play with all the fancy programming tools Joe takes for granted.    In particular, without a good mocking framework, mock objects in C++ have to be rolled by hand. Boy, is that tedious! (Not to mention how error-prone it is.) Why should you endure this?   Dread no more. Google Mock is finally here to help! It's a Google-originated open-source framework for creating and using C++ mocks. Inspired by jMock and EasyMock, Google Mock is easy to use, yet flexible and extensible. All you need to get started is the ability to count from 0 to 10 and use an editor.  Think you can do it? Let's try this simple example: you have a ShoppingCart class that gets the tax rate from a server, and you want to test that it remembers to disconnect from the server even when the server has generated an error. It's easy to write the test using a mock tax server, which implements this interface:  class TaxServer {  \u00a0 // Returns the tax rate of a location  \u00a0 // (by postal code) or -1 on error.  \u00a0 virtual double FetchTaxRate(  \u00a0\u00a0\u00a0 const string& postal_code) = 0;  \u00a0 virtual void CloseConnection() = 0;  };  Here's how you mock it and use the mock server to verify the expected behavior of ShoppingCart:    class MockTaxServer : public TaxServer { \u00a0\u00a0\u00a0\u00a0// #1 \u00a0\u00a0MOCK_METHOD1(FetchTaxRate, double(const string&)); \u00a0\u00a0MOCK_METHOD0(CloseConnection, void());  };  TEST(ShoppingCartTest,\u00a0  \u00a0\u00a0\u00a0 StillCallsCloseIfServerErrorOccurs) { \u00a0 MockTaxServer mock_taxserver; \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// #2 \u00a0 EXPECT_CALL(mock_taxserver, FetchTaxRate(_))  \u00a0\u00a0\u00a0 .WillOnce(Return(-1)); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// #3 \u00a0 EXPECT_CALL(mock_taxserver, CloseConnection()); \u00a0 ShoppingCart cart(&mock_taxserver); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// #4  \u00a0 cart.CalculateTax(); \u00a0// Calls FetchTaxRate()  \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// and CloseConnection().  } \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// #5      Derive the mock class from the interface. For each virtual method, count how many arguments it has, name the result n, and define it using MOCK_METHODn, whose arguments are the name and type of the method.     Create an instance of the mock class. It will be used where you would normally use a real object.       Set expectations on the mock object (How will it be used? What will it do?). For example, the first EXPECT_CALL says that FetchTaxRate() will be called and will return an error. The underscore (_) is a matcher that says the argument can be anything. Google Mock has many matchers you can use to precisely specify what the argument should be like. You can also define your own matcher or use an exact value.   Exercise code that uses the mock object. You'll get an error immediately if a mock method is called more times than expected or with the wrong arguments.      When the mock object is destroyed, it checks that all expectations on it have been satisfied.    You can also use Google Mock for rapid prototyping \u2013 and get a better design. To find out more, visit the project homepage at http://code.google.com/p/googlemock/. Now, be the first one on your block to use Google Mock and prepare to be envied. Did I say life is unfair? Remember to download this episode and post it in your office!Toilet-Friendly Version", "by Mi\u0161ko HeveryGoogle Tech TalksNovember 20, 2008ABSTRACTIs your code full of if statements? Switch statements? Do you have the same switch statement in various places? When you make changes do you find yourself making the same change to the same if/switch in several places? Did you ever forget one?This talk will discuss approaches to using Object Oriented techniques to remove many of those conditionals. The result is cleaner, tighter, better designed code that's easier to test, understand and maintain.VideoSlides", "It is with great pleasure that I have been able to finally open-source the Guide to Writing Testable Code.I am including the first page here for you, but do come and check it out in detail.To keep our code at Google in the best possible shape we provided our software engineers with these constant reminders. Now, we are happy to share them with the world.Many thanks to these folks for inspiration and hours of hard work getting this guide done:Jonathan Wolter  Russ Ruffer Mi\u0161ko HeveryFlaw #1: Constructor does Real WorkWarning Signsnew keyword in a constructor or at field declaration  Static method calls in a constructor or at  field declaration  Anything more than field assignment in  constructors  Object not fully initialized after the  constructor finishes (watch out for initialize methods)  Control flow (conditional or looping logic)  in a constructor  Code does complex object graph construction  inside a constructor rather than using a factory or builder  Adding or using an initialization blockFlaw #2: Digging into CollaboratorsWarning SignsObjects are passed in but never used directly  (only used to get access to other objects) Law of Demeter violation: method call chain  walks an object graph with more than one dot (.) Suspicious names: context,  environment, principal,  container, or managerFlaw #3: Brittle Global State & SingletonsWarning SignsAdding or using singletons Adding or using static fields or static  methods Adding or using static initialization blocks Adding or using registries Adding or using service locatorsFlaw #4: Class Does Too MuchWarning SignsSumming up what the class does includes the  word \u201cand\u201d Class would be challenging for new team  members to read and quickly \u201cget it\u201d Class has fields that are only used in some  methods Class has static methods that only operate on  parameters", "Posted by Alek Icev, Test Engineering ManagerAs you may know our core vision is to build \"The perfect search engine that would understand exactly what you mean and give back exactly what you want.\". In order to do that we learn from our data, we learn from the past  and we love Machine Learning. Everyday we are trying to answer the following questions.  Is this email spam? Is this search result relevant?What product category does that query belong to? What is the ad that users are most likely to click on for the query \u201cflowers\u201d? Is this click fraudulent? Is this ad likely to result in a purchase (not merely a click)? Is this image pornographic? Does this page contain malware? \u2013Should this query bring up a maps onebox? Solving many problems require Machine Learning techniques. On all of them we can build prediction models that will learn from the past and try to give the most precise answers to our users. We use variety of Machine Learning algorithms at Google and we are experimenting with numerous old and new advancements in this field in order to find the most accurate, fast and reliable solution for the different problems that we are attacking. Of course one the biggest challenges that we are facing in the Test Engineering community is how are we going to test these algorithms. The amount of the data that Google generates goes beyond all of the known boundaries of environments where the current Machine Learning Solutions were being crafted and tested. We want to open discussion around the ideas how to test different online machine algorithms. From time to time we will present an algorithm and some ideas how to test it and solicit the feedback from the wider audience i.e. try to build a wisdom of the crowds over the testing ideas.So let's look at the Stochastic Gradient Descent Algorithm  Where X is the set of input values of Xi ,W is set of  the importance factors(weights) of every value Xi.  A positive weight means that that risk factor increases the probability of the outcome, while a negative weight means that that risk factor decreases the probability of that outcome. t is the target output value, \u03b7 is the learning rate(the role of the learning rate is to control the level to which the weights are modified at every iteration and f(z) is the output generated by the function that maps large input domain to a small set of output values in this case. The function f(z) in this case is the logistic function: z = x0w0 + x1w1 + x2w2 + ... + xkwk  The logistic function has nice characteristics since it can take any input, and basically squash it to 0 or 1. Ideal for predicting probabilities on events that are dependent on multiple factors(Xi) each with different importance weights(Wi). The Stochastic Gradient Descent provides  fast convergence to find the optimal minimums of the error(E) that the function is making on the prediction as well as if there are multiple local minimums the algorithms guarantees converging to the global minimum of the prediction error. So let\u2019s go back now into the real online world where we want to give answers (predictions) to our users in milliseconds  and ask the question how are we going to design automated  tests for the Stochastic Gradient Descent Algorithm embedded into a live online prediction system. The environment is pretty agile and dynamic, the code is being changed every hour, you want your tests to run on 24/7 basis, you want to detect errors upstream in the development process, but you don\u2019t want to block the development process with tests that are running days, on the other side you want to release new features fast, but the release process has to be error prone(imagine the world with google being down for 5 mins, that is a global catastrophe, isn\u2019t it?!  So let\u2019s look at some of the test strategies: Should we try to train the model(set of the importance factors) and test the model with the subset of the training data? What if this takes far more than hours, maybe days to do that? Should we try  to reduce the set of importance factors (Xi) and get the convergence(E->0) on the reduced model?Should we try to reduce the training data set(the variety of set of values for X as an input to the algorithm) and keep the original model and get the convergence by any price? Should we be happy with reducing both the model size and the training set? Are we going to worry for over-fitting in the test environment? Given the original data is online data and evolves fast, are we going to be satisfied with fixed data test set or change the input test data frequently? What are the triggers that will make you do so? What else should we do?Drop us a note, all ideas are more than welcome.", "by Mi\u0161ko HeveryGoogle Tech TalksNovember 13, 2008ABSTRACTClean Code Talk SeriesTopic: Global State and SingletonsSpeaker: Mi\u0161ko HeveryVideoSlides", "by Mi\u0161ko HeveryI think of bugs as being classified into three fundamental kinds of bugs.Logical: Logical bug is the most common and classical \"bug.\" This is your \"if\"s, \"loop\"s, and other logic in your code. It is by far the most common kind of bug in an application. (Think: it does the wrong thing)Wiring: Wiring bug is when two different objects are miswired. For example wiring the first-name to the last-name field. It could also mean that the output of one object is not what the input of the next object expects. (Think: Data gets clobbered in process to where it is needed.)Rendering: Rendering bug is when the output (typical some UI or a report) does not look right. The key here is that it takes a human to determine what \"right\" is. (Think: it \"looks\" wrong)NOTE: A word of caution. Some developers think that since they are building UI everything is a rendering bug! A rendering bug would be that the button text overlaps with the button border. If you click the button and the wrong thing happens than it is either because you wired it wrong (wiring problem) or your logic is wrong (a logical bug). Rendering bugs are rare.Typical Application Distribution (without Testability in Mind)The first thing to notice about these three bug types is that the probability is not evenly distributed. Not only is the probability not even, but the cost of finding and fixing them is different. (I am sure you know this from experience). My experience from building web-apps tells me that the Logical bugs are by far the most common, followed by wiring and finally rendering bugs.Cost of Finding the BugLogical bugs are notoriously hard to find. This is because they only show up when the right set of input conditions are present and finding that magical set of inputs or reproducing it tends to be hard. On the other hand wiring bugs are much easier to spot since the wiring of the application is mostly fixed. So if you made a wiring error, it will show up every time you execute that code, for the most part independent of input conditions. Finally, the rendering bugs are the easiest. You simply look at the page and quickly spot that something \"looks\" off.Cost of Fixing the BugOur experience also tells us how hard it is to fix things. A logical bug is hard to fix, since you need to understand all of the code paths before you know what is wrong and can create a solution. Once the solution is created, it is really hard to be sure that we did not break the existing functionality. Wiring problems are much simpler, since they either manifest themselves with an exception or data in wrong location. Finally rendering bugs are easy since you \"look\" at the page and immediately know what went wrong and how to fix it. The reason it is easy to fix is that we design our application knowing that rendering will be something which will be constantly changing.LogicalWiringRenderingProbability of OccurrenceHighMediumLowDifficulty of DiscoveringDifficultEasyTrivialCost of FixingHigh CostMediumLowHow does testability change the distribution?It turns out that testable code has effect on the distribution of the bugs. Testable code needs:Clear separation between classes (Testable Seams) --> clear separation between classes makes it less likely that a wiring problem is introduced. Also, less code per class lowers the probability of logical bug.Dependency Injection --> makes wiring explicit (unlike singletons, globals or service locators).Clear separation of Logic from Wiring --> by having wiring in a single place it is easier to verify.The result of all of this is that the number of wiring bugs are significantly reduced. (So as a percentage we gain Logical Bugs. However total number of bugs is decreased.)The interesting thing to notice is that you can get benefit from testable code without writing any tests. Testable code is better code! (When I hear people say that they sacrificed \"good\" code for testability, I know that they don't really understand testable-code.)We Like Writing Unit-TestsUnit-tests give you greatest bang for the buck. A unit test focuses on the most common bugs, hardest to track down and hardest to fix. And a unit-test forces you to write testable code which indirectly helps with wiring bugs. As a result when writing automated tests for your application we want to overwhelmingly focus on unit test. Unit-tests are tests which focus on the logic and focus on one class/method at a time.Unit-tests focus on the logical bugs. Unit tests focus on your \"if\"s and \"loop\"s, a Focused unit-test does not directly check the wiring. (and certainly not rendering)Unit-test are focused on a single CUT (class-under-test). This is important, since you want to make sure that unit-tests will not get in the way of future refactoring. Unit-tests should HELP refactoring not PREVENT refactorings. (Again, when I hear people say that tests prevent refactorings, I know that they have not understood what unit-tests are)Unit-tests do not directly prove that wiring is OK. They do so only indirectly by forcing you to write more testable code.Functional tests verify wiring, however there is a trade-off. You \"may\" have hard time refactoring if you have too many functional test OR, if you mix functional and logical tests.Managing Your BugsI like to think of tests as bug management. (with the goal of bug free) Not all types of errors are equally likley, therefore I pick my battles of which tests I focus on. I find that I love unit-tests. But they need to be focused! Once a test starts testing a lot of classes in a single pass I may enjoy high coverage, but it is really hard to figure out what is going on when the test is red. It also may hinder refactorings. I tend to go very easy on Functional tests. A single test to prove that things are wired together is good enough to me.I find that a lot of people claim that they write unit-tests, but upon closer inspection it is a mix of functional (wiring) and unit (logic) test.  This happens becuase people wirte tests after code, and therefore the code is not testable. Hard to test code tends to create mockeries. (A mockery is a test which has lots of mocks, and mocks returning other mocks in order to execute the desired code) The result of a mockery is that you prove little. Your test is too high level to assert anything of interest on method level. These tests are too intimate with implementation ( the intimace comes from too many mocked interactions) making any refactorings very painful.", "If you've got some multi-threaded code, you may have data races in it. Data races are hard to find and reproduce \u2013 usually they will not occur in testing but will fire once a month in production.For example, you ask each of your two interns to bring you a bottle of beer. This will usually result in your getting two bottles (perhaps empty), but in a rare situation that the interns collide near the fridge, you may get fewer bottles. \u00a04 int bottles_of_beer = 0;\u00a05 void Intern1() { bottles_of_beer++; }  // Intern1 forgot to use Mutex.\u00a06 void Intern2() { bottles_of_beer++; }  // Intern2 copied from Intern1.\u00a07 int main() {\u00a08   // Folks, bring me one bottle of beer each, please.\u00a09   ClosureThread intern1(NewPermanentCallback(Intern1)),10                 intern2(NewPermanentCallback(Intern2));11   intern1.SetJoinable(true); intern2.SetJoinable(true);12   intern1.Start();           intern2.Start();13   intern1.Join();            intern2.Join();14   CHECK_EQ(2, bottles_of_beer) Who didn't bring me my beer!?\";15 }Want to find data races in your code? Run your program under Helgrind!$ helgrind path/to/your/programPossible data race during read of size 4 at 0x5429C8   at 0x400523: Intern2() tott.cc:6\u00a0\u00a0by 0x400913: _FunctionResultCallback_0_0::Run() ...\u00a0\u00a0by 0x4026BB: ClosureThread::Run() ...\u00a0\u00a0...\u00a0\u00a0Location 0x5429C8 has never been protected by any lock\u00a0\u00a0Location 0x5429C8 is 0 bytes inside global var \"bottles_of_beer\"\u00a0\u00a0declared at tott.cc:4Helgrind will also detect deadlocks for you. Helgrind is a tool based on Valgrind. Valgrind is a binary translation framework which has other useful tools such as a memory debugger and a cache simulator. Related TotT episodes will follow.No beer was wasted in the making of this TotT.Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko HeveryGoogle Tech TalksNovember 6, 2008ABSTRACTClean Code Talk SeriesTopic: Don't Look For Things!Speaker: Mi\u0161ko Hevery VideoSlides", "by Mi\u0161ko Hevery\n\nGoogle Tech Talks October, 30 2008 ABSTRACT Clean Code Talks - Unit Testing Speaker: Misko Hevery\n\nVideo\n\n\n\nSlides", "Posted by Patricia Legaspi, Test Engineering ManagerOne of the challenges of automation is achieving complete automation. Ideally, complete or total automation would not require any human intervention or verification yet this is a difficult level to achieve. Investing Engineering time to completely automate tests is expensive and, many times, has diminishing returns. Rather than trying to achieve complete automation, investing in ways to make the most out of the automated test and the human time is time better spent.Effective test reportConsider an automated UI test... Routinely, automated UI tests result in false negatives due to timing issues, the complexity of the steps taken, and other factors. These have to be investigated which can be time consuming. A thorough report can be created for automated UI tests to present log information, error reports, screen shots of the state of the application when the test failed, and an easy way to re-run the test in question. A human can make use of the information provided and effectively investigate the possible issue. If we can reduce the amount of work that someone has to do to investigate a test failure, the UI tests become more valuable and the human plays a much more important role as a \"verifier.\" Had the report not provided any information for the failed test, the human would have spent, in some cases, hours investigating the issue rather than continuing to automate or run exploratory tests which would be of more value. Is there a way to maximize what people do well, while also maximizing the use of automation?Applying human intelligenceWhat about tests that require a human eye? There are many arguments about tests that require a human make a judgment about the appearance of rendered UI objects. Machines are great at running tests that return in a firm pass or fail but for tests that require opinion or judgment, the human has an advantage. We've been experimenting with image comparison tests. Screen shots of a golden version of the application under test are compared to screen shots of the current release candidate to verify that the application continues to render properly and that the UI \"looks good\". Although image comparison techniques can determine if the screen shots are different, they cannot help determine if the difference is \"important\". The human comes back into the picture to complement the automated test. To make effective use of a person's time, the test results should be organized and well presented. For image comparison tests, a report which shows the golden and release candidate screen shots side-by-side with clearly highlighted differences or allows you to replace the golden screen shot is key. A tester can quickly navigate the reported differences and determine if they are actual failures or acceptable differences. Frequently, an application's UI will change which will result in expected image differences. If the differences are expected changes, the tester should be able to replace the golden with the new image with the click of a button for future comparison.Aside from test automation, efficiency can also be improved by streamlining the various components of the process. This includes test environment setup, test data retrieval, and test execution. Release cycles tighten and so should the testing processes. Automating the environment setup can be a major time saver and allow for added time to run in depth tests. Creating continuous builds to run automated tests ahead of time and preparing environments overnight results in added testing time.Automated tests are rarely bullet proof, they are prone to errors and false failures. In some cases, creating an infrastructure to make test analysis easy for humans, is much more effective than trying to engineer tests to automatically \"understand\" the UI. We coined this, \"Partial Automation.\" We found that by having a person in the loop dramatically reduces the time spent trying to over engineer the automated tests. Obviously, there are trade-offs to this approach and one size doesn't fit all. We believe that automation does not always need to mean complete automation; we automate as much as you can and consider how human judgment can be used efficiently.", "Many modules must access elements of their environment that are too heavyweight for use in tests, for example, the file system or network.  To keep tests lightweight, we mock out these elements.  But what if no mockable interface is available, or the existing interfaces pull in extraneous dependencies?  In such cases we can introduce a mediator interface that's directly associated with your module (usually as a public inner class).  We call this mediator an \"Env\" (for environment); this name helps readers of your class recognize the purpose of this interface. For example, consider a class that cleans the file system underlying a storage system:// Deletes files that are no longer reachable via our storage system's// metadata.class FileCleaner {public:\u00a0\u00a0class Env {\u00a0\u00a0public:\u00a0\u00a0\u00a0\u00a0virtual bool MatchFiles(const char* pattern, vector* filenames) = 0;\u00a0\u00a0\u00a0\u00a0virtual bool BulkDelete(const vector& filenames) = 0;\u00a0\u00a0\u00a0\u00a0virtual MetadataReader* NewMetadataReader() = 0; \u00a0\u00a0\u00a0\u00a0virtual ~Env();\u00a0\u00a0};\u00a0\u00a0// Constructs a FileCleaner.  Uses \u201cenv\u201d to access files and metadata.\u00a0\u00a0FileCleaner(Env* env, QuotaManager* qm);\u00a0\u00a0// Deletes files that are not reachable via metadata.  \u00a0\u00a0// Returns true on success.\u00a0\u00a0bool CleanOnce();};FileCleaner::Env lets us test FileCleaner without accessing the real file system or metadata.  It also  makes it easy to simulate various kinds of failures, for example, of the file system:class NoFileSystemEnv : public FileCleaner::Env {\u00a0\u00a0virtual bool MatchFiles(const char* pattern, vector* filenames) {\u00a0\u00a0match_files_called_ = true;\u00a0\u00a0return false;\u00a0\u00a0}\u00a0\u00a0...};TEST(FileCleanerTest, FileCleaningFailsWhenFileSystemFails) {\u00a0\u00a0NoFileSystemEnv* env = new NoFileSystemEnv();\u00a0\u00a0FileCleaner cleaner(env, new MockQuotaManager());\u00a0\u00a0ASSERT_FALSE(cleaner.CleanOnce());\u00a0\u00a0ASSERT_TRUE(env->match_files_called_);}An Env object is particularly useful for restricting access to other modules or systems, for example, when those modules have overly-wide interfaces.  This has the additional benefit of reducing your class's dependencies. However, be careful to keep the \u201creal\u201d Env implementation simple, lest you  introduce hard-to-find bugs in the Env.  The methods of your \u201creal\u201d Env implementation should just delegate to other, well-tested methods.The most important benefits of an Env are that it documents how your class accesses its environment and it encourages future modifications to your module to keep tests small by extending and mocking out the Env.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Philip Zembrod, Software Engineer in Test, SwedenSo you're working on TheFinalApp - the ultimate end-user application, with lots of good features and a really neat GUI. You have a team that's keen on testing and a level of unit test coverage that others only dream of. The star of the show is your suite of automatic GUI end-to-end tests \u2014 your team doesn't have to manually test every release candidate.Life would be good if only the GUI tests weren't so flaky. Every once and again, your test case clicks a menu item too early, while the menu is still opening. Or it double-clicks to open a tree node, tries to verify the open too early, then retries, which closes the node (oops). You have tried adding sleep statements, which has helped somewhat, but has also slowed down your tests.Why all this pain? Because GUIs are not designed to synchronize with other computer programs. They are designed to synchronize with human beings, which are not like computers:Humans act much more slowly. Well-honed GUI test robots drive GUIs at near theoretical maximum speed.Humans are much better at observing the GUI, and they react intelligently to what they see.Humans extract more meaningful information from a GUI.In contrast to testing a server, where you usually find enough methods or messages in the server API to synchronize the testing with the server, a GUI application usually lacks these means of synchronization. As a result, a running automated GUI test often consists of one long sequence of race conditions between the automated test and the application under test.GUI test synchronization boils down to the question: Is the app under test finished with what it's doing? \"What it's doing\" may be small, like displaying a combo box, or big, like a business transaction. Whatever \"it\" is, the test must be able to tell whether \"it\" is finished. Maybe you want to test something while \"it\" is underway, like verify that the browser icon is rotating while a page is loading. Maybe you want to deliberately click the \"Submit\" button again in the middle of a transaction to verify that nothing bad happens. But usually, you want to wait until \"it\" is done.How to find out whether \"it\" is done? Ask! Let your test case ask your GUI app. In other words: provide one or several test hooks suitable for your synchronization needs.The questions to ask depend on the type, platform, and architecture of your application. Here are three questions that  worked for me when dealing with a single-threaded Win32 MFC database app:The first is a question for the OS. The Win32 API provides a function to wait while a process has pending input events:DWORD WaitForInputIdle(HANDLE hProcess, DWORD dwMilliseconds). Choosing the shortest possible timeout (dwMilliseconds = 1) effectively turns this from a wait-for to a check-if function, so you can explicitly control the waiting loop; for example, to combine several different check functions. Reasoning: If the GUI app has pending input, it's surely not ready for new input.The second question is: Is the GUI app's message queue empty? I did this with a test hook, in this case a WM_USER message; it could perhaps also be done by calling PeekMessage() in the GUI app's process context via CreateRemoteThread(). Reasoning: If the GUI app still has messages in its queue, it's not yet ready for new input.The third is more like sending a probe than a question, but again using a test hook. The test framework resets a certain flag in the GUI app (synchronously) and then (asynchronously) posts a WM_USER message into the app's message queue that, upon being processed, sets this flag. Now the test framework checks periodically (and synchronously again) to see whether the flag has been set. Once it has, you know the posted message has been processed. Reasoning: When the posted message (the probe) has been processed, then surely messages and events sent earlier to the GUI app have been processed. Of course, for multi-threaded applications this might be more complex.These three synchronization techniques resulted in fast and stable test execution, without any test flakiness due to timing issues. All without sleeps, except in the synchronization loop.Applying this idea to different platforms requires finding the right questions to ask and the right way to ask them. I'd be interested to hear if someone has done something similar, e.g. for an Ajax application. A query into the server to check if any XML responses are pending, perhaps?", "Testability Explorer: Using Byte-Code Analysis to Engineer Lasting Social Changes in an Organization\u2019s Software Development Process. (Or How to Get Developers to Write Testable Code) Presented at 2008 OOPSLA by Mi\u0161ko Hevery a Best Practices Coach @ Google Abstract Testability Explorer is an open-source tool that identifies hard-to-test Java code. Testability Explorer provides a repeatable objective metric of \u201ctestability.\u201d This metric becomes a key component of engineering a social change within an organization of developers. The Testability Explorer report provides actionable information to developers which can be used as (1) measure of progress towards a goal and (2) a guide to refactoring towards a more testable code-base.Keywords: unit-testing; testability; refactoring; byte-code analysis; social engineering. 1.\u2003Testability Explorer Overview In order to unit-test a class, it is important that the class can be instantiated in isolation as part of a unit-test. The most common pitfalls of testing are (1) mixing object-graph instantiation with application-logic and (2) relying on global state. The Testability Explorer can point out both of these pitfalls. 1.1\u2003Non-Mockable Cyclomatic Complexity Cyclomatic complexity is a count of all possible paths through code-base. For example: a main method will have a large cyclomatic complexity since it is a sum of all of the conditionals in the application. To limit the size of the cyclomatic complexity in a test, a common practice is to replace the collaborators of class-under-test with mocks, stubs, or other test doubles. Let\u2019s define \u201cnon-mockable cyclomatic complexity\u201d as what is left when the class-under-test has all of its accessible collaborators replaced with mocks. A code-base where the responsibility of object-creation and application-logic is separated (using Dependency Injection) will have high degree of accessible collaborators; as a result most of its collaborators will easily be replaceable with mocks, leaving only the cyclomatic complexity of the class-under-test behind. In applications, where the class-under-test is responsible for instantiating its own collaborators, these collaborators will not be accessible to the test and as a result will not be replaceable for mocks. (There is no place to inject test doubles.) In such classes the cyclomatic complexity will be the sum of the class-under-test and its non-mockable collaborators. The higher the non-mockable cyclomatic complexity the harder it will be to write a unit-test. Each non-mockable conditional translates to a single unit of cost on the Testability Explorer report. The cost of static class initialization and class construction is automatically included for each method, since a class needs to be instantiated before it can be exercised in a test. 1.2\u2003Transitive Global-State Good unit-tests can be run in parallel and in any order. To achieve this, the tests need to be well isolated. This implies that only the stimulus from the test has an effect on the code execution, in other words, there is no global-state. Global-state has a transitive property. If a global variable refers to a class than all of the references of that class (and all of its references) are globally accessible as well. Each globally accessible variable, that is not final, results in a cost of ten units on the Testability Explorer. 2.\u2003Testability Explorer Report A chain is only as strong as its weakest link. Therefore the cost of testing a class is equal to the cost of the class\u2019 costliest method. In the same spirit the application\u2019s overall testability is de-fined in terms of a few un-testable classes rather than a large number of testable ones. For this reason when computing the overall score of a project the un-testable classes are weighted heavier than the testable ones.  3.\u2003How to Interpret the Report By default the classes are categorized into three categories: \u201cExcellent\u201d (green) for classes whose cost is below 50; \u201cGood\u201d (yellow) for classes whose cost is below 100; and \u201cNeeds work\u201d (red) for all other classes. For convenience the data is presented as both a pie chart and histogram distribution and overall (weighted average) cost shown on a dial. [-]ClassRepository [ 323 ] [-]ClassInfo getClass(String) [ 323 ]   line 51:     ClassInfo parseClass(InputStream) [318]     InputStream inputStreamForClass(String) [2] [-]ClassInfo parseClass(InputStream) [318]   line 77: void accept(ClassVisitor, int) [302]   line 75: ClassReader(InputStream) [15] Clicking on the class ClassRepository allows one to drill down into the classes to get more information. For example the above report shows that ClassRepository has a high cost of 318 due to the parseClass(InputStream) method. Looking in closer we see that the cost comes from line 77 and an invocation of the accept() method. 73:ClassInfo parseClass(InputStream is) {74:  try {75:    ClassReader reader = new ClassReader(is);76:    ClassBuilder v = new ClassBuilder (this);77:    reader.accept(v, 0);78:    return visitor.getClassInfo();79:  } catch (IOException e) {80:    throw new RuntimeException(e);81:  }82:} As you can see from the code the ClassReader can never be replaced for a mock and as a result the cyclomatic complexity of the accept method can not be avoided in a test \u2014 resulting in a high testability cost. (Injecting the ClassReader would solve this problem and make the class more test-able.) 4.\u2003Social Engineering In order to produce a lasting change in the behavior of developers it helps to have a measurable number to answer where the project is and where it should be. Such information can provide in-sight into whether or not the project is getting closer or farther from its testability goal. People respond to what is measured. Integrating the Testability Explorer with the project\u2019s continuous build and publishing the report together with build artifacts communicate the importance of testability to the team. Publishing a graph of overall score over time allows the team to see changes on per check-in basis. If Testability Explorer is used to identify the areas of code that need to be refactored, than compute the rate of improvement and project expected date of refactoring completion and create a sense of competition among the team members. It is even possible to set up a unit test for Testability Explorer that will only allow the class whose testability cost is better than some predetermined cost.", "by Mi\u0161ko Hevery After reading the article on Singletons (the design anti-pattern) and how they are really global variables and dependency injection suggestion to simply pass in the reference to the singleton in a constructor (instead of looking them up in global state), many people incorrectly concluded that now they will have to pass the singleton all over the place. Let me demonstrate the myth with the following example. Let's say that you have a LoginPage which uses the UserRepository for authentication. The UserRepository in turn uses Database Singleton to get a hold of the global reference to the database connection, like this: class UserRepository { private static final BY_USERNAME_SQL = \"Select ...\"; User loadUser(String user) {   Database db = Database.getInstance();   return db.query(BY_USERNAME_SQL, user); }}class LoginPage { UserRepository repo = new UserRepository(); login(String user, String pwd) {   User user = repo.loadUser(user);   if (user == null || user.checkPassword(pwd)) {     throw new IllegalLoginException();   } }} The first thought is that if you follow the advice of dependency injection you will have to pass in the Database into the LoginPage just so you can pass it to the UserRepository. The argument goes that this kind of coding will make the code hard to maintain, and understand. Let's see what it would look like after we get rid of the global variable Singleton Database look up. First, lets have a look at the UserRepository. class UserRepository { private static final BY_USERNAME_SQL = \"Select ...\"; private final Database db;  UserRepository(Database db) {   this.db = db; } User loadUser(String user) {   return db.query(BY_USERNAME_SQL, user); }} Notice how the removal of Singleton global look up has cleaned up the code. This code is now easy to test since in a test we can instantiate a new UserRepository and pass in a fake database connection into the constructor. This improves testability. Before, we had no way to intercept the calls to the Database and hence could never test against a Database fake. Not only did we have no way of intercepting the calls to Database, we did not even know by looking at the API that Database is involved. (see Singletons are Pathological Lairs) I hope everyone would agree that this change of explicitly passing in a Database reference greatly improves the code. Now lets look what happens to the LoginPage... class LoginPage { UserRepository repo;  LoginPage(Database db) {   repo = new UserRepository(db); } login(String user, String pwd) {   User user = repo.loadUser(user);   if (user == null || user.checkPassword(pwd)) {     throw new IllegalLoginException();   } }} Since UserRepository can no longer do a global look-up to get a hold of the Database it musk ask for it in the constructor. Since LoginPage is doing the construction it now needs to ask for the Databse so that it can pass it to the constructor of the UserRepository. The myth we are describing here says that this makes code hard to understand and maintain. Guess what?! The myth is correct! The code as it stands now is hard to maintain and understand. Does that mean that dependency injection is wrong? NO! it means that you only did half of the work! In how to think about the new operator we go into the details why it is important to separate your business logic from the new operators. Notice how the LoginPage violates this. It calls a new on UserRepository. The issue here is that LoginPage is breaking a the Law of Demeter. LoginPage is asking for the Database even though it itself has no need for the Database (This greatly hinders testability as explained here). You can tell since LoginPage does not invoke any method on the Database. This code, like the myth suggest, is bad! So how do we fix that? We fix it by doing more Dependency Injection. class LoginPage { UserRepository repo;  LoginPage(UserRepository repo) {   this.repo = repo; } login(String user, String pwd) {   User user = repo.loadUser(user);   if (user == null || user.checkPassword(pwd)) {     throw new IllegalLoginException();   } }} LoginPage needs UserRepository. So instead of trying to construct the UserRepository itself, it should simply ask for the UserRepository in the constructor. The fact that UserRepository needs a reference to Database is not a concern of the LoginPage. Neither is it a concern of LoginPage how to construct a UserRepository. Notice how this LoginPage is now cleaner and easier to test. To test we can simply instantiate a LoginPage and pass in a fake UserRepository with which we can emulate what happens on successful login as well as on unsuccessful login and or exceptions. It also nicely takes care of the concern of this myth. Notice that every object simply knows about the objects it directly interacts with. There is no passing of objects reference just to get them into the right location where they are needed. If you get yourself into this myth then all it means is that you have not fully applied dependency injection. So here are the two rules of Dependency Injection: Always ask for a reference! (don't create, or look-up a reference in global space aka Singleton design anti-pattern)If you are asking for something which you are not directly using, than you are violating a Law of Demeter. Which really means that you are either trying to create the object yourself or the parameter should be passed in through the constructor instead of through a method call. (We can go more into this in another blog post) So where have all the new operators gone you ask? Well we have already answered that question here. And with that I hope we have put the myth to rest! BTW, for those of you which are wondering why this is a common misconception the reason is that people incorrectly assume that the constructor dependency graph and the call graph are inherently identical (see this post). If you construct your objects in-line (as most developers do, see thinking about the new operator) then yes the two graphs are very similar. However, if you separate the object graph instantiation from the its execution, than the two graphs are independent. This independence is what allows us to inject the dependencies directly where they are needed without passing the reference through the intermediary collaborators.", "By Roshan Sembacuttiaratchy, Software Engineer in Test, Google Zurich, SwitzerlandWhen I tell people that I'm a Test Engineer at Google, I get a confused look from them and questions as to what I actually do.   Do I sit in front of a keyboard clicking every button and link on screen?  Do I watch over the infinite number of monkeys, checking to make sure they produce Shakespeare's work while clicking said links?  Or do we create better and smarter pigeons?  Well, it's actually a bit of everything, but not quite in the way you might think at first. The people working in Test Engineering are software developers with a passion for testing and test automation.  Whereas most software engineers at Google are working on developing products, Test Engineering's objective is to help improve the quality of the projects we're involved in.  We work integrated with the project team, developing test frameworks and setting up test systems.  One of our key mantras is automation, so our first task whenever we're assigned to a new project is to evaluate the current state of test automation, identify what we could do to improve the results obtained, reduce the total run time, and make the best use of available resources.  Load and performance testing is another important task we perform, along with analysis of the results.  We're also not afraid to get our hands dirty by diving deep into the project's code to identify what re-factoring might be useful to help test the system better, creating and introducing stubs, fakes and mocks  as necessary.  As you might guess, we're big fans of Test Driven Design, Continuous Integration and other agile techniques and play the role of evangelists, spreading the word on new tools, techniques and best practices. Individual project assignments tend to be limited in duration, as we're a relatively small group of people in Test Engineering, helping support a much larger group of software developers.  So once we've completed our tasks for one project, we move on to the next, working with a different team of people and a different set of problems, changing projects every few months. That pretty much describes what I do, and how our team in Zurich works.  So to answer the questions posed initially, I write software which performs the task of clicking every button and every link, and which then verifies the result of these actions. I run this on a few hundred machines to mimic virtual monkeys, and hand it over to the development team so that they can use it to check that their product is working, even after I've left their project.  And if it produces the works of Shakespeare, that's just an added bonus! :-)", "If your code manipulates floating-point values, your tests will probably involve floating-point values as well.When comparing floating-point values, checking for equality might lead to unexpected results. Rounding errors can lead to a result that is close to the expected one, but not equal. As a consequence, an assertion might fail when checking for equality of two floating-point quantities even if the program is implemented correctly.The Google C++ Testing Framework provides functions for comparing two floating-point quantities up to a given precision.In C++, you can use the following macros:ASSERT_FLOAT_EQ(expected, actual);ASSERT_DOUBLE_EQ(expected, actual);EXPECT_FLOAT_EQ(expected, actual);EXPECT_DOUBLE_EQ(expected, actual);In Java, JUnit overloads Assert.assertEquals for floating-point types:assertEquals(float expected, float actual, float delta);assertEquals(double expected, double actual, double delta);An example (in C++):TEST(SquareRootTest, CorrectResultForPositiveNumbers) {\u00a0\u00a0EXPECT_FLOAT_EQ(2.0f, FloatSquareRoot(4.0f));\u00a0\u00a0EXPECT_FLOAT_EQ(23.3333f, FloatSquareRoot(544.44444f));\u00a0\u00a0EXPECT_DOUBLE_EQ(2.0, DoubleSquareRoot(4.0));\u00a0\u00a0EXPECT_DOUBLE_EQ(23.33333333333333, DoubleSquareRoot(544.44444444444444));\u00a0\u00a0\u00a0\u00a0// the above succeeds\u00a0\u00a0EXPECT_EQ(2.0, DoubleSquareRoot(4.0));\u00a0\u00a0\u00a0\u00a0// the above fails\u00a0\u00a0EXPECT_EQ(23.33333333333333, DoubleSquareRoot(544.44444444444444));} Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko Hevery Dependency injection asks us to separate the new operators from the application logic. This separation forces your code to have factories which are responsible for wiring your application together. However, better than writing factories,  we want to use automatic dependency injection such as GUICE to do the wiring for us. But can DI really save us from all of the new operators? Lets look at two extremes. Say you have a class MusicPlayer which needs to get a hold of AudioDevice. Here we want to use DI and ask for the AudioDevice in the constructor of the MusicPlayer. This will allow us to inject a test friendly AudioDevice which we can use to assert that correct sound is coming out of our MusicPlayer. If we were to use the new operator to instantiate the BuiltInSpeakerAudioDevice we would have hard time testing. So lets call objects such as AudioDevice or MusicPlayer \"Injectables.\" Injectables are objects which you will ask for in the constructors and expect the DI framework to supply. Now, lets look at the other extreme. Suppose you have primitive \"int\" but you want to auto-box it into an \"Integer\" the simplest thing is to call new Integer(5) and we are done. But if DI is the new \"new\" why are we calling the new in-line? Will this hurt our testing? Turns out that DI frameworks can't really give you the Integer you are looking for since it does not know which Integer you are referring to. This is a bit of a toy example so lets look at something more complex. Lets say the user entered the email address into the log-in box and you need to call new Email(\"a@b.com\"). Is that OK, or should we ask for the Email in our constructor. Again, the DI framework has no way of supplying you with the Email since it first needs to get a hold of a String where the email is. And there are a lot of Strings to chose from. As you can see there are a lot of objects out there which DI framework will never be able to supply. Lets call these \"Newables\" since you will be forced to call new on them manually. First, lets lay down some ground rules. An Injectable class can ask for other Injectables in its constructor. (Sometimes I refer to Injectables as Service Objects, but that term is overloaded.) Injectables tend to have interfaces since chances are we may have to replace them with an implementation friendly to testing. However, Injectable can never ask for a non-Injectable (Newable) in its constructor. This is because DI framework does not know how to produce a Newable. Here are some examples of classes I would expect to get from my DI framework: CreditCardProcessor, MusicPlayer, MailSender, OfflineQueue. Similarly Newables can ask for other Newables in their constructor, but not for Injectables (Sometimes I refer to Newables as Value Object, but again, the term is overloaded). Some examples of Newables are: Email, MailMessage, User, CreditCard, Song. If you keep this distinctions your code will be easy to test and work with. If you break this rule your code will be hard to test. Lets look at an example of a MusicPlayer and a Song class Song { Song(String name, byte[] content);}class MusicPlayer { @Injectable MusicPlayer(AudioDevice device); play(Song song);} Notice that Song only asks for objects which are Newables. This makes it very easy to construct a Song in a test. Music player is fully Injectable, and so is its argument the AudioDevice, therefore, it can be gotten from DI framework. Now lets see what happens if the MusicPlayer breaks the rule and asks for Newable in its constructor. class Song { String name; byte[] content; Song(String name, byte[] content);}class MusicPlayer { AudioDevice device; Song song; @Injectable MusicPlayer(AudioDevice device, Song song); play();} Here the Song is still Newable and it is easy to construct in your test or in your code. The MusicPlayer is the problem. If you ask DI framework for MusicPlayer it will fail, since the DI framework will not know which Song you are referring to. Most people new to DI frameworks rarely make this mistake since it is so easy to see: your code will not run. Now lets see what happens if the Song breaks the rule and ask for Injectable in its constructor. class MusicPlayer { AudioDevice device; @Injectable MusicPlayer(AudioDevice device);}class Song { String name; byte[] content; MusicPlayer palyer; Song(String name, byte[] content, MusicPlayer player); play();}class SongReader { MusicPlayer player @Injectable SongReader(MusicPlayer player) {   this.player = player; } Song read(File file) {   return new Song(file.getName(),                   readBytes(file),                   player); }} At first the world looks OK. But think about how the Songs will get created. Presumably the songs are stored on a disk and so we will need a SongReader. The SongReader will have to ask for MusicPlayer so that when it calls the new on a Song it can satisfy the dependencies of Song on MusicPlayer. See anything wrong here? Why in the world does SongReader need to know about the MusicPlayer. This is a violation of Law of Demeter. The SongReader does not need to know about MusicPlayer. You can tell since SongReader does not call any method on the MusicPlayer. It only knows about the MusicPlayer because the Song has violated the Newable/Injectable separation. The SongReader pays the price for a mistake in Song. Since the place where the mistake is made and where the pain is felt are not the same this mistake is very subtle and hard to diagnose. It also means that a lot of people make this mistake. Now from the testing point of view this is a real pain. Suppose you have a SongWriter and you want to verify that it correctly serializes the Song to disk. Why do you have to create a MockMusicPlayer so that you can pass it into a Song so that you can pass it into the SongWritter. Why is MusicPlayer in the picture? Lets look at it from a different angle. Song is something you may want to serialize, and simplest way to do that is to use Java serialization. This will serialize not only the Song but also the MusicPlayer and the AudioDevice. Neither MusicPlayer nor the AudioDevice need to be serialized. As you can see a subtle change makes a whole lot of difference in the easy of testability. As you can see the code is easiest to work with if we keep these two kinds objects distinct. If you mix them your code will be hard to test.  Newables are objects which are at the end of your application object graph. Newables may depend on other Newables as in CreditCard may depend on Address which may depend on a City but these things are leafs of the application graph. Since they are leafs, and they don't talk to any external services (external services are Injectables) there is no need to mock them. Nothing behaves more like a String like than a String. Why would I mock User if I can just new User, Why mock any of these: Email, MailMessage, User, CreditCard, Song? Just call new and be done with it. Now here is something very subtle. It is OK for Newable to know about Injectable. What is not OK is for the Newable to have a field reference to Injectable. In other words it is OK for Song to know about MusicPlayer. For example it is OK for an Injectable MusicPlayer to be passed in through the stack to a Newable Song. This is because the stack passing is independent of DI framework. As in this example: class Song { Song(String name, byte[] content); boolean isPlayable(MusicPlayer player);} The problem becomes when the Song has a field reference to MusicPlayer. Field references are set through the constructor which will force a Law of Demeter violation for the caller and we will have hard time to test.", "Sometimes you need to test client-side JavaScript code that uses setTimeout() to do some work in the future. jsUnit contains the Clock.tick() method, which simulates time passing without causing the test to sleep.For example, this function will set up some callbacks to update a status message over the course of four seconds:function showProgress(status) {\u00a0\u00a0status.message = \"Loading\"; \u00a0\u00a0for (var time = 1000; time \u00a0\u00a0\u00a0\u00a0// Append a '.' to the message every second for 3 secs.\u00a0\u00a0\u00a0\u00a0setTimeout(function() {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0status.message += \".\";\u00a0\u00a0\u00a0\u00a0}, time);\u00a0\u00a0}\u00a0\u00a0setTimeout(function() {\u00a0\u00a0\u00a0\u00a0// Special case for the 4th second.\u00a0\u00a0\u00a0\u00a0status.message = \"Done\";\u00a0\u00a0}, 4000);}The jsUnit test for this function would look like this:function testUpdatesStatusMessageOverFourSeconds() {\u00a0\u00a0Clock.reset();  // Clear any existing timeout functions on the event queue.\u00a0\u00a0var status = {};\u00a0\u00a0showProgress(status);  // Call our function.\u00a0\u00a0assertEquals(\"Loading\", status.message);\u00a0\u00a0Clock.tick(2000);  // Call any functions on the event queue that have been\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// scheduled for the first two seconds.\u00a0\u00a0assertEquals(\"Loading..\", status.message);\u00a0\u00a0Clock.tick(2000);  // Same thing again, for the next two seconds.\u00a0\u00a0assertEquals(\"Done\", status.message);}This test will run very quickly - it does not require four seconds to run. Clock supports the functions setTimeout(), setInterval(), clearTimeout(), and clearInterval(). The Clock object is defined in jsUnitMockTimeout.js, which is in the same directory as jsUnitCore.js.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Jessica Tomechak, Test Engineering TeamJulian Harty, one of our senior test engineers, is presenting a keynote at the STARWEST conference today (Wednesday, October 1) on Six Thinking Hats for Software Testers. Expanding on the Thinking Hats concept, originated many years ago by Edward De Bono and used by large numbers of people and various types of businesses, Julian's talk will add his experience and views about how Thinking Hats can be used in software testing. At Google, we have found it delivers breakthroughs in the short term and great results in the longer term -- one Googler called it the \"universal unblocker.\"For those of you unable to attend the conference, a video recording will be available (UPDATE: Video now available). Julian has also written an article on this topic to appear in Better Software magazine within the next few months.", "by Mi\u0161ko Hevery      We talked about how it is important to separate the new operators from the application logic. This separation forces your code to have factories which are responsible for wiring your application together. By separating these responsibilities the tests can always wire together a subset of an application with key components replaced for friendlies making testing easier and more focused.\rLet's look at a sample factory\r class CarFactory {\rCar create() {\r  return new Car(\r        new EngineCompartment(\r              new Engine(),\r                new Door(new PowerWindow()),\r              new Door(new PowerWindow()),\r              new PowerSeat(), new PowerSeat()                new ManualTransmission(),\r              new PowerSteering(),\r              new Battery()\r           ),\r        new Cabin(\r              new Door(new PowerWindow()),\r              new Door(new PowerWindow()),\r              new PowerSeat(), new PowerSeat()\r          ),\r       Arrays.asList(\r            new Wheel(new Tire(), new Rim()),\r            new Wheel(new Tire(), new Rim()),\r            new Wheel(new Tire(), new Rim()),\r            new Wheel(new Tire(), new Rim())\r          )\r     );\r}\r}    This factory builds a car. The first thing to notice is that all of the new operators are here (If you were to look inside each of the classes it would be devoid of \"new\"s). The second thing to notice is a complete lack of logic (no loops or conditions). And thirdly, your application behavior is controlled by the way the classes are wired together. If I wanted a automatic-transmission car, all I would have to do is to wire the classes differently. The wiring responsibility is in the factory class not with the application logic.\r But why do we need to tell the JVM how to wire these Classes together? Is it not self obvious? Just look at the constructors of these classes:\rCar(EngineCompartment ec, Cabin c, List ws);\rEngineCompartment(Engine e, Transmission t,\r            Steering s, Battery b);\rCabin(Door driverDoor, Door passengerDoor,\r            Seat driverSeat, Seat passengerSeat);\rEngine(float dissplacement, int pistonCount);\rBattery(float voltage);\rDoor(Window window);\r                new Door(new PowerWindow()),\r              new Door(new PowerWindow()),\r              new PowerSeat(), new PowerSeat()PowerWindow() implements Window;\rPowerSeat() implements Seat;\rWheel(Tire tire, Rim rim);\r...      Imagine you could just ask for things. Lets start simple and look at the Wheel. The constructor of Wheel needs a Tire and Rim. So when we ask for a Wheel it should be obvious that we want new Wheel(new Tire(), new Rim()). Why do we need to make this explicit in our factory? Lets build a framework from which we can ask for a class and it returns an instance of that class. So in our case if we ask for getInstance(Wheel.class) it returns a new Wheel(new Tire(), new Rim()). Now a framework like this is easy to build since all we need to do is look at the constructor and recursively try to instantiate the objects until all recursive constructors are satisfied.\rBut things are a bit more complicated than that. What if we ask for Cabin, as in getInstance(Cabin.class)? Well Cabin needs two Doors and two Seats, but Seat is an interface so we have to make a decision: What subclass of Seat should we instantiate? To help our framework make that decision, somewhere we will add a bind method such as bind(Seat.class, PowerSeat.class). Great! Now when we call getInstance(Seat.class) the framework returns new PowerSeat(). Similarly, we will have to call bind(Window.class, PowerWindow.class). Now we can call getInstance(Cabin.class) and the framework will return new Cabin(new Door(new PowerWindow()), new Door(new PowerWindow()), new PowerSeat(), new PowerSeat()).\rNotice that closer a class you ask for is to the root (Car in this case), the more work will the framework do for us. So ideally we just want to ask for the root object, Car. Calling getInstance(Car.class) will cause the framework to do all of the work originally in our factory.\rAs you can see a framework which will call the new operators on your behalf is very useful. This is because you only have to ask for the root object (in our case the Car) and the framework will build the whole object graph on your behalf. This kinds of frameworks are called Automatic Dependency Injection frameworks and there are few of them our there. Namely GUICE, PicoContainer, and Spring.\rSince I know most about GUICE, The above example can be rewritten in GUICE like this:\r class CarModule extends AbstractModule() {\rpublic void bind() {\r  bind(Seat.class, PowerSeat.class);\r  bind(Seat.class, PowerSeat.class);\r  bind(Transmission.class, ManualTransmission.class);\r  // maybe use a provider method?(jwolter)\r  // maybe explain the need for different wheels, so use a Provider;\r  bind(new TypeLiteral>(){})\r    .toProvider(new Provider>(){\r      @Inject Provider wp;\r      List get() {\r        return Array.asList(wp.get(), wp.get(),\r                            wp.get(), wp.get());\r      }\r    });\r}\r\r// or, what I think is simpler and clearer\r@Provides\rList provideWheels(Provider wp) {\r  return Array.asList(wp.get(), wp.get()\r                      wp.get(), wp.get());\r}\r}\r// Then somewhere create your application, using the injector only\r//   once, for the root object.\rInjector injector = Guice.createInjector(new CarModule());\rCar car = injector.getInstance(Car.class);As you can see Automatic Dependency Injection frameworks can do a lot of things for you. Namely, that you don't have to worry about writing the factories. You simply declare dependencies, and write your application logic. As needed, you ask for your dependencies in a constructor and let the framework resolve all of them for you. You move the responsibility of calling the new operator to the framework. The DI-framework is your new \"new\". Now DI-frameworks can do lot of other things, which are beyond the scope of this article, such as manage object lifetimes, enforce singletons (in a good way, see: Root Cause of Singletons and Singletons are Pathological Liars) and manage different configurations of your application such as production vs development server.\r For a more real life example of a Guice Module see: CalculatorServerModule.java  Also, the curious may wonder why a Provider is injected into the provider method for List<Wheel>. This is because our 4 wheeled car needs to have different wheels. If we injected a single wheel, it would be assigned as the same wheel on all 4 positions. Providers, when used without any explicit scopes will return a new instance every time get() is called on them.", "[A light hearted episode this week... but still with a serious message.  Enjoy.  -Dave]HALP!  Mah unit tests be doin' too much I/O!  Testin' this lil' codes uses MOAR RESOURCES!GIMME lol_io LIKE LOLIOSO IM LIKE PROCESSIN WIT DATAZ OK?\u00a0\u00a0GIMME EACH BUCKET IN UR DATAZ OK?\u00a0\u00a0\u00a0\u00a0BUCKET OWN FUBARRED?\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0N CAN HAS NONE\u00a0\u00a0\u00a0\u00a0NOPE?\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0N CAN HAS 1\u00a0\u00a0KTHXBYE NIZ __name__ KINDA LIKE \u201c__main__\u201d?\u00a0\u00a0UR PROCESSIN WIT LOLIO OWN GET_SOME_DATAZ\u00a0\u00a0BTW, GET_SOME_DATAZ USES UR INTERNETS LOLOh NOES!  Usin' internets in ur unit testz?  Don't clog the tubes!  Is not big truck!  Mock the LOLIO thingy.  No moar tubes!GIMME mock_lol_io LIKE LOLIO BTW, GIMME THING TO TESTBTW, TEST THE THING NOW KTHXNow ur test runs fast!  You can use mock_lol_io for killin' nondeterminism, too like for  exceptions n stuff.  Is fun, makes ur code execute pathz it nevar seen b4.  Wit dis, you can see wut happens when theres a OH NOES like the tubez bein clogged.BTW, SOMETIMES THEY BE CALLIN DIS DEPENDENCY INJECTION ROFLBTW, YOU CAN UZE MOCKZ N STUF FER DIS LOOK:IN MAI library GIMME mock_filesystem LIKE LOL_FAKE_FILEYSTEMBTW, NOW U CAN USE LOL_FAKE_FILESYSTEM TO MAKE FAKE FILEZ IN MEMORY N STUFFBTW, IS FASTER THAN OPENIN FILEZ ON TEST SERVAR Now U know the sekrit for faster tests.  Shh, don't tell Microsawft or the Yahew.  They might be in our base, stealin our tech!KTHXBYE!Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Marc Kaplan, Test Engineering ManagerWhen looking at whether to check a new change in, there are several testing-related questions that can be asked, which at least include:1. Does the new functionality work?2. Did this change break existing functionality?3. What is the performance impact of this change?We can either answer these questions before we check things in, or after. We found that we can save a lot of time and effort in trying to do detective work and track bad changes down if we do this before check in to the depot. In most teams developers kick off the tests before they check anything in, but in case somebody forgets, or doesn't run all of the relevant tests, a more automated system was desired. So we have what we call presubmit systems at Google to run all of the relevant tests in an automated fashion pre-checkin, without the developer doing anything before the code is actually checked in. The idea behind this is that once the code is finished being written there are minutes or hours before it's checked in while it's being reviewed via the Google code review process (more info on that at Guido's Tech Talk on Mondrian). So we have taken advantage of this time by running tests via the presubmit system that finds all of the tests that are relevant to the change, runs the tests, and reports the results. Most of the work around presubmit at Google has been looking at functionality (question 1, and 2 above), however in the GFS Team where we are making a lot of performance-related changes we wondered whether a performance presubmit system would be possible. So recently we developed such a system and have begun to start using it. Basically a hook was added to the code review process such that once a developer sends out a change for review, and automated performance test is started via the GFS Performance Testing Tool that we previously described on another Testing Blog post. Once two runs of the performance test are finished, they are compared against a known-good baseline that gets automatically created at the beginning of each week, and a comparison graph is generated and e-mailed to the user submitting the CL. So now we know, prior to checking some code in if it has some unexpected hit to performance. Additionally, now when a developer wants to make a change they think will improve performance they don't need to do anything other than a normal code review which will trigger an automatic performance test to see if they got the performance bump they were hoping for.Example of a performance presubmit report:I've seen many companies and projects where performance testing is cumbersome and not run very often until near release date. As you might expect, this is very problematic because once you find a performance bug you have to go back through all of the changes and try to narrow it down, and many times it's simply not that easy. Doing the performance testing early, and often helps narrow things down, but we've found that it's even better to make sure that these performance regressions never creep into the code with pre-check in performance testing. Certainly you should still do a final performance test once this code is frozen, but with the presubmit performance testing system in place you are essentially certain that the performance will be as good as you expect it to be.", "The Google Maps API is one of our most popular developer products here at Google, and is also one of the trickiest to test because of its visual nature and diverse uses. It has to go through the standard backend and JsUnit testing, but then also through a suite of Selenium tests to make sure that DOM elements are positioned correctly, mouse events are triggered correctly, and even tests to address random bugs like Issue 517, where IE errored out when an implicitly closed base tag was used in the developer's HTML. And to top it off, all those tests must pass in every one of the API's supported browsers.So, we decided to open source our selenium test suite so that developers could see the kind of integration tests we're currently running, and could even contribute their own tests that test specific functionality or an order of operations used on their site. What better way to make sure your site works with our API than contribute a test to make sure of it?You can read more about the suite on our blog, peruse through our tests, and if you're a Maps API developer yourself, send us a test or two!", "By Mi\u0161ko Hevery(the other title: Your Application has a Wiring Problem)In My main() Method Is Better Than Yours we looked into what a main() method should look like. There we introduced a clear separation between (1) the responsibility of constructing the object graph and (2) the responsibility of running the application. The reason that this separation is important was outlined in How to Think About the \u201cnew\u201d Operator. So let us look at where have all of the new operators gone...Before we go further I want you to visualize your application in your mind. Think of the components of your application as physical boxes which need to be wired together to work. The wires are the references one component has to another. In an ideal application you can change the behavior of the application just by wiring the components differently. For example instead of instantiating LDAPAuthenticator you instantiate KerberosAuthenticator and you wire the KerberosAuthenticator to appropriate components which need to know about Authenticator. That is the basic idea. By removing the new operators from the application logic you have separated the responsibility of wiring the components from the application logic, and this is highly desirable. So now the problem becomes, where have all the new operators gone?First lets look at a manual wiring process. In the main() method we asked the ServerFactory to build us a Server (in our case a Jetty Web Server) Now, server needs to be wired together with servlets. The servlets, in turn, need to be wired with their services and so on. Notice that the factory bellow is full of \"new\" operators. We are new-ing the components and we are passing the references of one component to another to create the wiring. This is the instantiation and wiring activity I asked you to visualize above. (Full source):  public Server buildServer() {  Server server = new Server();  SocketConnector socketConnector       = new SocketConnector();  socketConnector.setPort(8080);  server.addConnector(socketConnector);  new ServletBuilder(server)    .addServlet(\"/calc\", new CalculatorServlet(                         new Calculator()))    .addServlet(\"/time\", new TimeServlet(                         new Provider() {      public Date get() {        return new Date();      }    }));  return server;}When I first suggest to people that application logic should not instantiate its own dependencies, I get two common objections which are myths:\"So now each class needs a factory, therefore I have twice as many classes!\" Heavens No! Notice how our ServerFactory acted as a factory for many different classes. Looking at it I counted 7 or so classes which we instantiated in order to wire up our application. So it is not true that we have one to one correspondence. In theory you only need one Factory per object lifetime. You need one factory for all long-lived objects (your singletons) and one for all request-lifetime objects and so on. Now in practice we further split those by related concepts. (But that is a discussion for a separate blog article.) The important thing to realize is that: yes, you will have few more classes, but it will be no where close to doubling your load.\"If each object asks for its dependencies, than I will have to pass those dependencies through all of the callers. This will make it really hard to add new dependencies to the classes.\" The myth here is that call-graph and instantiation-graph are one and the same. We looked into this myth in Where have all the Singletons Gone. Notice that the Jetty server calls the TimeServlet which calls the Date. If the constructor of Date or TimeServlet all of a sudden needed a new argument it would not effect any of the callers. The only code which would have to change is factory class above. This is because we have isolated the instantiation/wiring problem into this factory class. So in reality this makes it easier to add dependencies not harder.Now there are few important things to remember. Factories should have no logic! Just instantiation/wiring (so you will probably not have any conditionals or loops). I should be able to call the factory to create a server in a unit test without any access to the file-system, threads or any other expensive CPU or I/O operations. Factory creates the server, but does not run it. The other thing you want to keep in mind is that the wiring process is often controlled by the command line arguments. This makes is so that your application can behave differently depending what you pass in on a command line. The difference in behavior is not conditionals sprinkled throughout your code-base but rather a different way of wiring your application up.Finally, here are few thoughts on my love/hate of Singletons (mentioned here and here) First a little review of singletons. A singleton with a lower case 's' is a good singleton and simply means a single instance of some class. A Singleton with an upper case 'S' is a design pattern which is a singleton (one instance of some class) with a global \"instance\" variable which makes it accessible from anywhere. It is the global instance variable which makes it globally accessible , which turns a singleton into a Singleton. So singleton is acceptable, and sometimes very helpful for a design, but Singleton relies on mutable global state, which inhibits testability and makes a brittle, hard to test design. Now notice that our factory created a whole bunch of singletons as in a single instance of something . Also notice how those singletons got explicitly passed into the services that needed them. So if you need a singleton you simply create a single instance of it in the factory and than pass that instance into all of the components which need them. There is no need for the global variable.For example a common use of Singleton is for a DB connection pool. In our example you would simply instantiate a new DBConnectionPool class in the top-most factory (above) which is responsible for creating the long-lived objects. Now lets say that both CalculatorServlet and TimeServlet would need a connection pool. In that case we would simply pass the same instance of the DBConnectionPool into each of the places where it is needed. Notice we have a singleton (DBConnectionPool) but we don't have any global variables associated with that singleton.  public Server buildServer() {  Server server = new Server();  SocketConnector socketConnector       = new SocketConnector();  socketConnector.setPort(8080);  server.addConnector(socketConnector);  DBConnectionPool pool = new DBConnectionPool();  new ServletBuilder(server)    .addServlet(\"/calc\", new CalculatorServlet(                         pool,                         new Calculator()))    .addServlet(\"/time\", new TimeServlet(                         pool,                         new Provider() {      public Date get() {        return new Date();      }    }));  return server;}", "Posted by Philip ZembrodSo the Test-Driven-Development and Extreme-Programming people tell you you should write your tests even before you write the actual code. \"Now this is taking things a bit too far,\" you might think. \"To the extreme, even. Why would I want to do this?\"In this post, I'll tell you my answer to this question. I now really do want to write my tests first...and here's why!After many years of writing code without using or writing unit tests, I took a colleague's advice and read Kent Beck's \"Extreme Programming Explained.\" I picked \"write tests first\" as the first XP practice to try out in my daily coding.The practice is: Write a failing test for each feature you plan to implement. Run the test and see it fail. Then implement the feature until the test succeeds. Refactor now and begin again.Why write the test first? The obvious reason, I thought, was to make it more likely that tests will get written at all. But I heard the promise that this was not just a way to ensure tests aren't overlooked, but a way to higher productivity. I tried it, and found that getting tests written was indeed one of the less important reasons to write tests first!Writing tests firsts leads you to think about the interface first. Of course, you do that anyway when you write the header file with the C++ class definition or when you write a Java interface before you implement any methods. However, writing a test lets you focus on how the new interface will be used before even writing the interface. You could call writing the interface the supply side and writing the test the demand side of the deal. Writing the test first, you set out with the customer's or user's view of the new class.Another way of seeing the same thing is to regard the test as a coded specification. In the test, you specify what service the new class or feature should provide, and you specify, by example, the syntax with which this service will be requested. In contrast to specifications written in natural language, a specification written into a test contains a technical safeguard against growing stale: if it does, the test will probably fail.These two aspects of unit tests are enough to make me feel excited about writing them first. Tests are no longer a necessary chore, but the place and time where I start to design something new. That's what I love to do. How soon can I get started writing my next test?But this is still not the best part: If I write a test first, run it to see it fail (often even fail to compile), and write the code to satisfy the test, then I have everything in place to see my code running the minute it is written and compiled! No more dread of strange behaviour or system crashes the first time I launch the system with my new code! No more laborious navigating through the application to my code's feature! No more wondering: Did my code actually get executed or not?Just a quick run-the-testcase, and I know how my code runs: green - good. Red - not yet good. Read failure message and fix code until green. Debugging sucks, testing rocks, indeed!Of course, all the complex issues of integration and system testing remain. Good unit testing gives me a good head start for integration, but I might still be in for unpleasant surprises there.The point I want to make here, though, is about my state of mind when I write new code. For me, writing new code for complex systems was always accompanied by fear: fear of crashes I'd have to debug, fear of creating bugs I might not discover, fear of the dreary work of searching for bugs I might have created. Fear that took up a considerable amount of my mind space and slowed me down.Now, this fear is gone! I happily go about writing my code because I know the tests are already in place. It will cost me just a few keystrokes to run my finished code, and I will immediately see what it does. Hooray, I wrote a program, and it works, and it's easy to prove it!It's the same old enthusiasm that I felt more than 20 years ago when I wrote and ran my first programs. Many of you have felt it, too - the joy of inducing some life into this dead piece of hardware through our written word. And now this joy sits in my mind again where fear of crashes was before. You'd better believe that speeds up my coding! Want to give it a try yourself?", "By Mi\u0161ko HeveryPeople are good at turning concrete examples into generalization. The other way around, it does not work so well. So when I write about general concepts it is hard for people to know how to translate the general concept into concrete code. To remedy this I will try to show few examples of how to build a web application from ground up. But I can't fit all of that into a single blog post ... So lets get started at the beginning...Here is what your main method should look like (no matter how complex your application) if you are using GUICE: (src)public static void main(String[] args)  throws Exception {    // Creation Phase    Injector injector = Guice.createInjector(             new CalculatorServerModule(args));    Server server = injector.getInstance(Server.class);    // Run Phase    server.start();}Or if you want to do manual dependency injection: (src)public static void main(String[] args)  throws Exception {    // Creation Phase    Server server = new ServerFactory(args)                               .createServer();    // Run Phase    server.start();}The truth is I don't know how to test the main method. The main method is static and as a result there are no places where we can inject test-doubles. (I know we can fight static with static, but we already said that global state is bad here, here and here). The reason we can't test this is that the moment you execute the main method the whole application runs, and that is not what we want and there is nothing we can do to prevent that.But the method is so short that I don't bother testing it since it has some really cool properties: Notice how the creation-phase contains the code which builds the object graph of the application. The last line runs the application. The separation is very important. We can test the ServerFactory in isolation. Passing it different arguments and than asserting that the correct object graph got built. But, in order to do that the Factory class should do nothing but object graph construction. The object constructors better do nothing but field assignments. No reading of files, starting of threads, or any other work which would cause problems in unit-test. All we do is simply instantiate some graph of objects. The graph construction is controlled by the command line arguments which we passed into the constructor. So we can test creation-phase in isolation with unit-test. (Same applies for GUICE example) The last line gets the application running. Here is where you can do all of your fun threads, file IO etc code. However, because the application is build from lots of objects collaborating together it is easy to test each object in isolation. In test I just instantiate the Server and pass in some test doubles in the constructor to mock out the not so interesting/hard to test code.As you can see we have a clear separation of the object graph construction responsibility from the application logic code. If you were to examine the code in more detail you would find that all of the new operators have migrated from the run-phase  to creation-phase (See How to Think About the \u201cnew\u201d Operator) And that is very important. New operator in application code is enemy of testing, but new in tests and factories is your friend. (The reason is that in tests we want to use test-doubles which are usually a subclass or an implementation of the parent class. If application code calls new than you can never replace that new with a subclass or different implementation.) The key is that the object creation responsibility and the the application code are two different responsibilities and they should not be mixed. Especially in the main method!A good way to think about this is that you want to design your application such that you can control the application behavior by controlling the way you wire the objects together (Object collaborator graph). Whether you wire in a InMemory, File or Database repository, PopServer or IMAPServer, LDAP or file based authentication. All these different behaviors should manifest themselves as different object graphs. The knowledge of how to wire the objects together should be stored in your factory class. If you want to prevent something from running in a test, you don't place an if statement in front of it. Instead you wire up a different graph of objects. You wire NullAthenticator in place of LDAPAuthenticator. Wiring your objects differently is how the tests determines what gets run and what gets mocked out. This is why it is important for the tests to have control of the new operators (or putting it differently the application code does not have the new operators). This is why we don't know how to test the main method. Main method is static and hence procedural. I don't know how to test procedural code since there is nothing to wire differently. I can't wire the call graph different in procedural world to prevent things from executing, the call graph is determined at compile time.In my experience that main method usually is some of the scariest code I have seen. Full of singleton initialization and threads. Completely untestable. What you want is that each object simply declares its dependencies in its constructor. (Here is the list of things I need to know about) Then when you start to write the Factory it will practically write itself. You simply try to new the object you need to return, which declares its dependencies, you in turn try to new those dependencies, etc... If there are some singletons you just have to make sure that you call the new operator only once. But more on factories in our next blog post...", "When writing a unit test, it is tempting to exercise many scenarios by writing a data-driven test. For example, to test the function IsWord, you could write (ARRAYSIZE is a macro that returns the number of elements in an array):const struct {const char* word; bool is_word;} test_data[] = {\u00a0\u00a0{\"milk\", true}, {\"centre\", false}, {\"jklm\", false},};TEST(IsWordTest, TestEverything) {\u00a0\u00a0for (int i = 0; i < ARRAYSIZE(test_data); i++)\u00a0\u00a0\u00a0\u00a0EXPECT_EQ(test_data[i].is_word, IsWord(test_data[i].word));}This keeps the test code short and makes it easy to add new tests but makes it hard to identify a failing test assertion (and to get the debugger to stop in the right place). As your code grows the test data tends to grow faster than linearly. For example, if you add a parameter called locale to IsWord, the test could become: Locale LOCALES[] = { Word::US, Word::UK, Word::France, ... };const struct {const char* word; bool is_word[NUM_LOCALES];} data[] = {\u00a0\u00a0{\"milk\", {true, true, false, ...}},  // one bool per language\u00a0\u00a0{\"centre\", {false, true, true, ...}}, \u00a0\u00a0{\"jklm\", {false, false, false, ...}}}; TEST(IsWordTest, TestEverything) {\u00a0\u00a0for (int i = 0; i < ARRAYSIZE(data); i++) \u00a0\u00a0\u00a0\u00a0for (int j = 0; j < ARRAYSIZE(LOCALES); j++) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0EXPECT_EQ(data[i].is_word[j], IsWord(data[i].word, LOCALES[i]));}The change was relatively easy to make: change the data structure, fill in the boolean values for other locales and add a loop to the test code. But even this small changed has made the test harder to read and slower as it repeats potentially unnecessary checks. In addition, both the test AND the code have changed. How do you know the test is not broken? (Actually, it is broken. Can you see the bug?)  By contrast, keeping the data in the test gives us: TEST(IsWordTest, IsWordInMultipleLocales) {\u00a0\u00a0EXPECT_TRUE(IsWord(\"milk\", Word::UK)); \u00a0\u00a0EXPECT_TRUE(IsWord(\"milk\", Word::US));\u00a0\u00a0EXPECT_FALSE(IsWord(\"milk\", Word::France));}TEST(IsWordTest, IsWordWithNonExistentWord) {   // 'jklm' test is not repeated \u00a0\u00a0EXPECT_FALSE(IsWord(\"jklm\", Word::US));            // as it uses the same code path}The difference between these two code snippets is minor but real-life data-driven tests quickly become unmanageable. A complete example would not fit on this page but if you look at your code base, you will find a few specimens lurking in some (not-so) forgotten test classes. They can be identified by their large size, vague names and the fact that they provide little to no information about why they fail.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by John Thomas and Markus Clermont\n\n\n\n\n\n\n\n\n\n\nThis is the second of a two part blog series titled 'Taming the Beast : How to test AJAX applications'. In part one we discussed some philosophies around web application testing. In this part we walk through a real example of designing a test strategy for an AJAX application by going 'beyond the GUI'.\n\nApplication under test\n\nThe sample application we want to test is a simple inventory management system that allows users to increase or decrease the number of parts at various store locations. The application is built using GWT (Google Web Toolkit) but the testing methodology described here could be applied to any AJAX application.\n\nTo quickly recap from part one, here's our recipe for testing goodness: \n\nExplore the system's functionality\nIdentify the system's architecture\nIdentify the interfaces between components\nIdentify dependencies and fault conditions\nFor each function\n\nIdentify the participating components\nIdentify potential problems\nTest in isolation for problems\nCreate a 'happy path' test\n\n\nLet's look at each step in detail:\n\n1. Explore the system's functionality\n\nSimple as this sounds, it is a crucial first step to testing the application. You need to know how the system functions from a user's perspective before you can begin writing tests. Open the app, browse around, click on buttons and links and just get a 'feel' of the app. Here's what our example app looks like:  \n\n\nThe app has a NavPane to filter the inventory by locations, list number of items in each location, increase/decrease the balance for items and sort the list by office and by product.\n\n\n2. Identify the architecture\nLearning about the system architecture is the next critical step. At this point think of the system as a set of components and figure out how they talk to each other. Design documents and architecture diagrams are helpful in this step. In our example we have the following components:  \n\nGWT client: Java code compiled into JavaScript that lives in the users browser. Communicates with the server via HTTP-RPC \nServlet: standard Apache Tomcat servlet that serves the \"frontend.html\" (main page) with the injected JavaScript and also serves RPCs to communicate with the client-side JavaScript.  \nServer-side implementation of the RPC-Stubs: The servlet dispatches the RPC over HTTP calls to this implementation. The RPCImpl communicates with the RPC-Backend via protocol-buffers over RPC \nRPC backend: deals with the business logic and data storage. \nBigtable: for storing data \n\nIt helps to draw a simple diagram representing the data flows between these components, if one doesn't already exist:   \n\nIn our sample application, the RPC-Implementation is called \"StoreService\" and the other RPC-Backend is called \"OfficeBackend\".\n3. Identify the interfaces between components \n\nSome obvious ones are: \n\ngwt_module target in Ant build file\n\"service\" servlet of Apache Tomcat\ndefinition of the RPC-Interface \nProtocol buffers\nBigtable\nUI (it is an interface, after all!)\n\n4. Identify dependencies and fault conditionsWith the interfaces correctly identified, we need to identify dependencies and figure out input values that are needed to simulate error conditions in the system.\n\nIn our case the UI talks to the servlet which in turn talks to StoreService (RPCImpl). We should verify what happens when the StoreService:  \n\nreturns null\nreturns empty lists\nreturns huge lists\nreturns lists with malformed content (wrongly encoded, null or long strings)\ntimes out\ngets two concurrent calls\n\nIn addition the RPCImpl (StoreService) talks to the RPC-Backend (OfficeAdministration). Again we want to make sure the proper calls are made and what happens when the backend:  \n\nreturns malformed content\ntimes out\nsends two concurrent requests \nthrows exceptions\n\nTo achieve these goals, we will want to replace the RPCImpl (StoreService) with a mock that we can control, and have the servlet talk to the mock. The same is true for the OfficeAdministration - we will want to replace the real RPCBackend with a more controllable fake, and have StoreService communicate with the mock instead.\n\nTo get a better overview, we will first look at individual use-cases, and see how the components interact. An example would be the filter-function in the UI (only those items under a 'checked' in a checked-location in the NavPane will be displayed in the table).Analyze the NavPane filter  \n\nClient\n\n\nGets all offices from RPC\nOn select, fetch items with RPC. On completion, update table.\nOn deselect, clear items from table.\n\n\n\nRPCImpl\n\n\nGets all offices from RPC-Backend \nFetches all stock for an office from RPC-Backend\n\n\n\nRPC-Backend   \n\n\nScan bigtable for all offices\nQuery stock for a given office from bigtable.   \n\n\nOur next step is to figure out the \"smallest test\" that can give us confidence that each of the components works as expected.Test client-side behaviorMake sure that de-selecting an item removes it. For that, we need to be sure what items will be in the list. A fake RPCImpl could do just that - independent of other tests that might use the same datasource.\n\nThe task is to make the Servlet talk to the \"MockStoreService\" as RPCImpl. We have different possibilities to achieve that:  \n\nIntroduce a flag to switch\nUse the proxy-pattern\nSwitch it at run time\nAdd a different constructor to the servlet\nIntroduce a different build-target that links to the fake implementation\nUse dependency injection to swap out real for fake implementations \n\nAny one of these options would do the job depending on the application. Solutions like adding a new constructor to the servlet would need production code to depend on test code, which is obviously a bad idea. Switching implementations at run time (using class loader trickery) is also an option but could expose security holes. Dependency injection offers a flexible and efficient way to do the same job without polluting production code.\n\nThere are various frameworks to allow this form of dependency injection. We want to briefly introduce GuiceBerry as one of them.\n\nGuiceberry is a framework to allow you to use a composition model for the services your test needs. In other words, if your test depends on certain services you can have those services \"injected\" into your tests using a popular dependency injection tool called Guice.\n\nIn our example we need to annotate the RPCImpl object with \"@Inject\" in the servlet test class and create an alternate implementation called MockStoreService to swap in at run time. Here's a code snippet that shows how:\n\n@GuiceBerryEnv(StoreGuiceBerryEnvs.NORMAL)public class StorePortalTest extends TestCase {\n\n@InjectStoreServiceImpl storeService;\n\npublic void testStorePortal() {...  storeService.doSomething();...}\n}\n\n\n\nIn the code snippet above, note the lines marked in bold. That's Guiceberry magic that allows us to inject a StoreServiceImpl object into the StorePortalTest class. The construction of the StoreServiceImpl is done inside a Guiceberry environment class called NormalStoreGuiceBerryEnvs (and linked to StorePortal via the StoreGuiceBerryEnvs class). To inject a mock RPCImpl into StorePortalTest we would need to create a MockStoreGuiceBerryEnvs (which would instantiate a mock StoreService) and swap that for NormalStoreGuiceBerryEnvs at run time. All we need to do is to specify the following JVM args for the test ...\n\nJVM_ARGS=\"-DNormalStoreGuiceBerryEnvs=MockStoreGuiceBerryEnvs\" \n\nThis is just a quick peek at how Guiceberry works. Go to the official Guiceberry website to learn more.\n\nThis will be enough to decouple the client from the rest of the system. GwtTestCase does the rest of the job on the client side. You find more details here. Don't forget to inject all possible failure scenarios through the MockStoreService.\n\nLet's see what we found out so far:  \n\nWe know that\n\nUI callbacks work correctly\nInteraction UI - Frontend works fine\nExpected errors are handled adequately by the UI \n\nWe don't know whether\n\n\nthings are rendered correctly\nthings we expect to be on a page are really there \n\n\nAlthough we already found out quite a lot about the UI, it is too early to be confident that the client works as expected. We need to know more about the UI to be able to answer the remaining two questions.\n\nThis is where some more traditional techniques, namely automated UI tests, enter the stage.   \n\nAdd JavaScript hooks into the page, that return the elements (JSNI is the way to go here) \nUse Selenium for UI tests (using both the hooks and the MockStoreService). All we need to do is check whether\n\nthe elements exist  \nall the buttons (which need to be clicked on) are clickable \nscrollbars are added when needed\n\n\nWe don't need to do more work here - the GwtTestcase helped us to determine that that the \"Model\" and the \"Controllor\" work properly. All we needed to look at is the \"View\".\n\nOne problem we have often had with Selenium tests in the past was that people relied overly on XPath queries to retrieve the elements from webpages. Of course, when the DOM changed it caused many tests to break. One way to work around that is to introduce JavaScript hooks. They are only added when the application runs with a special \"testing\" flag and they directly return the elements needed.\n\nYou might wonder why this approach is any better? Well for one, we can catch problems earlier, and fix them without even looking at the tests that use them. A small and fast JsUnit test can be used to determine whether a hook is broken. If so, it is only a line of code to fix the problem.\n\nLet's review what we have found out so far:  \n\nWe know that\n\nUI callbacks work correctly\nInteraction UI - Frontend works fine\nExpected errors are handled adequately by the UI\nThings are rendered appropriately\nDOM is correct  \n\nWe don't know whether\n\nOther (non-UI) components work as expected         \n\n\n\n\nTest the StoreService (RPCImpl)The methods in StoreService (RPCImpl) need a lot of good unit testing. If we write a good amount of unit tests, we probably already have a MockOfficeAdministration (RPC-Backend) that we can use for our further testing efforts.\n\nThe main value we can add here is to verify that (1) each interface method in the StoreService behaves correctly, even in the face of communication errors with the RPC-Backend and (2) each method behaves as expected. By using a MockOfficeAdministration as RPC-Backend, we don't have to worry about setting up the data (plus injecting faults is easy!)\n\nBesides testing the basic functionality, e.g.  \n\nAre all the records that we expect retrieved\nAre no records that shouldn't be retrieved passed on to the caller\nDoes the application behave correctly, even if no records are found\n\n... we can now also look at   \n\nMalformed or Unexpected data\nToo much data\nEmpty replies\nExceptions \nTime-outs\nConcurrency problems \n\nHow can we replace our real RPC-Backend with the mock? That shouldn't be all that difficult, as using an RPC mechanism already forced us to define interfaces for the server. All we need to do is implement a mock-RPC-Backend and run that instead. You might want to consider running the mock-RPC-Backend on the same machine as the tests, to make your tests run faster.\n\nSome example test cases at this level are:  \n\n\nRetrieve the list of all offices Let the mock-RPC-Backend \n\nreturn no office\nreturn 100 offices, 1 malformed encoded\nreturn 100 offices, 1 null\n...\nthrow an exception\ntime out\n\nRetrieve product / stock for an office Let the mock-RPC-Backend stubby return\n\n...\n\nRetrieve a product for an office Let the mock-RPC-Backend block, and\n\n issue a second query for the same product at the same time (and to make it more interesting, play with the results that the mock could return!). \n\n....\n\nLet's see what we have found out so far:  We know that \n\nthe UI works in isolation as expected\nthe StoreService (RPCImpl) appropriately invokes the RPC-Backend-Service \nthe StoreService (RPCImpl) properly handles any error-conditions\nA little bit about the app's behavior under concurrency\n\n\n\nWe don't know whether \n\nthe RPC-Backend-Service really expects the behavior the StoreServiceImpl exposes.\n\nIt is easy to see that we can do the same excercise for OfficeAdministration (RPC-Backend) and possibly use a MockBigtable implementation.  After that, we would know that: \n\nBackend correctly reads from Bigtable\nBusiness logic in the backend works correctly\nBackend knows how to handle error-conditions\nBackend knows how to handle missing data\n\nWe don't know whether \n\nBackend is used correctly, i.e. in the way it is intended to be used\n\nTest the OfficeAdministration (RPC-Backend) and StoreService (RPCImpl)Now let us verify the interaction between OfficeAdministration (RPC-Backend) and StoreService (RPCImpl). This is an essential task, and is not really that difficult. The following points should make testing this quick and easy:  \n\n\nEasy to test (through Java API)\nEasy to understand\nIdeally contains all the business logic\nAvailable early\nExecutes fast (MockBigtable is an option here)\nMaintenance burden is low (because of stable interfaces)\nPotentially subset of tests as for StoreService (RPCImpl) alone \n\n\n\nLet's see what we have found out so far:  We know that \n\nthe UI works in isolation as expected\nthe OfficeAdministration (RPC-Backend) and the StoreService (RPCImpl) work together as expected \n\nWe don't know whether \n\n\nThe results find their way to the user     \n\n\n\nLast but not the least ... system test!Now we need to plug all the components together and do the 'big' system test. In our case, a typical set up would be: \n\nManipulate the \"real\" Bigtable and populate with \"good\" data for our test\n\n5 offices, each with 5 products and each with a stock of 5\n\nUse Selenium (with the hooks) to\n\nNavigate via the Navbar\nExclude an item\nAdd an item\n... \n\n\n\nWe now know that all components plugged together can handle one typical use case. We should repeat this test for each function that we can invoke through the UI.\n\nThe biggest advantage, however, is that we just need to look for communication issues between all 3 building blocks. We don't need to verify boundary cases, inject network errors, or other things (because we have already verified that earlier!)\n\n\nConclusionOur approach requires that we \n\nUnderstand the system\nUnderstand the platform\nUnderstand what can go wrong (and where)\nStart early with our tests\nInvest in infrastructure to run our tests (mocks, fakes, ...)\n\n\n\nWhat we get in return is \n\nFaster test execution\nLess maintenance for the tests\n\nshared ownership\nearly execution > early breakage > easy fix\n\nShorter feedback loops\nEasier debugging / better localization of bugs due to fewer false negatives.", "by Mi\u0161ko HeverySince I have gotten lots of love/hate mail on the Singletons are Pathological Liars and Where Have All the Singletons Gone I feel obliged to to do some root cause analysis.Lets get the definition right. There is Singleton the design pattern (Notice the capital \"S\" as in name of something) and there is a singleton as in one of something (notice the lower case \"s\"). There is nothing wrong with having a single instance of a class, lots of reasons why you may want to do that. However, when I complain about the Singletons, I complain about the design pattern. Specifically: (1) private constructor and (2) global instance variable which refers to the singleton. So from now on when I say Singleton, I mean the design (anti)pattern.I would say that at this point most developers recognize that global state is harmful to your application design. Singletons have global instance variable which points to the singleton. The instance is global. The trouble with global variables is that they are transitive. It is not just the global variable marked with static which is global but any other variable/object which is reachable by traversing the object graph. All of it is global! Singletons, usually are complex objects which contain a lot of state. As a result all of the state of Singleton is global as well. I like to say that \"Singletons are global state in sheep's clothing.\" Most developers agree that global state is bad, but they love their Singletons.The moment you traverse a global variable your API lies about its true dependencies (see: Singletons are Pathological Liars) The root problem is not the Singleton design pattern, the root problem here is the global reference to singleton. But the moment you get rid of the global variable you get rid of the Singleton design pattern. So from my point of view blaming Singletons or blaming global state is one and the same. You can't have a Singleton design pattern and at the same time not have the global state.Someone pointed out that any design pattern can be abused. I agree, but with Singleton design pattern, I don't know how I can possibly use it in a good way. The global reference and hence the global state is ever so present. Now, in my line of work I don't see too much global state in classical sense of the word, but I see a lot of global state masquerading as Singletons. Hence, I complain about Singletons. If I would complain about global state no one would care, as that is old news.Now, there is one kind of Singleton which is OK. That is a singleton where all of the reachable objects are immutable. If all objects are immutable than Singleton has no global state, as everything is constant. But it is so easy to turn this kind of singleton into mutable one, it is very slippery slope. Therefore, I am against these Singletons too, not because they are bad, but because it is very easy for them to go bad. (As a side note Java enumeration are just these kind of singletons. As long as you don't put state into your enumeration you are OK, so please don't.)The other kind of Singletons, which are semi-acceptable are those which don't effect the execution of your code, They have no \"side effects\". Logging is perfect example. It is loaded with Singletons and global state. It is acceptable (as in it will not hurt you) because your application does not behave any different whether or not a given logger is enabled. The information here flows one way: From your application into the logger. Even thought loggers are global state since no information flows from loggers into your application, loggers are acceptable. You should still inject your logger if you want your test to assert that something is getting logged, but in general Loggers are not harmful despite being full of state.So the root cause is \"GLOBAL STATE!\" Keep in mind that global state is transitive, so any object which is reachable from a global variable is global as well. It is not possible to have a Singleton and not have a global state. Therefore, Singleton design patter can not be used in \"the right way.\" Now you could have a immutable singleton, but outside of limited use as enumerations, they have little value. Most applications are full of Singletons which have lots of global state, and where the information flows both directions.", "You've got some code that uses threads, and it's making your tests flaky and slow. How do you fix it? First, most of the code is probably still single-threaded: test those parts separately. But how to test the threading behavior itself?Often, threaded tests start out using sleeps to wait for something to happen. This test is trying to verify that DelegateToIntern spawns its work argument into a parallel thread and invokes a callback when it's done.def testInternMakesCoffee(self):\u00a0\u00a0self.caffeinated = False\u00a0\u00a0def DrinkCoffee(): self.caffeinated = True\u00a0\u00a0DelegateToIntern(work=Intern().MakeCoffee, callback=DrinkCoffee)\u00a0\u00a0self.assertFalse(self.caffeinated, \"I watch YouTubework; intern brews\")\u00a0\u00a0time.sleep(60)  # 1min should be long enough to make coffee, right?\u00a0\u00a0self.assertTrue(self.caffeinated, \"Where's mah coffee?!?\")Aside from abusing your intern every time you run the test, this test takes a minute longer than it needs to, and it may even fail when the machine (or intern!) is loaded in odd ways. You should always be skeptical of sleep statements, especially in tests. How can we make the test more reliable?The answer is to explicitly control when things happen within DelegateToIntern with a threading.Event in Python, a Notification in C++, or a CountDownLatch(1) in Java.def testInternMakesCoffee(self):\u00a0\u00a0is_started, can_finish, is_done = Event(), Event(), Event()\u00a0\u00a0def FakeCoffeeMaker():\u00a0\u00a0\u00a0\u00a0is_started.set()  # Allow is_started.wait() to return.\u00a0\u00a0\u00a0\u00a0# Wait up to 1min for can_finish.set() to be called. The timeout\u00a0\u00a0\u00a0\u00a0# prevents failures from hanging, but doesn't delay a passing test.\u00a0\u00a0\u00a0\u00a0can_finish.wait(timeout=60)  # .await() in Java\u00a0\u00a0DelegateToIntern(work=FakeCoffeeMaker, callback=lambda:is_done.set())\u00a0\u00a0is_started.wait(timeout=60)\u00a0\u00a0self.assertTrue(is_started.isSet(), \"FakeCoffeeMaker should have started\")\u00a0\u00a0self.assertFalse(is_done.isSet(), \"Don't bug me before coffee's made\")\u00a0\u00a0can_finish.set()  # Now let FakeCoffeeMaker return.\u00a0\u00a0is_done.wait(timeout=60)\u00a0\u00a0self.assertTrue(is_done.isSet(), \"Intern should ping when coffee's ready\")Now we're guaranteed that no part of the test runs faster than we expect, and the test passes very quickly. It could run slowly when it fails, but you can easily lower the timeouts while you're debugging it.We'll look at testing for race conditions in a future episode.No interns were harmed in the making of this TotT.Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko HeveryIn Singletons are Pathological Liars we discussed the problems of having singletons in your code. Let's build on that and answer the question \"If I don't have singletons how do I ensure there is only one instance of X and how do I get X to all of the places it is needed?\"An OO application is a graph of objects. There are three different kinds of graphs I think of when I design an application Collaborator Graph: This is the graph of objects that would be emitted if you serialized your application. This shows which objects are aware of which others. (through object's fields) Construction Graph: This graph shows which object created which other ones. Call Graph: This graph shows which other methods each method calls. A stack-trace would be a single slice through this graph.If the new operators are mixed with application logic (see: How to Think About the new Operator) then the Constructor Graph and the Collaborator Graph tend to be one and the same. However, in an application which uses Dependency Injection the two graphs are completely independent. Lets have a closer look at our CreditCardProcessor example. Suppose this is our collaborator graph which we need to execute a request.The above shows the application collaborator graph. The letter (S/R) in the corner designates object lifetime; either Singleton or Request scope. Now, just to be clear, there is nothing wrong with having a single instance of a class. The problem arises only when the singleton is available through a global \"instance\" variable as in Singleton.getInstance().The HTTP request would come to AuthenticatorPage which would collaborate with Authenticator to make sure the user is valid and forward a valid request onto ChargePage which would then try to load the user from UserRepository and create the credit card transaction which would be processed by CrediCardProcessor. This in turn would collaborate with OfflineQueue to get the work done.Now, in order to have a testable codebase we have to make sure that we don't mix the object construction with application logic. So all of the above objects should rarely call the new operator (value objects are OK). Instead each of the objects above would declare its collaborators in the constructor. AuthenticatorPage would ask for ChargePage and Authenticator. ChargePage would ask for CreditCardProcessor and UserRepository. And so on. We have moved the problem of construction elsewhere.In our tests it is now easy to instantiate the graph of objects and substitute test-doubles for our collaborators. For example if we would like to test the AuthenticatorPage, we would instantiate a real AuthenticatorPage with mock ChargePage and mock Authenticator. We would than assert that a request which comes in causes appropriate calls on Authenticator and ChargePage only if authentication is successful. If the AuthenticatorPage were to get a reference to Authenticator from global state or by constructing it, we would not be able to replace the Authenticator with a test-double. (This is why it is so important not to mix object construction with application logic. In the unit-test what you instantiate is a sub-set of the whole application. Hence the instantiation logic has to be separate from application logic! Otherwise, it's a non-starter.)So now the problem is, how do we construct the graph of objects?In short we move all of the new operators to a factory. We group all of the objects of similar lifetime into a single factory. In our case all of the singletons end up in ApplicationFactory and all of the Pages end up in RequestFactory. The main method of our application instantiates an ApplicationFactory. When we call build() the ApplicationFactory in turn instantiates its list of objects (Database, OfflineQueue, Authenticator, UserRepository, CreditCardProcessor and RequestFactory). Because each of the objects declares its dependency, the ApplicationFactory is forced to instantiate the objects in the right order. In our case it must instantiate the Database first and than pass the reference to UserRepository and OfflineQueue. (The code will simply not compile any other way.)Notice that when we create a RequestFactory we must pass in references to the Authenticator, UserRepository and CreditCardProcessor. This is because when we call build() on RequestFactory it will try to instantiate AuthenticatorPage which needs the Authenticator. So we need to pass the Authenticator into the constructor of RequestFactory and so on.At run-time an HTTP request comes in. The servlet has a reference to RequestFactory and calls build(). The servlet now has a reference to the AuthenticatorPage and it can dispatch the request for processing.Important things to notice: Every object only has references to what it needs directly! No passing around of objects which are not directly needed by the code. There is no global state at all. Dependencies are obvious since each object only asks for what it needs. If an object needs a reference to a new dependency it simply declares it. This change only affects the corresponding factory, and as a result, it is very isolated. All of the new operators end up in the factories; application logic is devoid of new operators. You group all of the objects with the same lifetime into a single factory (If the factory gets too big you can break it up into more classes, but you can still think of it as a single factory) The problem of \"how do I ensure that I only have one of something\" is nicely sidestepped. You instantiate only a single ApplicationFactory in your main, and as a result, you only instantiate a single instance of all of your singletons.Now the factories become largely a series of object creations. Totally boring stuff, so boring a computer could generate the code. Simply look at the constructor and recursively instantiate whatever the constructor wants. Wait, a computer can generate it! Its called PicoContainer or GUICE! So you don't actually have to write the factories.", "by Mi\u0161ko Hevery So you join a new project, which has an extensive mature code base. Your new lead asks you to implement a new feature, and, as a good developer, you start by writing a test. But since you are new to the project, you do a lot of exploratory \"What happens if I execute this method\" tests. You start by writing this: testCreditCardCharge() {CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(100);} This code: Only works when you run as part of the suite.When run in isolation, throws NullPointerException.When you get your credit card bill, you are out $100 for every time the test runs. Now, I want to focus on the last point. How in the world did the test cause an actual charge on my credit card? Charging a credit card is not easy. The test needs to talk to a third party credit card web-service. It needs to know the URL for the web-service. It needs to authenticate, pass the credentials, and identify who the merchant is. None of this information is present in the test. Worse yet, since I don't even know where that information is present, how do I mock out the external dependencies so that every run does not result in $100 actually being charged? And as a new developer, how was I supposed to know that what I was about to do was going to result in me being $100 poorer? That is \"Spooky action at a distance!\" But why do I get NullPointerException in isolation while the test works fine when run as part of the suite? And how do I fix it? Short of digging through lots of source code, you go and ask the more senior and wiser people on the project. After a lot of digging, you learn that you need to initialize the CreditCardProcessor. testCreditCardCharge() {CreditCardProcessor.init();CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(100);} You run the test again; still no success, and you get a different exception. Again, you chat with the senior and wiser members of the project. Someone tells you that the CreditCardProcessor needs an OfflineQueue to run. testCreditCardCharge() {OfflineQueue.init();CreditCardProcessor.init();CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(100);} Excited, you run the test again: nothing. Yet another exception. You go in search of answers and come back with the knowledge that the Database needs to be initialized in order for the Queue to store the data. testCreditCardCharge() {Database.init();OfflineQueue.init();CreditCardProcessor.init();CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(100);} Finally, the test passes in isolation, and again you are out $100. (Chances are that the test will now fail in the suite, so you will have to surround your initialization logic with \"if not initialized then initialize\" code.) The problem is that the APIs are pathological liars. The credit card pretends that you can just instantiate it and call the charge method. But secretly, it collaborates with the CreditCardProcessor. The CreditCardProcessor API says that it can be initialized in isolation, but in reality, it needs the OfflineQueue. The OflineQueue needs the database. To the developers who wrote this code, it is obvious that the CreditCard needs the CreditCardProcessor. They wrote the code that way. But to anyone new on the project, this is a total mystery, and it hinders the learning curve. But there is more! When I see the code above, as far as I can tell, the three init statements and the credit card instantiation are independent. They can happen in any order. So when I am re-factoring code, it is likely that I will move and rearrange the order as a side-effect of cleaning up something else. I could easily end up with something like this: testCreditCardCharge() {CreditCardProcessor.init();OfflineQueue.init();CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(100);Database.init();} The code just stopped working, but I had no way to knowing that ahead of time. Most developers would be able to guess that these statements are related in this simple example, but on a real project, the initialization code is usually spread over many classes, and you might very well initialize hundreds of objects. The exact order of initialization becomes a mystery. How do we fix that? Easy! Have the API declare the dependency! testCreditCardCharge() {Database db = Database();OfflineQueue q = OfflineQueue(db);CreditCardProcessor ccp = new CreditCardProcessor(q);CreditCard c =  new CreditCard(  \"1234 5678 9012 3456\", 5, 2008);c.charge(ccp, 100);} Since the CreditCard charge method declares that it needs a CreditCardProcessor, I don't have to go ask anyone about that. The code will simply not compile without it. I have a clear hint that I need to instantiate a CreditCardProcessor. When I try to instantiate the CreditCardProcessor, I am faced with supplying an OfflineQueue. In turn, when trying to instantiate the OfflineQueue, I need to create a Database. The order of instantiation is clear! Not only is it clear, but it is impossible to place the statements in the wrong order, as the code will not compile. Finally, explicit reference passing makes all of the objects subject to garbage collection at the end of the test; therefore, this test can not cause any other test to fail when run in the suite. The best benefit is that now, you have seams where you can mock out the collaborators so that you don't keep getting charged $100 each time you run the test. You even have choices. You can mock out CreditCardProcessor, or you can use a real CreditCardProcessor and mock out OfflineQueue, and so on. Singletons are nothing more than global state. Global state makes it so your objects can secretly get hold of things which are not declared in their APIs, and, as a result, Singletons make your APIs into pathological liars. Think of it another way. You can live in a society where everyone (every class) declares who their friends (collaborators) are. If I know that Joe knows Mary but neither Mary nor Joe knows Tim, then it is safe for me to assume that if I give some information to Joe he may give it to Mary, but under no circumstances will Tim get hold of it. Now, imagine that everyone (every class) declares some of their friends (collaborators), but other friends (collaborators which are singletons) are kept secret. Now you are left wondering how in the world did Tim got hold of the information you gave to Joe. Here is the interesting part. If you are the person who built the relationships (code) originally, you know the true dependencies, but anyone who comes after you is baffled, since the friends which are declared are not the sole friends of objects, and information flows in some secret paths which are not clear to you. You live in a society full of liars.", "(This week, TotT issued our 100th internally published episode. That's more than have been published to this Testing Blog -- after all, the internal episodes had an 8-month head start, and many  would make no sense to readers outside our own stalls -- but we thought you'd still enjoy reading about the history and future of TotT.)Did you know there was a time before Testing on the Toilet? It's true! 19.3% of Googlers remember back before TotT's weekly entertainment and testing advice. In this 100th episode, let's reminisce a bit, then look toward our future ... and how you can help keep our toilets humorous and informative.After our first episode (May 2, 2006), TotT was met with some skepticism from Googlers and others, including Slashdot.org, who said \"It [is] faintly reminiscent of a cult.\" But soon Google embraced TotT. A few weeks later, someone complained on a mailing list: \"Why wasn't I informed of this [new testing] technique at my nearby toilet?\" Nooglers eagerly sign up to distribute episodes with our motto: \u201cDebugging sucks. Testing rocks!\u201d Some mottos that didn't make the cut:   \"Testing on the Toilet: it's not just for pregnancy anymore!\"   \"Make software testing your number one priority!\"   \"Testing: you can't just wash your hands of it.\"Now, TotT appears weekly:In hundreds of stalls in 30 Google offices  With episodes covering many programming languages and application domainsWritten by volunteer authors from offices worldwide. TotT is also published to fans outside our walls on Google's public Testing Blog.It's all done by a volunteer, grassroots effort of dedicated, passionate Googlers. This bottom-up activism \u2013 engineers making other engineers' lives better \u2013 is a hallmark of Google culture. Other grouplets have adopted TotT's techniques to effectively spread their own messages in flyers like Hiring on the Table, and \u201cTotT Presents\u201d guest spots have shown items such as Production on the Potty.Little known fact: TotT's testing advocacy is a leading factor in the recovery of the red kangaroo population, due to the drop in demand for \u201cbuild red\u201d phosphorus.Remember to download this special episode of Testing on the Toilet and post it in your office.", "The progressive developer knows that in this complex modern world, things aren't always black-and-white, even in  testing. Sometimes we know the software won't return the best answer, or even a correct answer, for all input. You may be tempted to write only test cases that your software can pass. After all, we can't have automated tests that fail in even one instance. But, this would miss an opportunity.Speaking of black-and-white, take decoding of two-dimensional barcodes, like QR Codes. From a blurry, skewed, rotated image of a barcode, software has to pick it out, transform it, and decode it: http://google.com/gwt/n?u=bluenile.comEven the best software can't always find that barcode. What should tests for such software do?We have some answers, from experience testing such software. We have two groups of black-box tests that verify that images decode correctly: must-have and nice-to-have. Tests verify that the must-have set \u2013 the easy images \u2013 definitely decode correctly. This is what traditional tests would include, which typically demand a 100% pass rate. But we also see how well we do on the more difficult nice-to-have set. We might verify that 50% of them decode, and fail otherwise.The advantage? We can include tougher test cases in unit tests, instead of avoiding them. We can observe small changes \u2013 improvements as well as degradations \u2013 in decode accuracy over time. It doubles as a crude quality evaluation framework.Where can this progressive thinking be applied? Maybe when your code...Only needs to be correct in most cases. As here, write tests to verify easy cases work, but also that some hard cases pass too.Needs to be fast. You write unit tests that verify it runs \"fast enough\" on simple input. How about writing tests that make sure it runs \"fast enough\" on most of some larger inputs too?Is heuristic. You write unit tests that verify that the answer is \u201creally close\u201d to optimal on simple input, but also that it's \u201ckind of close\u201d on difficult input.By the way, did we mention project ZXing, Google's open-source decoder project? Or that Print Ads is already helping clients place these two-dimensional barcodes in the New York Times? Or that there are other formats like Data Matrix? or that you can put more than just a URL in these barcodes? This is a technology going global, so, time to read up on it.Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko HeverySo you decided to finally give this testing thing a try. But somehow you just can't figure out how to write a unit-test for your class. Well there are no tricks to writing tests, there are only tricks to writing testable code. If I gave you testable code you would have no problems writing a test for it. But, somehow you look at your code and you say, \"I understand how to write tests for your code, but my code is different \". Well your code is different because you violated one or more of the following things. (I will go into the details of each in a separate blog posts)Mixing object graph construction with application logic: In a test the thing you want to do is to instantiate a portion (ideally just the class under test) of your application and apply some stimulus to the class and assert that the expected behavior was observed.  In order to instantiate the a class in isolation we have to make sure that the class itself does not instantiate other objects (and those objects do not instantiate more objects and so on). Most developers freely mix the \"new\" operator with the application logic. In order to have a testable code-base your application should have two kinds of classes. The factories, these are full of the \"new\" operators and are responsible for building the object graph of your application, but don't do anything. And the application logic classes which are devoid of the \"new\" operator and are responsible for doing work. In test we want to test the application logic. And because the application logic is devoid of the \"new\" operator, we can easily construct an object graph useful for testing where we can strategically replace the real classes for test doubles. (see: How to Think About the \u201cnew\u201d Operator with Respect to Unit Testing)Ask for things, Don't look for things (aka Dependency Injection / Law of Demeter): OK, you got rid of your new operators in you application code. But how do I get a hold of the dependencies. Simple: Just ask for all of the collaborators you need in your constructor. If you are a House class then in your constructor you will ask for the Kitchen, LivingRoom, and BedRoom, you will not call the \"new\" operator on those classes (see 1). Only ask for things you directly need, If you are a CarEngine, don't ask for FuelTank, only ask for Fuel. Don't pass in a context/registry/service-locator. So if you are a LoginPage, don't ask for UserContext, instead ask for the User and the Athenticator. Finally don't mix the responsibility of work with configuration, If you are an Authenticator class don't pass in a path of the configuration information which you read inside the constructor to configure yourself, just ask for the configuration object and let some other class worry about reading the object from the disk. In your tests you will not want to write a configuration into a disk just so that your object can read it in again. (see: Breaking the Law of Demeter is Like Looking for a Needle in the Haystack)Doing work in constructor: A class under tests can have tens of tests. Each test instantiates a slightly different object graph and than applies some stimulus and asserts a response.  As you can see the most common operation you will do in tests is instantiation of object graphs, so make it easy on yourself and make the constructors do no work (other than assigning all of the dependencies into the fields). Any work you do in a constructor, you will have to successfully navigate through on every instantiation (read every test). This may be benign, or it may be something really complex like reading configuration information from the disk. But it is not just a direct test for the class which will have to pay this price, it will also be any related test which tries to instantiate your class indirectly as part of some larger object graph which the test is trying to create.Global State: Global state is bad from theoretical, maintainability, and understandability point of view, but is tolerable at run-time as long as you have one instance of your application. However, each test is a small instantiation of your application in contrast to one instance of application in production. The global state persists from one test to the next and creates mass confusion. Tests run in isolation but not together. Worse yet, tests fail together but problems can not be reproduced in isolation. Order of the tests matters. The APIs are not clear about the order of initialization and object instantiation, and so on. I hope that by now most developers agree that global state should be treated like GOTO.Singletons (global state in sheep's clothing): It amazes me that many developers will agree that global state is bad yet their code is full of singletons. (Singletons which enforce their own singletoness through private constructor and a global instance variable) The core of the issue is that the global instance variables have transitive property! All of the internal objects of the singleton are global as well (and the internals of those objects are global as well... recursively). Singletons are by far the most subtle and insidious thing in unit-testing. I will post more blogs on this topic later as I am sure it will create comments from both sides.Static methods: (or living in a procedural world): The key to testing is the presence of seams (places where you can divert the normal execution flow). Seams are essentially polymorphism (Polymorphism: at compile-time the method you are calling can not be determined). Seams are needed so that you can isolate the unit of test. If you build an application with nothing but static methods you have procedural application. Procedural code has no seams, at compile-time it is clear which method calls which other method. I don't know how to test application without seams. How much a static method will hurt from a testing point of view depends on where it is in you application call graph. A leaf method such as Math.abs() is not a problem since the execution call graph ends there. But if you pick a method in a core of your application logic than everything behind the method becomes hard to test, since there is no way to insert test doubles (and there are no seams). Additionally it is really easy for a leaf method to stop being a leaf and than a method which was OK as static no longer is. I don't know how to unit-test the main method!Favor composition over inheritance: At run-time you can not chose a different inheritance, but you can chose a different composition, this is important for tests as we want to test thing in isolation. Many developers use inheritance as code reuse which is wrong. Whether or not inheritance is appropriate depends on whether polymorphism is going on. Inheriting from AuthenticatedServlet will make your sub-class very hard to test since every test will have to mock out the authentication. This will clutter the focus of test, with the things we have to do to successfully navigate the super class. But what if AuthenticatedServlet inherits from DbTransactionServlet? (that gets so much harder)Favor polymorphism over conditionals: If you see a switch statement you should think polymorphisms. If you see the same if condition repeated in many places in your class you should again think polymorphism. Polymorphism will break your complex class into several smaller simpler classes which clearly define which pieces of the code are related and execute together. This helps testing since simpler/smaller class is easier to test.Mixing Service Objects with Value Objects: There should be two kinds of objects in your application. (1) Value-objects, these tend to have lots of getters / setters and are very easy to construct are never mocked, and probably don't need an interface. (Example: LinkedList, Map, User, EmailAddress, Email, CreditCard, etc...). (2) Service-objects which do the interesting work, their constructors ask for lots of other objects for colaboration, are good candidates for mocking, tend to have an interface and tend to have multiple implementations (Example: MailServer, CreditCardProcessor, UserAthenticator, AddressValidator). A value-object should never take a service object in its constructor (since than it is not easy to construct). Value-objects are the leafs of your application graph and tend to be created freely with the \"new\" operator directly in line with your business logic (exception to point 1 since they are leafs). Service-objects are harder to construct and as a result are never constructed with a new operator in-line, (instead use factory / DI-framework) for the object graph construction. Service-objects don't take value-objects in their constructors since DI-frameworks tend to be unaware about the how to create a value-object. From a testing point of view we like value-objects since we can just create them on the fly and assert on their state. Service-objects are harder to test since their state is not clear and they are all about collaboration and as a result we are forced to use mocking, something which we want to minimize. Mixing the two creates a hybrid which has no advantages of value-objects and all the baggage of service-object.Mixing of Concerns: If summing up what the class does includes the word \"and\", or class would be challenging for new team members to read and quickly \"get it\", or class has fields that are only used in some methods, or class has static methods that only operate on parameters than you have a class which mixes concerns. These classes are hard to tests since there are multiple objects hiding inside of them and as a resulting you are testing all of the objects at once.So here is my top 10 list on testability, the trick is translating these abstract concepts into concrete decisions in your code.", "Posted by Lydia Ash, GTAC Conference Chair - 2008A brief reminder that there are only two weeks left to submit applications for this year's Google Test Automation Conference. The application deadline is August 15th, after which the selection process will begin.The Call For Attendance application is available on the Google website here. http://services.google.com/events/gtac2008See you in October!", "by Mi\u0161ko HeverySo you discovered dependency injection and GUICE and you are happily refactoring and writing new tests for you code until you come across this circular reference.class A {final B b;A(B b){  this.b = b;}}class B {final A a;B(){  this.a = new A(this);}}+---------+      +---------+|    A    ||         |+---------+      +---------+Hm, dependency injection says that you need to ask for your dependencies and so the resulting code will be:class A {final B b;A(B b){  this.b = b;}}class B {final A a;B(A a){  this.a = a;}}But now we have a problem, we can't instantiate this (I know GUICE can through proxy, but it is not clean and it does not help us in tests). So the real problem in situation like this is mixing of concerns. One of the two objects is hiding another object C. Either A contains C or B contains C. To find out which one it is, list all of the methods in your class A and class B and. The shorter of the two lists is your hidden Class C.+---------+      +---------+|    A    ||C| ||         |------+---->| | ||         |      |     +-+ |+---------+      +---------+Suppose B has the shorter list. We now extract all of the methods in B which are accessing the state of hidden C methods into a new object C like this:                         +---------++---------+              |    B    ||    A    || C |When you go through this exercise you will realize that the C was always an object in its own right but you have never thought about it that way, so the new code is actually better OO. From testing point of view you can now test each class in isolation.", "To quell a lingering feeling of inadequacy, you took the time to build your own planetary death ray, a veritable rite of passage in the engineering profession. Congratulations. And you were feeling pretty proud until the following weekend, when you purchased the limited-edition Star Wars trilogy with Ewok commentary, and upon watching the Death Star destroy Alderaan, you realized that you had made a bad decision: Your planetary death ray has a blue laser, but green lasers look so much cooler. But it's not a simple matter of going down to Radio Shack to purchase a green laser that you can swap into your existing death ray. You're going to have to build another planetary death ray from the ground-up to have a green laser, which is fine by you because owning two death rays instead of one will only make the neighbors more jealous.Both your planetary death rays should interoperate with a variety of other bed-wettingly awesome technology, so it's natural that they export the same Java API:public interface PlanetaryDeathRay {\u00a0\u00a0public void aim(double xPosition, double yPosition);\u00a0\u00a0public boolean fire();  /* call this if she says the rebel\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0base is on Dantooine */}public class BlueLaserPlanetaryDeathRay\u00a0\u00a0\u00a0\u00a0implements PlanetaryDeathRay { /* implementation here */ }public class GreenLaserPlanetaryDeathRay\u00a0\u00a0\u00a0\u00a0implements PlanetaryDeathRay { /* implementation here */ }Testing both death rays is important so there are no major malfunctions, like destroying Omicron Persei VIII instead of Omicron Persei VII. You want to run the same tests against both implementations to ensure that they exhibit the same behavior \u2013 something you could easily do if you only once defined tests that run against any PlanetaryDeathRay implementation. Start by writing the following abstract class that extends junit.framework.TestCase:public abstract class PlanetaryDeathRayTestCase\u00a0\u00a0\u00a0\u00a0extends TestCase {\u00a0\u00a0protected PlanetaryDeathRay deathRay;\u00a0\u00a0@Override protected void setUp() {\u00a0\u00a0\u00a0\u00a0deathRay = createDeathRay();\u00a0\u00a0}\u00a0\u00a0@Override protected void tearDown() {\u00a0\u00a0\u00a0\u00a0deathRay = null;\u00a0\u00a0}\u00a0\u00a0protected abstract PlanetaryDeathRay createDeathRay();\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0/* create the PlanetaryDeathRay to test */\u00a0\u00a0public void testAim() {\u00a0\u00a0\u00a0\u00a0/* write implementation-independent tests here against\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0deathRay.aim() */\u00a0\u00a0}\u00a0\u00a0public void testFire() {\u00a0\u00a0\u00a0\u00a0/* write implementation-independent tests here against\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0deathRay.fire() */\u00a0\u00a0}}Note that the setUp method gets the particular PlanetaryDeathRay implementation to test from the abstract createDeathRay method. A subclass needs to implement only this method to create a complete test: the testAim and testFire methods it inherits will be part of the test when it runs:public class BlueLaserPlanetaryDeathRayTest\u00a0\u00a0\u00a0\u00a0extends PlanetaryDeathRayTestCase {\u00a0\u00a0protected PlanetaryDeathRay createDeathRay() {\u00a0\u00a0\u00a0\u00a0return new BlueLaserPlanetaryDeathRay();\u00a0\u00a0}}You can easily add new tests to this class to test functionality specific to BlueLaserPlanetaryDeathRay.Remember to download this episode of Testing on the Toilet and post it in your office.", "by Mi\u0161ko Hevery, Jonathan Wolter, Russ Ruffer, Brad Cross, and lots of other test infected GooglersThis guide lists principles that will help you write impossible to tests code. Or, avoiding these techniques will help you write code that can be tested. Make Your Own Dependencies - Instantiate objects using new in the middle of methods, don't pass the object in. This is evil because whenever you new up an object inside a block of code and then use it, anyone that wants to test that code is also forced to use that concrete object you new'ed up. They can't \"inject\" a dummy, fake, or other mock in to simplify the behavior or make assertions about what you do to it. Heavy Duty Constructors - Make constructors that do lots of work in them. The more work you do in the constructor, the hard it is to create your object in a test fixture. And if your constructor can construct other things that are hard themselves to construct, that's even better! You want the transitive dependencies of every constructor to be enormous. Enormous is hard to get under test. Depend on Concrete Classes - Tie things down to concrete classes - avoid interfaces wherever possible. (They let people substitute the concrete classes you're using for their own classes which would implement the same contract in the interface or abstract class. Testing do-gooders might want to do this to get your code under a test harness - don't let them!) Conditional Slalom - Always, always, feel good when writing lengthy if branches and switch statements. These increase the number of possible execution paths that tests will need to cover when exercising the code under test. The higher the Cyclomatic complexity, the harder it is to test! When someone suggests to use polymorphism instead of conditionals, laugh at their thoughtfulness towards testing. Make the branching both deep and wide: if you're not consistently at least 5 conditionals deep, you're spoon feeding testable code to the TDD zealots. Depend on Large Context Objects - Pass around ginormous context objects (or small ones with hard to construct contents). These will reduce the clarity of methods [myMethod(Context ctx) is less clear than myMethod(User user, Label label)]. For testing, the context objects will need to be created, populated, and passed around. Use Statics - Statics, statics everywhere! They put a great big crunch in testability. They can't be mocked, and are a smell that you've got a method without a home. OO zealots will say that a static method is a sign that one of the parameters should own the method. But you're being 3v1L! Use More Statics - Statics are a really powerful tool to bring TDD Infected engineers to their knees. Static methods can't be overridden in a subclass (sometimes subclassing a class and overriding methods is a technique for testing). When you use static methods, they can't be mocked using mocking libraries (nixing another trick up the pro-testing engineer's sleeve). Use Global Flags - Why call a method explicitly? Just like L Ron Hubbard, use \"mind over matter\" to set a flag in one part of your code, in order to cause an effect in a totally different part of your application (it's even more fun when you do it concurrently in different threads!). The testers will go crazy trying to figure out why all of a sudden a conditional that was evaluating true one minute is all of a sudden evaluating to false. Use Singletons Everywhere - Why pass in a dependency and make it obvious when you can use a singleton? It's hard to set up a test that requires singletons, and the TDDers will be in for a world of hurt when all their tests depend on each other's state. Be Defensive - They're out to Get Your Code! - Defensively assert about the state of parameters passed in methods, constructors, and mid-method. If someone can pass in a null, you've left your guard down. You see, there are testing freaks out there that like to instantiate your object, or call a method under test and pass in nulls! Be aggressive in preventing this: rule your code with an iron fist! (And remember: it's not paranoia if they really are out to get you.) Use Primitives Wherever Possible - Instead of using a \"cookie object,\" pass in primitives and do all the parsing you need, every time you need a value. Primitives make people work harder by having to parse and massage them to get the data out -- where objects are mockable (gasp) and nullable, and encapsulate state (who'd want to do that?) Look for Everything You Need - By Looking for things you are asserting your objects dominance as the object which knows where everything is. This will make the life of tester hard, since he will have to mimic the environment so that your code can get a hold of what it needs. And don't be afraid of how many objects you need to reach out to to, the more the harder it will be for test to mock them all out in unisin. If you are an InvoiceTaxCalculator, feel free to do things like: invoiceTaxCalculator.getUser().getDbManager().getCaRateTables().getSalesTaxRate(). Cover your ears when some testing weenie tells you about Dependency Injection, Law of Demeter, or not looking for things. Use static initializes - Do as much work as possible when your class is loaded. Testing nuts will be so frustrated when they find out just loading your class causes nasty stuff like network or file access. Couple functional code directly to the external systems it depends on If your product uses external systems such as a databases, file systems or a network, make sure your business logic is coded to reference as many low level implementation details of these systems as possible. This will prevent others from using your code in ways you don't intend, (like small tests that run in 2 ms instead of 5 minutes). Mix Object Lifecycles - Have long lived objects reference short lived objects. This confuses people as the long lived object references the short lived object still after it's no longer valid or alive. This is especially insidious, both bad design, and evil, hard to test.Side Effects are the Way to Go Your best bet is to perform a large number of side effect producing operations in your methods. This is especially true for setters. The more non-obvious the side effects better. Peculiar and seemingly irrational side effects are particularly helpful for unit testing. To add another layer of sweet creamy goodness on top, you want to make it possible to initialize your objects in an invalid state, with uninitialized member fields. Once you have achieved this, be sure to make calls on the methods of the uninitialized fields as side effects from your setters in order to cause SEGV's or NPE's, depending on your language's vernacular. Why go to all this trouble? Clean, readable, and testable code that works, that's why! Side effect free functions are for intellectual weaklings that think a function name should give some kind of an indication of what the function does. Create Utility Classes and Functions/Methods - For instance, you have a String which is a URL you're passing around (obeying \"Use Primitives Wherever Possible\"). Create another class with static methods such as isValidUrl(String url). Don't let the OO police tell you to make that a method on a URL object. And if your static utility methods can call to external services as well, that's even better! Create Managers and Controllers - all over the place have these Managers and Controllers meddling in the responsibilities of other objects. Don't bother trying to pull that responsibility into other individual objects. Look at a SomeObjectManager class and you have no idea what it's going to do. Do Complicated Creation Work in Objects - Whenever someone suggests you to use a Factory to instantiate things, know that you are smarter than them. You're more intelligent than they must be, because your objects can have multiple responsibilities and be thousands of lines long. Greenlight if-branches and switch statements - Go ahead, don't feel dirty about nesting if-branches. It's \"more readable\" that way. OO cowboys will want to have a whole polymorphic soup of collaborating objects. Say no to the OO-ist. When you nest and branch conditionals, all you need to do is read the code from top to bottom. Like a great novel, one simple linear prose of code. With the OO-overboard paradigm, it's like a terrible choose-your-own-adventure kid's book. You're constantly flipping between classes and juggling patterns and so many more complex concepts. Just if-things out and you'll be fine. Utils, Utils, Utils! - Code smell? No way - code perfume! Litter about as many util and helper classes as you wish. These folks are helpful, and when you stick them off somewhere, someone else can use them too. That's code reuse, and good for everyone, right? Be forewarned, the OO-police will say that functionality belongs in some object, as that object's responsibility. Forget it, you're way to pragmatic to break things down like they want. You've got a product to ship after all! Use \"Refactoring\" whenever you need to get away with something - This is a word that Test-Driven and OO-goons like. So if you want to do something far reaching, involving new functionality, without tests, just tell them you're \"Refactoring.\" It'll trick them every time. No matter that they think you need to have tests around everything before you can refactor, and that it should never add new functionality. Ignore their hubbub, and do things your own way!Java Specific Final Methods - Use final classes and methods all the time. They can't be overridden for testing (-; But don't bother making fields final, or using value objects (without setters) - just let your objects' state be changed by anything and anyone. No sense in guaranteeing state, it'd make things too easy. Handcuff your users to Specific Types - Use instanceof as much as possible. This will make Mocking a pain, and show people that you're in control of the objects allowed.C++ Specific Use non-virtual methods - Unless you need to override the method in a deep and scary inheritance hierarchy, just make the method non-virtual. If you make it virtual, a TDD zealot may mock your class! An even nicer trick is to keep your destructor non-virtual - then when the testing freaks try to subclass, whooooaoaaaaaa.... Never use pure abstract classes - Depending on pure abstract classes is a sure-fire way to let the TDD crazies inject stubs and mocks into your code, making it testable. Macros are your friends - Always use #ifdef PROD and other compile-time switches to make sure the testies can't get at your really important blocks of code and test them. In fact, this code won't even run: until it gets to production!", "by Mi\u0161ko HeveryEvery time I see Law of Demeter violation I imagine a haystack where the code is desperately trying to locate the needle.class Mechanic {Engine engine;Mechanic(Context context) {  this.engine = context.getEngine();}}The Mechanic does not care for the Context. You can tell because Mechanic does not store the reference to Context. Instead the Mechanic traverses the Context and looks for what it really needs, the Engine.  So what is wrong with code like this you say? The problems with such code are very subtle:Most applications tend to have some sort of Context object which is the kitchen sink and which can get you just about any other object in the system aka the service locator.If you want to reuse this code at a different project, the compiler will not only need Mechanic and Engine but also the Context. But Context is the kitchen sink of your application. It tends to have reference to just about every other class in your system, hence the compiler will need those classes too. This kind of code is not very reusable.Even if you don't plan to reuse the code, the Context has high coupling with the rest of the system. Coupling is transitive, this means Mechanic inherits all of the badness through association.Your JavaDoc is not very useful! Yes, by examining the API I can see that the Mechanic needs Context, but Context is the kitchen sink. What does the mechanic really need? (If you don't have source code nearby than it may be hard to figure out).But here is the real killer! Writing tests for such code base sucks!Every time I have to write a test I have to create a graph of objects (the haystack) which no one really needs or cares for, and into this haystack I have to carefully place the needles (the real object of interests) so that the code under test can go and look for them. I have to create the Context just so when I construct the Mechanic it can reach in the Context and get what it realy needs, the Engine. But context is never something which is easy to create. Since context is a kitchen sink which knows just about everything else, its constructor is usually scary. So most of the time I can't even instantiate Mechanic, not to mention run any tests on it. The testing game is over before it even started.Ok, but today we have fancy tools such as JMock and EasyMock, surely i can mock out Context. Yes, you can! BUT: (1) typical setup of a mock is about 5 lines of code. So your test will contain a lot of junk which will mask the real purpose of the test. (2) These tests will be fragile. Every time you refactor something in context, or how context interacts, you are running the risk of breaking your tests. (False negatives) (3) What if you want to test class Shop which needs a reference to Mechanic? Well then you have to mock out Context again. This mocking tax will be spread all over your tests. In the end the mocking setup will drown out your tests and make for one unreadable test base.Please stop looking for the needle in the haystack and just ask for the things you directly need in your code. You will thank me later...class Mechanic {Engine engine;Mechanic(Engine engine) {  this.engine = engine;}}PS: Now imagine how hard will it be to write a test for this class:class Monitor {SparkPlug sparkPlug;Monitor(Context context) {  this.sparkPlug = context.        getCar().getEngine().        getPiston().getSparkPlug();}}GOOD LUCK!", "Posted by Patrick Copeland, Engineering Productivity DirectorA smart person once said that, \"all computer science problems can be solved by introducing another abstraction layer.\" From a testing perspective, every new abstraction layer also introduces every problem in computer science.Frank Cohen, of PushToTest, wrote a blog post recently that focuses on the Google Web Tool Kit and puts the challange of dealing new abstration layers into a brief historical perspective.http://tinyurl.com/6r34jf", "Posted by Lydia Ash, Test Engineering Manager Call for AttendanceGoogle Test Automation Conference 2008Seattle, WAOctober 23 - 24Google's Test Automation Conference is built on our participants each bringing their experience, ideas, and insight to the discussions we host. Our participants are all experienced industry professionals working as software development engineers or software testing engineers. For the limited number of spaces available in each year's conference, we ask each applicant to share what they would bring to the discussions as an active participant in the conference.To apply for a participant space at the GTAC 2008 conference, you can apply:http://services.google.com/events/gtac2008TimelineDeadline for applying as a participant: August 15Notification of acceptance: August 29Wait list callbacks: October 3GTAC 2008 Conference: October 23 and 24Scheduled PresentationsAtom Publishing Protocol, Testing a Server Implementation by David CalaveraJInjector: a Coverage and End-To-End Testing Framework for J2ME and RIM by Julian Harty, Olivier Gaillard, and Michael SamaAdvances in Automated Software Testing Technologies by Elfriede DustinTaming the Beast: How to test an AJAX Application by Markus Clermont and John ThomasAutomated Model-Based Testing of Web Applications by Oluwaseun Akinmade and Prof. Atif M MemonBoosting Your Testing Productivity with Groovy by Andres AlmirayPracticing Testability in the Real World by Vishal ChowdharyThe New Genomics: Software Development at Petabyte Scale by Matt WoodThe Value of Small Tests by Christopher SemtursDeployment and Test Automation: Extending the Notion of 'Done' to 'System Tested on a Production Deployment' by Marc-Elian BeginNo Registration FeesThe GTAC conference has no registration fees. Once invitations have been sent to participate, full registration instructions will be sent to each invitee. Breakfast and lunch will be provided each day of the conference, as well as a reception the evening of October 23.CancellationIf you register as a participant and will not be able to attend, please send email immediately to gtac@google.com to allow a wait list person the opportunity to participate in that slot.Hotel AccommodationsW Seattle    (Conference Venue)1112 Fourth AvenueSeattle, WA 98101(206) 264-6000Other Hotels in the AreaHotel Vintage Park1100 5th AveSeattle, WA(206) 624-8000Madison Renaissance Hotel515 Madison StSeattle, WA(206) 583-0300Hotel Monaco1101 4th AvenueSeattle, WA(206) 621-1770", "By Mi\u0161ko HeveryUnit Testing as the name implies asks you to test a Class (Unit) in isolation.If your code mixes Object Construction with Logic you will never be able to achieve isolation.In order to unit-test you need to separate object graph construction from the application logic into two different classesThe end goal is to have either: classes with logic OR classes with \"new\" operators.Unit-Testing as the name implies is testing of a Unit (most likely a Class) in isolation. Suppose you have a class House. In your JUnit test-case you simply instantiate the class House, set it to a particular state, call the method you want to test\u00a0 and then assert that the class' final state is what you would expect. Simple stuff really...class House {\u00a0 private boolean isLocked;\u00a0 private boolean isLocked() {\u00a0\u00a0\u00a0 return isLocked;\u00a0 }\u00a0 private boolean lock() {\u00a0\u00a0\u00a0 isLocked = true;\u00a0 }}If you look at House closely you will realize that this class is a leaf of your application. By leaf I mean that it is the end of the dependency graph, it does not reference any other classes. As such all leafs of any code base are easy to tests, because the dependency graph ends with the leaf. But testing classes which are not leafs can be a problem because we may not be able to instantiate the class in isolation.class House {\u00a0 private final Kitchen kitchen = new Kitchen();\u00a0 private boolean isLocked;\u00a0 private boolean isLocked() {\u00a0\u00a0\u00a0 return isLocked;\u00a0 }\u00a0 private boolean lock() {\u00a0\u00a0\u00a0 kitchen.lock();\u00a0\u00a0\u00a0 isLocked = true;\u00a0 }}In this updated version of House it is not possible to instantiate House without the Kitchen. The reason for this is that the new operator of Kitchen is embedded within the House logic and there is nothing we can do in a test to prevent the Kitchen from getting instantiated. We say that we are mixing the concern of application instantiation with concern of application logic. In order to achieve true unit testing we need to instantiate real House with a fake Kitchen so that we can unit-test the House in isolation. class House {\u00a0 private final Kitchen kitchen;\u00a0 private boolean isLocked;\u00a0\u00a0 public House(Kitchen kitchen) {\u00a0\u00a0\u00a0 this.kitchen = kitchen;\u00a0 }\u00a0 private boolean isLocked() {\u00a0\u00a0\u00a0 return isLocked;\u00a0 }\u00a0 private boolean lock() {\u00a0\u00a0\u00a0 kitchen.lock();\u00a0\u00a0\u00a0 isLocked = true;\u00a0 }}Notice how we have removed the new operator from the application logic. This makes testing easy. To test we simply new-up a real House and use a mocking framework to create a fake Kitchen. This way we can still test House in isolation even if it is not a leaf of an application graph. But where have the new operators gone? Well, we need a factory object which is responsible for instantiating the whole object graph of the application. An example of what such an object may look like is below. Notice how all of the new operators from your applications migrate here.class ApplicationBuilder {\u00a0 House build() {\u00a0\u00a0\u00a0 return new House(\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 new Kitchen(new Sink(), new Dishwasher(), new Refrigerator())\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 );\u00a0 }}As a result your main method simply asks the ApplicationBuilder to construct the object graph for you application and then fires of the application by calling a method which does work.class Main {\u00a0 public static void(String...args) {\u00a0\u00a0\u00a0 House house = new ApplicationBuilder().build();\u00a0\u00a0\u00a0 house.lock();\u00a0 }}Asking for your dependencies instead of constructing them withing the application logic is called \"Dependency Injection\" and is nothing new in the unit-testing world. But the reason why Dependency Injection is so important is that within unit-tests you want to test a small subset of your application. The requirement is that you can construct that small subset of the application independently of the whole system. If you mix application logic with graph construction (the new operator) unit-testing becomes impossible for anything but the leaf nodes in your application. Without Dependency Injection the only kind of testing you can do is scenario-testing, where you instantiate the whole application and than pretend to be the user in some automated way.", "Because the Google C++ Testing Framework was opensourced last week, there will be episodes focusing on it published here in the future.  Watch for them.  I've reshuffled the schedule to get one out this week.Google C++ Testing Framework supports two families of assertions with the same interface:ASSERT: Fails fast, aborting the current function.EXPECT: Continues after the failure.As Moka the code monkey has shown Uni the testing robot, EXPECT is often more appropriate because it:  reveals more failures in a single run, and allows more to be fixed in a single edit/compile/run-tests cycle.  Example:TEST(WordStemmerTest, StemsPluralSuffixes) {\u00a0\u00a0EXPECT_STREQ(\"jump\", stemmer->Stem(\"jumps\"));\u00a0\u00a0EXPECT_STREQ(\"pixi\", stemmer->Stem(\"pixies\"));\u00a0\u00a0EXPECT_STREQ(\"prioriti\", stemmer->Stem(\"priorities\"));\u00a0\u00a0// etc ...}Use ASSERT when it doesn't make sense to continue.  Example:TEST(FileGeneratorTest, CreatesFileContainingSequenceOfChars) {\u00a0\u00a0ASSERT_TRUE(fp = fopen(path, \"r\"))\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ASSERT_EQ(10, fread(buffer, 1, 10, fp))\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0buffer[10] = '\\0';\u00a0\u00a0EXPECT_STREQ(\"123456789\", buffer)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Zhanyong Wan, Software EngineerWe all know the importance of writing automated tests to cover our code. To make it easier for everyone to write good C++ tests, today we have open-sourced Google C++ Testing Framework (Google Test for short), a library that thousands of Googlers have been using in our C++ programs.  Highlights of the project include:      Google Test is portable: it works on a variety of platforms (Linux, Windows, Mac OS X, and more), with several versions of GCC and MSVC compilers, and with or without exceptions. You can even use it in embedded systems like Windows CE and Symbian. Build tools and test runners for many of these are under active development, with Linux Autotools support already in place.      It supports both fatal and nonfatal assertions. The test will continue after a nonfatal failure. This allows more problems to be uncovered and fixed in a single edit-compile-test cycle.      It provides many assertions for common testing needs, and lets you easily define new assertions for less common cases.        On Linux, you can write death tests to ensure that your code crashes with expected errors.   Because it's based on the popular xUnit architecture, Google Test is easy to learn if you've used any testing framework in this family before.   It will take you about 10 minutes to learn the basics and get started. Stay tuned to this blog for helpful Google Test information in upcoming Testing on the Toilet episodes.Please send questions and feedback to googletestframework@googlegroups.com (the Google Test Discussion Group). See you there!", "You're pair programming and, as many brilliant people are apt to do, talking out loud.  \"I'll make a mock, inject it, and rerun the test. It should pa- ...D'OH\"   Your partner notices the exception \"ConnectionFactory not initialized\".  \"What?\" she says, \"Something is using the database? Dang, and this was supposed to be a small test.\"\n\nUpon inspection you find that your class is calling a static method on some other class.  You've got Static Cling! If you're (ab)using a data persistence layer that generates code which relies on static methods, and weren't careful, your code might look something like this:\n\n\npublic class MyObject {\u00a0\u00a0public int doSomething(int id) {\u00a0\u00a0\u00a0\u00a0return TheirEntity.selectById(id).getSomething();\u00a0\u00a0}}\n\n\nAs a result, you can't call doSomething without calling TheirEntity's static method. This code is hard to test because static methods are impossible to mock in Java.\n\nSo, how do you get rid of this form of Static Cling and get that small test to pass?  You can use a technique sometimes known as the Repository Pattern, a form of Abstract Factory.  Create an interface and an implementation with the unmockable static method calls:\n\n\ninterface TheirEntityRepository {\u00a0\u00a0TheirEntity selectById(int id);\u00a0\u00a0// other static methods on TheirEntity can be\u00a0\u00a0// represented here too}public class TheirEntityStaticRepository\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0implements TheirEntityRepository {\u00a0\u00a0public TheirEntity selectById(int id) { // non-static\u00a0\u00a0\u00a0\u00a0return TheirEntity.selectById(id);    // calls static method\u00a0\u00a0}}\n\n\nNext, inject a TheirEntityRepository into MyObject and use it instead of calls to the static method:\n\n\npublic class MyObject {\u00a0\u00a0private final TheirEntityRepository repository;\u00a0\u00a0public MyEntity(TheirEntityRepository arg) {\u00a0\u00a0\u00a0\u00a0this.repository = arg;\u00a0\u00a0}\u00a0\u00a0public int doSomething(int id) {\u00a0\u00a0\u00a0\u00a0return repository.selectById(id).getSomething();\u00a0\u00a0}}\n\n\nYou can do this even if you don't have access to source code for TheirEntity, since you're not changing the source itself, but merely encapsulating its static methods in an injectable interface.  The techniques shown here generalize to the case where a static method acts as a Factory of objects.\n\nNow you can inject different implementations of the repository for different tests, such as \"never finds anything,\" \"always throws an exception,\" \"only returns a TheirEntity if the id is a prime,\" and so forth.  These kinds of tests would've been impossible before this refactoring.\n\nRemember to download this episode of Testing on the Toilet and post it in your office.", "Posted by George Pirocanac, Test Engineering ManagerFor the past nine months it has been my pleasure to work with a group of undergrad students from UC-Irvine as part of their senior class project. The course was run by professor Hadar Ziv and teaching assistant Sameer Patil. It focused on providing students industry experience by working with customers (in this case us) to formulate product requirements and deliver working software. Jason Robbins from the Google Irvine office was the lead for another project and several other local companies also participated.Our team members included Michelle Alvaraz, Jason Dramby, Peter Lee and Gabriela Marcu. It was the only project dealing directly with test engineering and one of our goals was specifically to create a plan and framework for testing the Google Mashup Editor (GME) tag language.For those unfamiliar with the GME, it is a framework for developing simple web applications and mashups using a custom set of XML tags, Javascript, CSS and HTML. More information about the GME can be found here.The first three months of the class were spent learning about the the GME and performing exploratory testing. The team became very familiar with the editor and created several mashups (You can try one of them here.). They also created a traditional test plan which focused on testing the tag language. Later they executed this test plan by compiling and running their sample mashups on a variety of browsers.After a couple of iterations of this test plan they quickly encountered some of the typical challenges associated with the traditional approach - namely human resource oversubscription in test execution and insufficient coverage.We addressed with the first issue through automation and the team learned to automate their manual tests of the mashups with Selenium. They first used Selenium IDE to learn the basic Selenium commands and concepts such as locators. Afterwards they used the \"Export Test As...\" feature in IDE to create Python tests that would be run under a local server with Selenium-RC. The latter got them to a point where they could execute the existing test planautomatically on three different platforms (Windows, Linux, MacOS).Expanding coverage was less straightforward. The traditional approach would be to use the existing resources to write more tests. We, however, decided to create a framework that would itself generate more tests. This dovetailed nicely with the classroom material which was product-centric and focused on gathering customer requirements, creating a design document and delivering the software. In our case, the group's product was to be a GME Test Suite Creator.As a starting point we looked at the following simple Python script which creates a simple cross product on lists of strings:#!/usr/bin/pythondef cross(args):  ans = [[]]  for arg in args:     ans = [x+[y] for x in ans for y in arg]  return ansdef pprint(lists):  for list in lists:    a = ''    for s in list:       a = a + s    print atags = [ ['         ['gm:page '],         ['', 'authenticate=true', 'authenticate=false',          'authenticate=invalid'],       ['/>'] ] lists = cross(tags)pprint(lists)Running the script yields the following combination of tags:< gm:page />< gm:page authenticate=true/>< gm:page authenticate=false/>< gm:page authenticate=invalid/>Each one could be used in a mashup that used the page tag. Likewise, the other tags from the GME tag language could be expanded with various combinations of valid and invalid attributes. These tag combinations could then be individually inserted into skeleton mashups producing a large number of both positive and negative tests which would be performed under Selenium-RC.This was the basic idea of the GME Test Suite Creator and the team implemented a GUI to facilitate the three steps in creating and running a testsuite:Code Generation - The selection of tags and creation of tests.Code Preview - The examination and execution of created testsTest Reporting - The examination of test results.The figure below shows the Code Generation tab of the GME Test Suite Creator. It displays a hierarchical view of the tabs and allows the user to select which tags to include in the sample tests. The sample test is generated from a skeleton test modeled after the documentation example scraped from the code.google.com website. This was a nice idea which added testing of the documentation to the process.An interesting problem that these types of automatic test generation frameworks can encounter is the combinatorial explosion of generated tests. For example, if each tag attribute can have 8 possible values and a sample mashup contains 10 tags, enumerating each combination would take roughly 1 billion (810 = 230) tests! To address this, the team created an Options dialog box that would allow the user to specify different test suite sizes in addition to the test suite name and type. A further refinement, allowing the user to select which set of specific values to use for tag attributes would have been implemented if the team had more time.The next figure shows the Code Preview tab of the GME Test Suite Creator. It shows the list of tests created under a given test suite and allows the user to manage and execute the test suite.Finally, the Test Report Tab shows the results of the tests executed under Selenium-RC.GME Test Suite Creator was itself written in Python and hosted on Windows, Linux and MacOS.The team presented and demonstrated the GME Test Suite Creator to faculty and other student/industry teams as part of the UC-Irvine ICS Student Show Case. Over the next few weeks I will be kicking the tires and evaluating the battle worthiness of the GME Test Suite Creator delivery which included source code and a complete set of documentation.I certainly had a wonderful time interacting with the team and participating in this program!The GME Test Suite Creator Team (from left to right: Gabriela Marcu, Peter Lee, Michelle Alvarez, Jason Dramby and George Pirocanac)", "Editor's note (Pat Copeland, Engineering Productivity Director, Google): On occasion we ask a special guest to post on an interesting topic. Ross has some really interesting ideas about how to use games to influence behavior. I'm a big fan. Enjoy!Ross Smith, Director of Test, Microsoft Windows Core Security\u201cQuality takes you out of yourself, makes you aware of the world around you.\u201d [1]The \u201cGamer Generation\u201d is here. According to the Washington Post, in 2006 at least 40% of Americans play video games on a computer or a console. The largest demographic is 18-34 year-olds, many of whom are working as our friends and peers as software developers and testers. There is a great book, Got Game, which describes the impact of games on the workforce of today and in the future.Many businesses are wondering these days how to use simple concepts popular in video games and apply them to work.It\u2019s happening already. From a construction site with a sign posted with \u201cdays since last accident\u201d to the car dealer with salespersons leader board scrawled out in chalk. Simple competition helps work get done in all kinds of industries. The success of sales contests is a great example of how well competition works.\u201cIn every job that must be done, there is an element of fun. You find the fun and\u2014SNAP\u2014the job\u2019s a game!\u201d \u2014Mary PoppinsA company called Seriosity has made interesting use of video game concepts at work with their application of virtual currency to email. To help with information overload from an overflowing inbox, email senders can \u201cspend\u201d virtual currency (Serios) to help prioritize their messages. Spending to denote the importance of a message is not a whole lot different than buying new golf clubs with virtual money earned in a Tiger Woods game, buying new tires in a car racing game \u2013 or using gamer points to upgrade your armor in an RPG (Role-playing Game). Wired Magazine had a great article on the use of WOW as a leadership training game. Playing games at work is more than just a quick game of online Hearts with one finger on the \u201cboss key\u201d. Games at work can have real impact.With other industries using productivity games, what about software development? How can productivity games help testers improve the quality of the products they test? One of the more famous test techniques is the bug bash. Teams pick a certain date, and everyone bangs on a certain area of the product to find bugs. At the end, prizes are awarded while everyone enjoys pizza and drinks. That sounds a lot like game, doesn't it? It\u2019s also a perfect example of a simple productivity game.As the world of software gets more complex and the test matrix grows, there is no shortage of opportunities for testers to do more testing. Quality is always \u201cimproved\u201d, it\u2019s never completed, fixed or finished. Therefore, testers must choose from a variety of techniques and activities to pursue tasks with the highest return and value. In 1990, Harry Markowitz won the Nobel Prize for his work on Portfolio Selection (that he began in the 1950\u2019s). The theory, familiar in financial markets and mutual funds, is that a portfolio of diverse assets with varying risk and returns actually reduces risk in time uncertain conditions. Testers can benefit from the work on portfolio selection by varying the techniques they use to find defects. However, some techniques are more expensive, more difficult, or less attractive than others. Some teams, in some situations, can rely on the altruism and volunteer efforts \u2013 organizational citizenship behaviors \u2013 of their members to take on the more difficult tasks. However, that doesn't always work, and other incentives may be required. That\u2019s where productivity games can help.When a project needs a bit of a push towards a certain behavior, building a game around it can help. For example, if a test team wants to improve the usage of unit tests by developers, encourage the team to \u201ceat their own dogfood\u201d or run automated tests on personal machines overnight, then creating a game will help people\u2019s motivation can motivate people to participate in those activities. Productivity games differ from traditional games in a few distinct ways. The goal of a productivity game is to attract players. These games don\u2019t need winners as much as they need players. In many cases, productivity games don\u2019t even need prizes. Productivity games can be simple. There\u2019s no need to invest in expensive graphics or 3-D modeling \u2013 people play the game to compete around a work activity, not because of the appeal of the user interface. A simple leader board might be all that\u2019s needed. Using games at work is a powerful and effective method to influence change in organizational behavior, and therefore requires care in the design and use. It is possible to overdo the use of games to a point where they are counter-productive or ineffective. Successful game design is best achieved through experience and experimentation, and the goal should be to keep things interesting enough to always attract players. A game where one player leaps out to an insurmountable lead is not effective, because that will discourage others from playing. When a team wants to encourage testers to invest in fault injection techniques, setting up a leader board with bug totals found via fault injection techniques will attract attention of other testers, often just by word-of-mouth. You should encourage team-wide competition, smack talk, and public embarrassment as a means to draw attention. Games should be short in duration. Many testers remember the Dilbert \u201cI\u2019m gonna write me a new minivan\u201d from 11/13/95. This is what results from a poorly designed productivity game.Productivity games are great at motivating people to do things and change behavior. And as Uncle Ben said to Spiderman, \u201cWith great power comes great responsibility\u201d. It\u2019s critical to have specific goals for the games, and to understand the impact before deploying a game \u2013 it\u2019s pretty likely that people will play, at least initially, and the impact may not be what you intended. Keep the duration of games short in order to be able to adjust. And finally, keep the games focused on volunteer or \u201cextra\u201d activities. A game designed around an individual\u2019s job or portions of everyone\u2019s regular job can introduce unusual feelings when it comes to rewards and performance evaluation. It\u2019s easier to steer clear of that than to try to figure out the right balance.Shigeru Miyamoto, who designed Super Mario Bros., talks about games as \u201cminiature gardens\u201d, metaphorically representing the cultural values, humanity, and challenges of everyday life the way a miniature garden might represent a real one. The whole idea, for me, is about the exploration of a hakoniwa, a miniature garden. It's like a garden in a box, where if you look at it from different angles, you can see different plants and arrangements. And you have that sense of surprise and exploration. There's always things you can find. [2]Games are a lot like testing. Software quality assurance and testing efforts represent a proxy for real users in much the same way that games are a proxy for the real world. The challenges that Mario faces with fire-breathing dragons and knife throwing turtles are balanced by the allure of coins and princesses. A testing organization has a good idea of the tools available in their portfolio to improve quality and eliminate those \u201cfire-breathing dragons\u201d for their customers. Productivity games allow teams to offer coins and position rewards around their equivalents of Markowitz\u2019s \u201cdiverse assets\u201d at their disposal, in the form of defect detection and removal techniques, to improve quality. For the players, productivity games offer people the ability to learn, solve problems, and earn a reward. They offer a way to challenge players to compete, to explore and discover, and to establish a surrogate identity or status via a leader board. There are many fundamental motivations to play a game. \u201cIf you build it, they will come [3].\" A good productivity game designer can build a game around \u201cwork that needs to be done that no one wants to do\u201d \u2013 and like the lone teenager who plays Halo for weeks on end, people will take on that work. Experiment and explore \u2013 the results can be surprising.I\u2019d be interested in any comments about Productivity Games that you might have.Thanks,Ross Smithhttp://www.defectprevention.org/ Interesting Links: When Work becomes a GameSerious Games InitiativeGot Game - How the Gamer Generation is Reshaping Business Forever Chapter 5 \u2013 Productivity Games - The Practical Guide to Defect Prevention2008 Serious Games Summit presentationMiniature Gardens and Magic Crayons: Games, Spaces, & WorldsHBR Leadership\u2019s Online LabsNPR: Software lets users assign value to emailGame Tycoon blog (How Video Games are transforming the Business World) Productivity Games BlogThe Levity EffectSpecial thanks to Joshua Williams for his help with this post.[1] Robert Pirsig, Zen and the Art of Motorcycle Maintenance[2] Wired Magazine Interview http://blog.wired.com/games/2007/12/interview-super.html[3] Field of Dreams, 1989", "Posted by Markus Clermont, John Thomas          This is the first in a two part blog series titled 'Taming the Beast: How to test AJAX applications\". In this part we discuss some philosophies around web application testing and how it should be done the 'right' way. In part 2, we walk through a real example of designing a test strategy for an AJAX application by going 'beyond the GUI.'BackgroundTypically we address the problem of testing an AJAX application through a plethora of big end-to-end tests and (hopefully) high unit-test coverage. Here we outline the main problems with this approach and demonstrate an effective testing strategy for a sample GWT-based application which goes beyond \"testing just through the GUI.\"Problems with GUI-only testingIn general testing through the GUI:is expensive (takes long to write the tests and execution is resource-intensive)gives limited insight into the systemoften take only the 'happy paths' into accountcombines multiple aspects in a single testis slow and brittleneeds a lot of maintenanceis hard to debugAnd while unit tests don't suffer from many of these problems, they alone are not sufficient mainly because they:give little insight how the components interact with each otherdon't give confidence that the business logic and functionality of the system meets the requirementsSolutionAlthough there is no 'one size fits all' solution, there are some basic principles we can use to solve the testing problem for web applications:Invest in integration tests (identify the smallest sub-system)Separation of Concerns (don't do the set-up through the interface you are testing)Test each interface separately (mock out everything that you are not testing)Consider dependencies in production (figure out how dependencies can fail, and test that)Use a mix of strategies and tools. There is no silver bullet.And no... you cannot scrap all of your end-to-end testsA recipe for testing goodnessUsing the principles above we can build up a recipe for testing web applications. In the second part of our blog we will walk through each of these steps for a real web application.Explore the system's functionalityIdentify the system's architectureIdentify the interfaces between componentsIdentify dependencies and fault conditionsFor each function:Identify the participating componentsIdentify potential problemsTest in isolation for problemsCreate a 'happy path' testEnd note: value of a testA question commonly asked by developers when writing tests is, \"is this really worth my time?\" The short answer is \"always!\". Since fixing a bug is way more expensive than preventing it in the first place, writing good tests is always worth the time.While there are many different classifications of tests, the most common way of classifying them is based on their size and the areas of a product they test. Each test answers specific questions about the product:Unit test: is the method fulfilling its contract?Small integration test: Can two classes interact with each other? Medium integration test: Is a class interacting properly with all its dependencies? Does it anticipate and handle errors correctly? Are the needed functions exposed on an API/GUI? Sub-system test: Can two sub-systems interact with each other? Does one of them anticipate all errors of the other and does it deal with them appropriately?System test: Does the entire system behave as expected?Keeping this questions in mind, testing at various levels allows us to write more focused and meaningful tests. Remember that effective tests are those that provide quick and useful feedback, i.e. quickly identify issues and pin point the exact location of the issue.In the next episode we will walk through the recipe proposed above using a real web application.", "When you want to test code that depends on something that is too difficult or slow to use in a test environment, swap in a test double for the dependency.A Dummy passes bogus input values around to satisfy an API.Item item = new Item(ITEM_NAME);ShoppingCart cart = new ShoppingCart();cart.add(item, QUANTITY);assertEquals(QUANTITY, cart.getItem(ITEM_NAME));  A Stub overrides the real object and returns hard-coded values. Testing with stubs only is state-based testing; you exercise the system and then assert that the system is in an expected state.  ItemPricer pricer = new ItemPricer() { \u00a0\u00a0public BigDecimal getPrice(String name){ return PRICE; }};ShoppingCart cart = new ShoppingCart(pricer);cart.add(dummyItem, QUANTITY);assertEquals(QUANTITY*PRICE, cart.getCost(ITEM_NAME));A Mock can return values, but it also cares about the way its methods are called (\u201cstrict mocks\u201d care about the order of method calls, whereas \u201clenient mocks\u201d do not.)  Testing with mocks is interaction-based testing; you set expectations on the mock, and the mock verifies the expectations as it is exercised.  This example uses JMock to generate the mock (EasyMock is similar):Mockery context = new Mockery();final ItemPricer pricer = context.mock(ItemPricer.class);context.checking(new Expectations() {{\u00a0\u00a0one(pricer).getPrice(ITEM_NAME);\u00a0\u00a0will(returnValue(PRICE));}});ShoppingCart cart = new ShoppingCart(pricer);cart.add(dummyItem, QUANTITY);cart.getCost(ITEM_NAME);     context.assertIsSatisfied();A Spy serves the same purpose as a mock: returning values and recording calls to its methods.  However, tests with spies are state-based rather than interaction-based, so the tests look more like stub style tests. TransactionLog log = new TransactionLogSpy();ShoppingCart cart = new ShoppingCart(log);cart.add(dummyItem, QUANTITY);assertEquals(1, logSpy.getNumberOfTransactionsLogged()); assertEquals(QUANTITY*PRICE, log.getTransactionSubTotal(1));A Fake swaps out a real implementation with a simpler, fake implementation. The classic example is implementing an in-memory database. Repository repo = new InMemoryRepository();ShoppingCart cart = new ShoppingCart(repo);cart.add(dummyItem, QUANTITY);assertEquals(1, repo.getTransactions(cart).Count); assertEquals(QUANTITY, repo.getById(cart.id()).getQuantity(ITEM_NAME));While this episode used Java for its examples, all of the above \u201cfriends\u201d certainly exist in C++, Python, JavaScript, and probably in YOUR favorite language as well.Remember to download this episode of Testing on the Toilet and post it in your office.", "A brief reminder that there are only three days left to submit proposals for this year's Google Test Automation Conference. The proposal deadline is June 6th, after which the selection process will begin.The Call For Proposals announcement is available at the Google Testing Blog here.See you in October!Lydia AshGTAC Conference Chair - 2008", "Posted by Rajat Jain and Marc Kaplan, Infrastructure Test Engineering \nGoogle is unique in that we develop most of our software infrastructure from scratch inside the company. Distributed filesystems are no exception, and we have several here at Google that all serve different purposes. One such filesystem is the Google File System (GFS) which is used to store almost all data at Google. Although, GFS is the ultimate endpoint for much of the data at Google, there are many other distributed file systems built on top of GFS for a variety of purposes (see Bigtable, for example -- but several others also exist) with developers constantly trying to improve performance to meet the ever-increasing demands of serving data at Google. The challenge to the teams testing performance of these filesystems is that running performance tests, analyzing the results, and repeating over and over is very time consuming. Also, since each filesystem is different, we  have traditionally had different performance testing tools for the different filesystems, which made it difficult to compare performance between the filesystems, and led to a lot of unnecessary maintenance work on the tools.In order to streamline testing of these filesystems, we wanted to create a new framework that is capable of easily performance testing the filesystems at Google. The goals of this system were as follows:\n\nGeneric: The testing framework should be generic enough to test any type of file-system inside Google. In having a generic framework, it will be easier to compare the performance of different filesystems across many operations.\nEase of use: The framework should be easy enough to use so that software developers can design and run their own tests without any help from the test team.\nScalable: Testing can be done at various scales depending on the scalability of the FS. The framework can issue any number of operations simultaneously. So, for a testing a Linux file system, we might only issue 1000 parallel requests, while for the Google File System, we might want to issue requests at a much larger scale.\nExtensible:\n\nFirstly, it should be easy to add a new kind of operation in the framework, if its developed in future (For example, RecordAppend operation in GFS).\n\n\nAlso, the framework should allow the user to easily generate complex types of loadscenarios on the server. For example, we might want to have a scenario in which we issue File Create operations simultaneously with Read, Write, and Delete operations. Thus, we want a good mix of operations but not in a randomized way, so that we can have benchmark results.\n\nUnified testing: The framework should be stand-alone or independent ie it should be a one-stop solution to setup, run the tests and monitor the results.\n\nWe developed a framework which allows us to achieve all the above mentioned goals. We used the Google's generic File API for writing the framework, since every file system can be tested just by changing the file namespace in which the testing data will be generated (e.x. /gfs vs. /bigtable). Following Google's standard, we developed a Driver + Worker system. The Driver co-ordinates the overall test, by reading configuration files to set up the test, automatically launching different number of workers depending on the load, monitoring the health of workers, collecting performance data from each worker and calculating the overall performance. TheWorker class is the one which loads the file systems with appropriate operations. A worker is an abstract class and a new child class can be created for each file operation, which gives us the flexibility to add any operation we want in the future. A separate Worker instance is launched on a different machine depending on the load that we want to generate. It is simple to run more or less workers on remote machines simply by changing the config file.The test is divided into various phases. In a phase, we can run a single operation N number of times (with a given concurrency) and collect performance data. So, we can run a create phase followed by a write phase followed by a read phase. We can also have multiple sub-phases inside a phase, which gives us the ability to generate many different simultaneous operations on the system. For example, in a phase, we might add three subphases create, write and delete, which will issue all the different kinds of operations simultaneously on remote client machines against the distributed filesystem.It is instructive to look at an example config file for an idea of how the load is specified against this filesystem:# Example of createphase {  label: \"create\"  shards: 200  create {    file_prefix: \"metadata_perf\"  }  count: 100}So in the example above, we launch 200 shards (which all run of different client machines) that all do creates of files with a prefix of metadata_perf, and suffixes based upon the index of the worker shard. In practice, the user of the performance test passes a flag into the performance test binary that specifies a base path to use: i.e. /gfs/cell1/perftest_path, and the resulting files will be /gfs/cell1/perftest_path/worker.i/metadata_perf.j, for i=1 until i=#shards, and j=1, until j=count.# Example of using subphasesphase {  label: \"subphase_test\"  subphase {    shards: 200    label: \"stat_subphase\"    stat {      file_prefix: \"metadata_perf\"  }    count: 100  }  subphase {    shards: 200    label: \"open_subphase\"    open {      file_prefix: \"metadata_perf\"    }    count: 100  }}In the example above, we simultaneously do stats and opens of the files that were initially created in the create phase. Different workers execute these, and then report their results to the driver.On conclusion of the test, the driver prints a performance test results report that details the aggregate results of all of the clients, in terms of MB/s for data intensive ops, ops/s for metadata intensive ops, and latency measures of central tendency and dispersion.In conclusion, Google's generic File API, use of Driver & Workers and the concept of phaseshave been very useful in the development of the performance testing framework and hence making performance testing easier. Almost as important, the fact that this is a simple script-driven method of testing complex distributed filesystems has resulted in an ease of use that has given both developers and testers, the ability to quickly experiment and iterate, resulting in faster code development and better performance overall.", "Full statement coverage may be necessary for good testing coverage, but it isn't sufficient. Two places where statement coverage will be inadequate are branches and loops. In this episode, we'll look at branches, and specifically the differences between statement coverage and branch coverage.Let's consider a case where branch coverage and statement coverage aren't the same.  Suppose we test the following snippet.  We can get complete statement coverage with a single test by using a berserk EvilOverLord:bool DeathRay::ShouldFire(EvilOverLord& o, Target& t) {\u00a0\u00a0double accumulated_rage = 0.0;\u00a0\u00a0if (o.IsBerserk())\u00a0\u00a0\u00a0\u00a0accumulated_rage += kEvilOverlordBerserkRage;\u00a0\u00a0accumulated_rage += o.RageFeltTowards(t);\u00a0\u00a0return (accumulated_rage > kDeathRayRageThreshold);}But what if DeathRay should fire at this Target even with a non-berserk Overlord?  Well, we need another test for that.  What should the test be?  Let's rewrite the code a little bit.  We would never see code like this in the real world, but it'll help us clarify an important point.bool DeathRay::ShouldFire(EvilOverLord& o, Target& t) {\u00a0\u00a0double accumulated_rage = 0.0;\u00a0\u00a0if (o.IsBerserk()) {\u00a0\u00a0\u00a0\u00a0accumulated_rage += kEvilOverlordBerserkRage;\u00a0\u00a0} else {\u00a0\u00a0}\u00a0\u00a0accumulated_rage += o.RageFeltTowards(t);\u00a0\u00a0return (accumulated_rage > kDeathRayRageThreshold);}Why do we add an else clause if it doesn't actually do anything?  If you were to draw a flowchart of both snippets (left as an exercise \u2013 and we recommend against using the paper provided), the flowcharts would be identical.  The fact that the else isn't there in the first snippet is simply a convenience for us as coders \u2013 we generally don't want to write code to do nothing special \u2013 but the branch still exists...  put another way, every if has an else.  Some of them just happen to be invisible.When you're testing, then, it isn't enough to cover all the statements \u2013 you should cover all the the edges in the control flow graph \u2013 which can be even more complicated with loops and nested ifs.  In fact, part of the art of large-scale white-box testing is finding the minimum number of tests to cover the maximum number of paths.  So the lesson here is, just because you can't see a branch doesn't mean it isn't there \u2013 or that you shouldn't test it.Remember to download this episode of Testing on the Toilet and post it in your office.", "Alek Icev Ads Quality Test Engineering ManagerI'd like to take a second and introduce you to the team testing the ads ranking algorithms. We'd like to think that we had a hand in the webs shift to a \"content meritocracy\". As you know the Google search results are unbiased by human editors, and we don't allow buying a spot at the top of the results list. This idea builds trust with users and allows the community to decide what's important.Recently, we started applying the same concept to the online advertising. We asked ourselves how to bring the same level of \"content meritocracy\" to the online advertising where everybody pays to have ads being displayed on Google and on our partner sites. In other words, we needed to change a system that was predominately driven by human influence into one that build its merit based on feedback from the community. The idea was that we would penalize the ranking of paid ads in several circumstances: few users were clicking on a particular ads, an ad's landing page was not relevant, or if users don't like an ad's content.  We want to provide our users with absolutely the most relevant ads for their click. In order to make our vision a reality we are building one of the largest online and real time machine learning labs in the world. We learn from everything: clicks, queries, ads, landing pages, conversions... hundreds of signals.The Google Ad Prediction System brought new challenges to Test Engineering. The problem is that we needed to build the abstraction layers and metrics systems that allow us to understand if the system is organically getting better or regressing. Put another way, we started off lacking the decision tree or a perceptron that a bank or credit card company have embedded into their risk analysis, or the neural net that's behind all broken speech recognition, or the latest tweaks on expectation-maximization algorithms needed to predict the protein transcription in the cells. The amount and versatility of the data that Google Ad Prediction models learn is immense. The amount of time needed to make the prediction is counted in milliseconds. The amount of computing resources, ads databases and infrastructure needed to serve predictions on every ad that is showing today is beyond imagination. Our challenge is to train and test learning models, that span clusters of servers and databases, simulate ads traffic and having everything compiled and running from the latest code changes submitted to the huge source depot. And the icing on the cake is to run that on 24/7 schedule.On top of all of the technical challenges, we are also challenging the industry definition of \"testing\" and are believers in automated tests that are incorporated upstream into the development process and run on continuous basis. Ads Quality Test Engineering Group at Google, works on a bleeding edge testing infrastructure to test, simulate and train Google Ads Prediction systems in real time.", "Posted by Joel Hynoski, Test Manager, Chat ClientsTesting Google Talk is challenging -- we have multiple client implementations, between the Google Talk client, the Google Talk Gadget, and Gmail chat, while also managing new features and development. We rely heavily on automation. Yet there's still a need to do manual testing before the release of the product to the public.We've found that one of the best ways to unearth interesting bugs in the product is to use Exploratory Testing (http://www.satisfice.com/articles/et-article.pdf) The trouble with ET is that while there appears to be a genetic disposition to be naturally good at exploring the product effectively, it's very easy to miss great swathes of the product when one follows their intuition through the product rather than focusing on coverage metrics. And speaking of coverage, how do we measure how well a team is doing finding bugs and getting coverage over the functional use cases for the product? All of the things that we rely on to measure the quality of the product, boundary and edge cases being covered? Plus, if not everyone is proficient at ET, how do we solve the overhead of having an experienced team member looking over people's shoulders to make sure they are executing well?To do this, we start with the definition of a Test Strategy. This is where we outline the approach we are taking to the testing of the product as a whole. It's not super-detailed -- instead it mentions the overarching areas that need to be tested, whether automation can be used to test the area, and what role manual testing needs to play. This information lets developers and PMs know what we think we need to test for the product, and allows them to add unit tests etc to cover more ground.Some basic test case definition go into the Test Plan. The aim of the test plan (and any test artifacts generated) is not to specify a set of actions to be followed in a rote manner, but instead a rough guide that encourages creative exploration. The test plan also acts as the virtual test expert, providing some framework under which exploratory testing can be executed effectively by the team. The plan decomposes the application into different areas of responsibility, that are doled out to members of the team in sessions that are one-working-day or less duration. By guiding people's thinking, we can cover the basics, fuzzy cases, and avoids a free-for-all, duplication, and missed areas.Finally we get a status report from the testers every day, that describes the testing that was performed that day, any bugs raised, and blocking issues identified. The reports acts as an execution of the \"contract\" and gives traceability, and the ability to tweak exploratory testing that has gone off track from where we've determined we need to concentrate efforts. We can use these status reports along with bug statistics to gauge the effectiveness of the test sessions.This is approach is fairly simple, but sometimes simple works best. Using this method has allowed us to make the best use of test engineers and maximized the effectiveness of each test pass. It's proven itself to be a fruitful approach and balances the need for reporting and accountability with the agility of exploratory testing.", "It's hard to test code that uses singletons.   Typically, the code you want to test is coupled strongly with the singleton instance.  You can't control the creation of the singleton object because often it is created in a static initializer or static method.  As a result, you also can't mock out the behavior of that Singleton instance.If changing the implementation of a singleton class is not an option, but changing the client of a singleton is, a simple refactoring can make it easier to test.  Let's say you had a method that uses a Server as a singleton instance:public class Client {\u00a0\u00a0public int process(Params params) {\u00a0\u00a0\u00a0\u00a0Server server = Server.getInstance();\u00a0\u00a0\u00a0\u00a0Data data = server.retrieveData(params);\u00a0\u00a0\u00a0\u00a0...\u00a0\u00a0}}You can refactor Client to use Dependency Injection and avoid its use of the singleton pattern altogether.  You have not lost any functionality, and have also not lost the requirement that only a singleton instance of Server must exist.  The only difference is that instead of getting the Server instance from the static getInstance method, Client receives it in its constructor.  You have made the class easier to test!public class Client {\u00a0\u00a0private final Server server;\u00a0\u00a0public Client(Server server) {\u00a0\u00a0\u00a0\u00a0this.server = server;\u00a0\u00a0}\u00a0\u00a0public int process(Params params){\u00a0\u00a0\u00a0\u00a0Data data = this.server.retrieveData(params);\u00a0\u00a0\u00a0\u00a0...\u00a0\u00a0}}When testing, you can create a mock Server with whatever expected behavior you need and pass it into the Client under test:public void testProcess() {\u00a0\u00a0Server mockServer = createMock(Server.class);\u00a0\u00a0Client c = new Client(mockServer);\u00a0\u00a0assertEquals(5, c.process(params));}Remember to download this episode of Testing on the Toilet and post it in your office.", "Consider the following function, which modifies a client-supplied object:bool SomeCollection::GetObjects(vector* objects) const {\u00a0\u00a0objects->clear();\u00a0\u00a0typedef vector::const_iterator Iterator;\u00a0\u00a0for (Iterator i = collection_.begin(); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0i != collection_.end(); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0++i) {\u00a0\u00a0\u00a0\u00a0if ((*i)->IsFubarred()) return false;\u00a0\u00a0\u00a0\u00a0objects->push_back(*i);\u00a0\u00a0}\u00a0\u00a0return true;}Consider when GetObjects() is called.  What if the caller doesn't check the return value, and assumes the data is in a valid state when it actually is not?  If the caller does check the return value, what can it assume about the state of its objects in the failure case?  When GetObjects() fails, it would be much better if either all the objects were collected or none of them.  This can help avoid introducing hard to find bugs.By using good design contracts and a solid implementation, it is reasonably easy to make functions like GetObjects() behave like transactions.  By following Sutter's rule of modifying externally-visible state only after completing all operations which could possibly fail [1], and mixing in Meyers's \u201cswap trick\u201d [2], we move from the realm of undefined behavior to what Abrahams defines as the strong guarantee [3]:bool SomeCollection::GetObjects(vector* objects) const {\u00a0\u00a0vector known_good_objects;\u00a0\u00a0typedef vector::const_iterator Iterator;\u00a0\u00a0for (Iterator i = collection_.begin(); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0i != collection_.end(); \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0++i) {\u00a0\u00a0\u00a0\u00a0if ((*i)->IsFubarred()) return false;\u00a0\u00a0\u00a0\u00a0known_good_objects->push_back(*i);\u00a0\u00a0}\u00a0\u00a0objects->swap(known_good_objects);\u00a0\u00a0return true;}At the cost of one temporary and a pointer swap, we've strengthened the contract of our interface such that, at best, the caller received a complete, new collection of valid objects; at worst, the state of the caller's objects remains unchanged.   The caller might not verify the return value, but will not suffer from undefined results.  This allows us to reason much more clearly about the program state, making it much easier to verify the intended outcome with automated tests as well as recreate, pinpoint, and banish bugs with regression tests.http://www.gotw.ca/publicationsScott Meyers, Effective C++http://www.boost.org/more/generic_exception_safety.htmlRemember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Lydia Ash - GTAC Conference ChairOverviewThe Test Automation Conference has been hosted by Google over the past several years bringing together a select set of industry practitioners as speakers and participants around the topic of software testing and automation. The annual conference provides a forum for presentations on important topics in our field connecting professions with others in their field through collaboration and targeted breakout sessions, but also has served the testing community by bringing the presentations public allowing anyone access to the information.GTAC conference participants are typically software engineers who are actively working on problems of software quality, automated testing, and testing techniques. GTAC conferences cover a number of areas, but many presentations focus on themes such as advanced techniques in quality evaluation, advanced approaches to automating software testing, and experiences and findings from quality efforts on software projects.The ConferenceThe conference is a two day event comprised of a single track of presentations. The philosophy is to engage a small set of active participants who all experience the same topics carrying the discussions into lightning talks, speaker Q&A, and topical discussion groups. The emphasis for the 2008 GTAC conference is solving the hard engineering problems in the quality of our software. Each year we have worked to identify a location that has a unique profile of technology professionals. This year the conference will be held in Seattle, WA, USA on October 23 and 24.Presentations are targeted at experienced engineers actively working on software quality. We encourage innovative ideas, controversial experiences, problems, and solutions that further the discussion of engineering and software quality. Presentations are generally 45 min in length and speakers should be prepared for an active question and answer session following their presentation.Process of SelectionAll presentation submissions will be handled electronically. Presentation proposals should be a relatively detailed extended abstract including the topic, outline, and details of what will be presented. Presentation proposals should be emailed to gtac@google.comAll presentation proposals must be received by June 6, 2008. Where employer or disclosure authorization is needed, the author(s) will need to obtain this prior to submitting their proposal.The program committee will evaluate proposals based on the quality and relevance. All submissions will be held confidentially prior to contacting the selected presenters and the publication in the proceedings.NotificationAccepted proposal authors will be contacted on or before July 10 to confirm their availability and travel needs. Authors of proposals not selected will be notified on or before July 10. Accepted proposals will be presented at the conference and made available to the public on YouTube.CopyrightTAC requires presenters to present at the conference and permit their presentation to be made available on YouTube.Important Dates for PresentationsJune 6 - Final deadline for proposal submissions for presentations July 10 - Deadline for selected presenters to be contacted by the selection committee and notified of their acceptanceOctober 23 and 24 - GTAC conference in SeattleAttendeesTAC conference has worked to invite a select audience, each member applies to the conference for committee selection. This is to ensure active participation from each attendee and provide a variety of technical perspectives to interact, discuss, and network.Important Dates for AttendeesJuly 7 - Call for attendee profile submissionsJuly 25 - Deadline for attendee profilesAugust 11 - Selected attendees to be notified by the conference committee, registration opens August 29 - Registration deadline, wait-list opensSeptember 19 - Wait list notifications and attendee closureOctober 23 and 24 - GTAC conference in SeattleQuestionsIf you have questions about the submission process or potential topics please send a mail to us at: gtac@google.com Please see the Google Testing blog for more information: http://googletesting.blogspot.com/search/label/GTAC", "Before the end of our last Google Test Automation Conference in August 2007, we were already getting questions from participants and blog readers wondering about the next conference. Now we can tell you when and where that will happen (drum roll please)... the 2008 Test Automation Conference will be held October 23 and 24 in Seattle. More details will be coming shortly...As with previous years, the focus of the conference will be solving software engineering challenges using tools and automation, with special focus on SaaS. Engineers in the testing world are frequently so busy shipping software that they do not take the time to share the technical details of the work they are doing, the approaches that are working, and the lessons they have learned. There will be a call for proposals in late April.As has been the precedent in previous years, our conference is for active and vocal participation, not quiet attendance. GTAC is a place to share great ideas and to get challenged. As we have done previously, attendants will apply by proposing what they will bring to the conference and how they can further the discussions. Applications for attendance will be opened in late June.We are hard at work developing the conference. Please send suggestions, questions and recommendations to: gtac@google.com or post your comments here to the blog.", "Flaky tests make your life more difficult. You get failure notifications that aren't helpful. You might become numb to failures and miss an actual failure condition. Your changes might get unfairly blamed for causing a flaky test to fail. Unfortunately, a myriad of factors can make a test flaky. Today, we tackle a simple example: file access from a unit test. Take this function and its test:def CreateGapLease(self):\u00a0\u00a0data_file = open('/leases/gap', 'w+')\u00a0\u00a0data_file.write(contract_data)\u00a0\u00a0data_file.close()def testCreateGapLease(self):\u00a0\u00a0contract_writer.CreateGapLease()\u00a0\u00a0self.assertEqual(ReadFileContents('/leases/gap'),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 contract_data)What if /leases/gap already exists and contains some data? testCreateGapLease will fail. This is a general problem where preconditions are assumed to be correct. This could just as easily happen by assuming a database contains the proper information (or no information). What if another test that uses that file was running concurrently? If you really want to test your code using live resources, always check your assumptions. In this case, clearing the file at the start of the test can reduce its brittleness:def testCreateGapLease(self):\u00a0\u00a0if os.path.exists(lease_file):\u00a0\u00a0\u00a0\u00a0RemoveFile(lease_file)\u00a0\u00a0...Unfortunately, this doesn't completely eliminate the flakiness of our test. If /leases/gap is an NFS path or can be written to by a different test, our test can still fail unexpectedly. It's better for the test to use a unique resource. This can be accomplished with a small refactoring of CreateGapLease:def CreateGapLease(self, lease_path=None):\u00a0\u00a0if lease_path is None:\u00a0\u00a0\u00a0\u00a0lease_path = '/leases/gap'\u00a0\u00a0...The callers of CreateGapLease can continue invoking it as usual, but the unit test can pass in a different path: def testCreateGapLease(self):\u00a0\u00a0lease_file = os.path.join(FLAGS.test_tmpdir, 'gap')\u00a0\u00a0contract_writer.CreateGapLease(lease_path=lease_file)\u00a0\u00a0self.assertEqual(ReadFileContents(lease_file),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 contract_data)Of course, to make your test as fast as possible, it would be better to forgo disk access altogether by using a mock file system.Remember to download this episode of Testing on the Toilet and post it in your office.Due to illness availability of the PDF will be slightly delayedThe PDF is now available at the above link", "How can a method be well tested when it's inputs can't be clearly identified? Consider this method in Java:/** Return a date object representing the start of the next minute from now */public Date nextMinuteFromNow() {\u00a0\u00a0long nowAsMillis = System.currentTimeMillis();\u00a0\u00a0Date then = new Date(nowAsMillis + 60000);\u00a0\u00a0then.setSeconds(0);\u00a0\u00a0then.setMilliseconds(0);\u00a0\u00a0return then;}There are two barriers to effectively testing this method:There is no easy way to test corner cases; you're at the mercy of the system clock to supply input conditions.When nextMinuteFromNow() returns, the time has changed. This means the test will not be an assertion, it will be a guess, and may generate low-frequency, hard-to-reproduce failures... flakiness! Class loading and garbage collection pauses, for example, can influence this.Is System.currentTimeMillis(), starting to look a bit like a random number provider? That's because it is! The current time is yet another source of non-determinism; the results of nextMinuteFromNow() cannot be easily determined from its inputs. Fortunately, this is easy to solve: make the current time an input parameter which you can control.public Date minuteAfter(Date now) {\u00a0\u00a0Date then = new Date(now.getTime() + 60000);\u00a0\u00a0then.setSeconds(0);\u00a0\u00a0then.setMilliseconds(0);\u00a0\u00a0return then;}// Retain original functionality@Deprecated public Date nextMinuteFromNow() {\u00a0\u00a0return minuteAfter(new Date(System.currentTimeMillis()));}Writing tests for minuteAfter() is a much easier task than writing tests for nextMinuteFromNow():public void testMinuteAfter () {\u00a0\u00a0Date now = stringToDate(\"2012-12-22 11:59:59.999PM\");\u00a0\u00a0Date then = minuteAfter(now);\u00a0\u00a0assertEquals(\"2012-12-23 12:00:00.000AM\", dateToString(then));}This is just one way to solve this problem. Dependency Injection and mutable Singletons can also be used.Remember to download this episode of Testing on the Toilet and post it in your office.", "Recently, somewhere in the Caribbean Sea, you implemented the PirateShip class. You want to test the cannons thoroughly in preparation for a clash with the East India Company. This requires that you run the crucial testFireCannonDepletesAmmunition() method many times with many different inputs. TestNG is a test framework for Java unit tests that offers additional power and ease of use over JUnit. Some of TestNG's features will help you to write your PirateShip tests in such a way that you'll be well prepared to take on the Admiral. First is the @DataProvider annotation, which allows you to add parameters to a test method and provide argument values to it from a data provider.public class PirateShipTest {\u00a0\u00a0@Test(dataProvider = \"cannons\")\u00a0\u00a0public void testFireCannonDepletesAmmunition(int ballsToLoad,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 int ballsToFire,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 int expectedRemaining) {\u00a0\u00a0\u00a0\u00a0PirateShip ship = new PirateShip(\"The Black Pearl\");\u00a0\u00a0\u00a0\u00a0ship.loadCannons(ballsToLoad);\u00a0\u00a0\u00a0\u00a0for (int i = 0; i \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ship.fireCannon();\u00a0\u00a0\u00a0\u00a0}\u00a0\u00a0\u00a0\u00a0assertEquals(ship.getBallsRemaining(), expectedRemaining);\u00a0\u00a0}\u00a0\u00a0@DataProvider(name = \"cannons\")\u00a0\u00a0public Object[][] getShipSidesAndAmmunition() {\u00a0\u00a0\u00a0\u00a0// Each 1-D array represents a single execution of a @Test that \u00a0\u00a0\u00a0\u00a0// refers to this provider. The elements in the array represent \u00a0\u00a0\u00a0\u00a0// parameters to the test call.\u00a0\u00a0\u00a0\u00a0return new Object[] {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{5, 1, 4}, {5, 5, 0}, {5, 0, 5}\u00a0\u00a0\u00a0\u00a0};\u00a0\u00a0}}Now let's focus on making the entire test suite run faster. An old, experienced pirate draws your attention to TestNG's capacity for running tests in parallel. You can do this in the definition of your test suite (described in an XML file) with the parallel and thread-count attributes.<suite name=\"PirateShip suite\" parallel=\"methods\" thread-count=\"2\">A great pirate will realize that this parallelization can also help to expose race conditions in the methods under test.Now you have confidence that your cannons fired in parallel will work correctly. But you didn't get to be a Captain by slacking off! You know that it's also important for your code to fail as expected. For this, TestNG offers the ability to specify those exceptions (and only those exceptions) that you expect your code to throw.@Test(expectedExceptions = { NoAmmunitionException.class })public void testFireCannonEmptyThrowsNoAmmunitionException() {\u00a0\u00a0PirateShip ship = new PirateShip(\"The Black Pearl\");\u00a0\u00a0ship.fireCannon();}Remember to download this episode of Testing on the Toilet and post it in your office.", "By Sharon Zhou, Kirkland Client Test LeadIn December, Google Pack shipped 10 new languages in 10 new countries/regions including China Pack. This was in addition to the 30 languages Pack was all ready available in. Localization testing for these 10 languages is not trivial. The testing needs to be done very quickly by experts in the language who may not have seen the application before. Localization testing (LQA) can also be costly since it requires multiple external vendors, and the LQA schedule is highly sensitive to changes in the product schedule.The process that has been followed so far has been to have each product documented by an engineer detailing the workflow to navigate to each area of the UI, the appropriate inputs to be entered at each step, and what should be expected. The documentation time is considerable, and changes with product changes. The vendors must each consume this documentation and become functional at using the product in order to navigate through the product. There are also challenges to get vendors the appropriate permissions to access our unreleased products, and to download them at the site where they work.To minimize the test cost, the Pack test team has implemented a significant amount of automation across the entire product driving the UI. One feature of the automation harness is the ability to record movies. For the new 10 languages, the Pack team tried a new process of using the automation to drive the UI, recording movies of the product UI, and sending these movies to our vendors along with a top level test plan. To evaluate the new approach, we also asked them to fill out a survey to have a quantitative concept of how much time we can save, and hence how much cost we can reduce.The survey results come back very positive and encouraging. We received valuable feedback on what vendors need to conduct a fast and efficient test pass. Overall, this experiment saved the vendors an estimated 25% of their time overall. It was just as effective as the previous process, but was much simpler for them to complete. Our next steps will be to drive more of the UI. If the automation can touch every page, link and dialog, it can replace the traditional LQA testing method of installing and running build as people perform functional testing.", "Posted by Antoine PicardWe have become test hoarders. Our focus on test-driven development, developer testing and other testing practices has allowed us to accumulate a large collection of tests of various types and sizes. Although this is valiant and beneficial, it is too easy to forget that each test, whether a unit test or a manual test has a cost as well. This cost should be balanced against the benefits of the test when deciding whether a test should be deleted or whether it should be written in the first place.Let's start with the benefits of a test. It is all too easy to think that the benefits of a test are to increase coverage or to satisfy an artificial policy set by the Test Certified program. Not so. Although it is difficult to measure for an individual test, its benefits are the number of bugs that it kept from reaching production.There are side benefits to well-tested code as well such as enforcing good design practices such as decomposition, encapsulation, etc. but these are secondary to avoiding bugs.Short examples of highly-beneficial tests are hard to come by, however counter-examples abound. The following examples have been anonymized but were found at various time in our code tree. Take this test:def testMyModule(self):mymodule.main()Although it probably creates a lot of coverage in mymodule, this test will only fail if main throws an exception. Certainly this is a useful condition to detect but it is wasteful to consume the time of a full run of mymodule without verifying its output. Let's look at another low-value test:def testFooInitialization(self):try:foo = Foo()self.assertEquals(foo.name, 'foo')self.assertEquals(foo.bar, 'bak')except:passThis one probably hits the bottom of the value scale for a test. Although it exercises Foo's constructor, catching all exceptions means that the test will never fail. It creates coverage but never catches any bugs.A fellow bottom-dweller of the value scale is the test that doesn't get fixed. This can be a broken test in a continuous build or a manual test that generates a bug that stays open: either way it's a waste of time. If it's an automated test, it is worth deleting and was probably not worth writing in the first place. If it's a manual test, it's probably a sign that QA is not testing what the PM cares about. Some even apply the broken-window principle to these tests saying that tests that don't get fixed give the impression that testing is not valuable.  Our final specimen is slightly higher value but still not very high. Consider the function Bar:def Bar():SlowBarHelper1()SlowBarHelper2()SlowBarHelper3() We could employ stubs or mocks to write a quick unit test of Bar but all we could assert is that the three helpers got called in the right order. Hardly a very insightful test. In non-compiled languages, this kind of test does serve as a substitute syntax-checker but provides little value beyond that.Let's now turn our attention to the dark side of testing: its cost. The budget of a whole testing team is easy to understand but what about the cost of an individual test?The first such cost is the one-time cost creating the test. Whether it is the time it takes to write down the steps to reproduce a manual test or the time it takes to code an automated test it is mostly dependent on the testability of the system or the code. Keeping this cost down is an essential part of test-driven development: think about your tests before you start coding.While the creation of a test has a significant cost, it can be dwarfed by the incremental cost of running it. This is the most common objection to manual testing since the salary of the tester must be paid with every run of the test but it applies to automated tests too. An automated test uses a machine while it's running, that machine and it's maintenance both have a cost. If a test requires specialized hardware to run, those costs go up. Similarly, adding a test that takes 20 minutes to run will consume 20 minutes of the time of each engineer that tries to run it, every time s/he tries to run it! If it's a test that's run before each check-in the cost of that test will go up rapidly. It could be worth the engineering time to reduce its run time to a more reasonable level.There is one more incremental cost to a test: the cost of its failure. Whenever a test fails, time is spent to diagnose the failure. The reduction of this cost is the reason behind two key principles of good testing:- don't write flaky tests: flaky tests waste time by making us investigate failures that are not really there- write self-diagnosing tests: a test should make it clear what went wrong when it fails to allow us to rapidly move towards a fixThe 'economics' of testing can be used to analyze various testing methodologies. For example, true unit tests (small, isolated tests) take one approach to this problem: they minimize the repeated costs (by being cheap to run and easy to diagnose) while incurring a slightly higher creation cost (have to mock/stub, refactor, ...) and slightly lesser benefits (confidence about a small pieces of the system as opposed to the overall system). By contrast, regression tests tend to incur a greater cost (since most regression tests are large tests) but attempt to maximize their benefits by targeting areas of previous failures under the assumption that those are most likely to have bugs in the future.So think about both the benefits and the costs of each test that you write. Weigh the one-time costs against the repeated costs that you and your team will incur and make sure that you get the benefits that you want at the least possible cost.", "Code coverage (also called test coverage) measures which lines of source code have been executed by tests. A common misunderstanding of code coverage data is the belief that:My source code has a high percentage of code coverage; therefore, my code is well-tested.The above statement is FALSE! High coverage is a necessary, but not sufficient, condition.Well-tested code\u00a0=======>\u00a0High coverage Well-tested code\u00a0<===X===\u00a0High coverageThe most common type of coverage data collected is statement coverage. It is the least expensive type to collect, and the most intuitive. Statement coverage measures whether a particular line of code is ever reached by tests. Statement coverage does not measure the percentage of unique execution paths exercised.Limitations of statement coverage:It does not take into account all the possible data inputs. Consider this code: int a = b / c;This could be covered with b = 18 and c = 6, but never tested where c = 0.Some tools do not provide fractional coverage. For instance, in the following code, when condition a is true, the code already has 100% coverage. Condition b is not evaluated.if (a || b) {\u00a0\u00a0// do something}Coverage analysis can only tell you how the code that exists has been exercised. It cannot tell you how code that ought to exist would have been executed. Consider the following:error_code = FunctionCall();// returns kFatalError, kRecoverableError, or kSuccessif (error_code == kFatalError) {\u00a0\u00a0// handle fatal error, exit} else {\u00a0\u00a0// assume call succeeded}This code is only handling two out of the three possible return values (a bug!). It is missing code to do error recovery when kRecoverableError is returned. With tests that generate only the values kFatalError and kSuccess, you will see 100% coverage. The test case for kRecoverableError does not increase coverage, and appears \u201credundant\u201d for coverage purposes, but it exposes the bug!So the correct way to do coverage analysis is:Make your tests as comprehensive as you can, without coverage in mind. This means writing as many test case as are necessary, not just the minimum set of test cases to achieve maximum coverage.Check coverage results from your tests. Find code that's missed in your testing. Also look for unexpected coverage patterns, which usually indicate bugs.Add additional test cases to address the missed cases you found in step 2.Repeat step 2-3 until it's no longer cost effective. If it is too difficult to test some of the corner cases, you may want to consider refactoring to improve testability.Reference for this episode: How to Misuse Code Coverage by Brian Marick from Testing Foundations.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Patrick Copeland, Engineering Productivity DirectorArticle covering some of Google's Test culture. In the article, there's some focus on the ideas of \"incremental testing\" and how practices are changing in the software as a service world. Even with all of our drive to find better approaches, one thing to note is that we still believe in the fundamentals of testing. Here's a direct link (they use a bunch of indirection to get to the PDF. So if you can't get it directly, here's the top level link).", "In the movie Amadeus, the Austrian Emperor criticizes Mozart's music as having \u201ctoo many notes.\u201d How many tests are \u201ctoo many\u201d to test one function?Consider the method decide:public void decide(int a, int b, int c, int d, \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0int e, int f) {\u00a0\u00a0if (a > b || c > d || e > f) {\u00a0\u00a0\u00a0\u00a0DoOneThing();\u00a0\u00a0} else {\u00a0\u00a0\u00a0\u00a0DoAnother();\u00a0\u00a0} // One-letter variable names are used here only\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0because of limited space.} // You should use better names. Do as I say, not\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0as I do. :-)How many tests could we write? Exercising the full range of int values for each of the variables would require 2192 tests. We'd have googols of tests if we did this all the time! Too many tests. What is the fewest number of tests we could write, and still get every line executed? This would achieve 100% line coverage, which is the criterion most code-coverage tools measure. Two tests. One where (a > b || c > d || e > f) is true; one where it is false. Not enough tests to detect most bugs or unintentional changes in the code.How many tests to test the logical expression and its sub-expressions? If you write a test of decide where a == b, you might find that the sub-expression a > b was incorrect and the code should have been a >= b. And it might make sense to also run tests where a < b and  a > b. So that's three tests for a compared to b. For all of the parameters, that would 3 * 3 * 3 = 27 tests. That's probably too many.How many tests to test the logical expression and its sub-expressions independently? Consider another version of decide, where the logical sub-expressions have been extracted out:public void decide(int a, int b, int c, int d,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0int e, int f) {\u00a0\u00a0if (tallerThan(a, b)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|| harderThan(c, d)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|| heavierThan(e, f)) {\u00a0\u00a0\u00a0\u00a0DoOneThing();\u00a0\u00a0} else {\u00a0\u00a0\u00a0\u00a0DoAnother();\u00a0\u00a0}}boolean tallerThan(int a, int b) { return a > b; }\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// Note \u201cpackage scope\u201dboolean harderThan(int c, int d) { return c > d; }\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// rather than public; JUnitboolean heavierThan(int e, int f) { return e > f; }\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// tests can access these.We can write four tests for decide.  One where tallerThan is true. One where harderThan is true. One where heavierThan is true. And one where they are all false. We could test each of the extracted functions with two tests, so the total would be 4 + 2 * 3 = 10 tests. This would be  just enough tests so that most unintentional changes will trigger a test failure. Exposing the internals this way trades decreased encapsulation for increased testability. Limit the exposure by controlling scope appropriately, as we did in the Java code above.How many tests is too many? The answer is \u201cIt depends.\u201d It depends on how much confidence the tests can provide in the face of changes made by others. Tests can detect whether a programmer changed some code in error, and can serve as examples and documentation. Don't write redundant tests, and don't write too few tests.Remember to download this episode of Testing on the Toilet and post it in your office.", "How quickly can you...  ...read all 25 words out loud: RED, GREEN, BLUE, ...  (Try it now!) ...say all 25 colors out loud: GREEN, YELLOW, WHITE... (Try it now!) Did the second task require more time and effort? If so, you're experiencing the Stroop Effect, which roughly says that when a label (in this case, the word) is in the same domain as its content (the color) with a conflicting meaning, the label interferes with your ability to comprehend the content.What does this have to do with testing? Consider the following code:public void testProtanopiaColorMatcherIsDistinguishable() {\u00a0\u00a0ColorMatcher colorMatcher = new ColorMatcher(PROTANOPIA);\u00a0\u00a0assertFalse(\u201cBLUE and VIOLET are indistinguishable\u201d,\u00a0\u00a0\u00a0\u00a0colorMatcher.isDistinguishable(Color.BLUE, Color.VIOLET));}When this test fails, it produces a message like this:Failure: testProtanopiaColorMatcherIsDistinguishable:Message: BLUE and VIOLET are indistinguishableQuick: what caused this error? Were BLUE and VIOLET indistinguishable, or not? If you're hesitating, that's the Stroop Effect at work! The label (the message) expresses a truth condition, but the content (in assertFalse) expresses a false condition. Is the ColorMatcher doing the wrong thing, or is the test condition bogus? This message is wasting your valuable time! Now consider this slight alteration to the test name and test message:Failure: testProtanopiaColorMatcherCannotDistinguishBetweenCertainPairsOfColorsMessage: BLUE and VIOLET should be indistinguishableDo you find this clearer? Protanopia (reduced sensitivity to the red spectrum) causes certain pairs of colors to be indistinguishable. BLUE and VIOLET should have been indistinguishable, but weren't.Here are some things to keep in mind when writing your tests: When someone breaks your test \u2013 will your test name and message be useful to them?Opinionated test names like testMethodDoesSomething can be more helpful than testMethod.Great test messages not only identify the actual behavior,but also the expected behavior.Should is a handy word to use in messages \u2013 it clarifies what expected behavior didn't actually happen.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Christopher Semturs, Software Engineer in Test, Zurich      For those German-speaking folks among our readers of this English Google Testing Blog we have exciting news: We have just launched the German Testing Blog!This is a tribute to the fact that the German-speaking software test community is one of the biggest non-english audiences of this blog. The german blog will contain a mix of German versions of postings from this blog as well as unique postings about regional-specific issues from Europe. Some of our biggest engineering offices outside of US are in Europe, e.g. in Switzerland and the UK.So German speakers around the world check out this new resource for interesting discussions about software test, performance test, load test, and how to create rock-solid software.", "Posted by Adam Porter, Professor Department of Computer Science, University of Maryland, and Associate Director, University of Maryland Institute for Advanced Computing Studies\n\n[From time to time, we invite software testing experts to write about their ideas and research. - Ed.]\n\n Motivation: \nSoftware systems are getting larger, more complex and more configurable all the time. While beneficial in many ways, these changes dramatically increase testing obligations.\nFor example, today's systems are increasingly built to run on multiple OS, compiler and library platforms. They are composed from multiple components, each with multiple versions. They are configured by manipulating numerous compile- and run-time options. Additionally, distributed applications add an allocation dimension in which runtime topologies can vary widely.  \nThis situation is further complicated by agile and flexible  development practices in which systems are evolved incrementally over short, but varying update cycles and in which development  teams may be geographically distributed.  \nBasically, each new configuration dimension increases the number of potential runtime configurations combinatorially. Since each of these configurations might behave differently or host different bugs, each of these configurations, at least in theory, must be tested.   Furthermore, this increased amount of testing must be done in  shorter and shorter time frames because the systems themselves  are changing faster than ever. \n\n The Skoll project: \nOur research aims to create better, faster and more powerful techniques for testing these kinds of systems. Our vision is to redesign traditional testing processes so that they can be executed around-the-world, around-the-clock.  These processes are logically divided into multiple tasks that are distributed intelligently to client machines around the world and then executed by them. The results from these distributed tasks are returned to central collection sites where they are merged and analyzed to complete the overall QA process. \nAt the second Google Test Automation Conference, I and my colleague Atif Memon presented an infrastructure and approach called  Skoll  that was created to support this vision. The video is embedded below.\n\n\n\n\n Skoll in action: \nTo give you a better sense of how this works we're going to walk you through how we set up a Skoll server to run a simplified version of the continuous build, test and integration process we've developed to run the  MySQL Build Farm Initiative.  This process runs on computing resources that are temporarily volunteered by members of the MySQL developer and user communities.  After reading the rest of this post, we hope you too will be ready and willing to volunteer as well! \nMySQL, as you probably know, is a very popular open source database comprising over 2 million lines of code. It is developed by a world wide community and runs on many different platforms, has over 150 configuration options, and allows substantial static and runtime end-user customization. For instance, you can select different database front- and back-ends, and you can run the system with different runtime topologies to support database replication. In short, it's exactly the kind of system we had in mind when we created Skoll.\nWe first started speaking with MySQL developers after the first Google Test Automation Conference. They were interested in running a continuous build, integration and test (CBIT) process. They had several goals, such as: testing a broader variety of configurations, not just a handful of popular ones; giving developers greater visibility into the quality and stability of the system; improving bug fix turnaround time; and managing test data to enable long term statistical analysis and improved decision making. \nSince they couldn't seem to find anything off-the-shelf that was sufficiently targeted to their needs, we worked with them to develop a Skoll-based CBIT process. This process has several parts: defining a configuration model, implementing a sampling strategy, executing the tests, and analyzing and visualizing the results.  \nWe will discuss each of these below. Readers who just want to run a client can jump straight to the Section marked Test execution.  \n\n Some more details:\nConfiguration Model:  We are starting out by looking at 23 options. There are some inter-option constraints as well.  For example, a configuration can compile in support for either the libedit library (--with-libedit) or the readline library (--with-readline), but not both. Here's a ling to the    current configuration model .  We will expand this model as we gain more experience with this process and more insight into the key issues concerning MySQL. \nSampling strategy: There are over 48 million unique configurations in this test space. Since testing 1 configuration can take up to 2 hours and because new releases come out more or less daily, exhaustive testing of each check-in is clearly impossible. Therefore, we only test specially-chosen subsets of configurations in which all t-way (2 <= t <= 4) combinations of option settings are tested at least once. Our particular algorithm for selecting these configurations, which we invented with  Myra Cohen  of the University of Nebraska, works incrementally. First it tests 3 sets of ~23 configurations, each set of which covers all combinations of settings between every pair of options (i.e., 2-way combinations).  We then move up to testing 3 sets of ~84 configurations that cover all settings of every triple of options and then move up again to cover all quadruples (~272 configurations), also 3 times each. This allows us to test low level interactions quickly, even if new releases come in before all levels of interactions can be tested. As we gain experience with this process, we will evaluate whether and how much we may need to increase t.\nTest execution: To participate in this process, users can go to our  MySQL 5.1 project page. On that page you can find links to  download a client along with instructions on how to install and run it. This client is a simple Perl script that connects to a Skoll server and asks for a test job. The server examines its internal databases, selects an outstanding job and returns it to the client who then executes it. Currently, testing a configurations involves compiling MySQL in a specific configuration and then running about 750 MySQL-supplied tests on that configuration. The test scripts determine which test cases can be run in the current configuration and runs them. After completing the tests, the client uploads the results to the server. \nFeedback, analysis and visualization: For this process, we are interested in understanding where configuration-related bugs might be hiding. To figure this out, we periodically analyze test results by building a classification tree for each test that has failed in a minimum number of configurations. These trees model test failures in terms of the configuration option and settings that were set when the tests failed. Users can look at a web page to see the    current test results .  This page shows summary statistics for each build ID. Clicking on a build ID takes you to a detail page listing each test that failed a minimum number of times. Each test presents classification information, both in a raw form and as an interactive   treemap. \n\n Current status:\nWe have just started running this process on a continuous basis using an  a small number of MySQL developer community machines. We hope to bring  many more test machines online in the coming weeks. Please check out the results web page to watch our progress. And don't let everyone else have all the fun --   download your client today! Thanks!", "Check out a short article by our very own Julian Harty on stickyminds.com. Here's the summary...It took eighteen months for Julian Harty to overcome the various challenges of testing mobile wireless applications. In turn, he has learned some valuable lessons that he wants to share with you in this week's column.Here's the post: http://www.stickyminds.com/r.asp?F=W13215", "(resuming our testing on the toilet posts...)In a previous episode, we extracted methods to simplify testing in Python. But if these extracted methods make the most sense as private class members, how can you write your production code so it doesn't depend on your test code? In Python this is easy; but in C++, testing private members requires more  friend contortions than a game of Twister\u00ae.    // my_package/dashboard.hclass Dashboard { private:  scoped_ptr<Database> database_;  // instantiated in constructor  // Declaration of functions GetResults(), GetResultsFromCache(),  // GetResultsFromDatabase(), CountPassFail()  friend class DashboardTest; // one friend declaration per test                              // fixture};   You can apply the Extract Class and Extract Interface refactorings to create a new helper class containing the implementation.  Forward declare the new interface in the .h of the original class, and have the original class hold a pointer to the interface. (This is similar to the Pimpl idiom.) You can distinguish between the public API and the implementation details by separating the headers into different subdirectories (/my_package/public/ and /my_package/ in this example):   // my_package/public/dashboard.hclass ResultsLog;  // extracted helper interfaceclass Dashboard { public:  explicit Dashboard(ResultsLog* results) : results_(results) { } private:  scoped_ptr<ResultsLog> results_;};// my_package/results_log.hclass ResultsLog { public:  // Declaration of functions GetResults(),  // GetResultsFromCache(),  // GetResultsFromDatabase(), CountPassFail()};// my_package/live_results_log.hclass LiveResultsLog : public ResultsLog { public:  explicit LiveResultsLog(Database* database)      : database_(database) { }};   Now you can test LiveResultsLog without resorting to friend declarations.  This also enables you to inject MockResultsLog instance when testing the Dashboard class. The functionality is still private to the original class, and the use of a helper class results in smaller classes with better-defined responsibilities. Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Markus Clermont, Test Engineering Manager, ZurichIn the last couple of years the practice of testing has undergone more than superficial changes. We have turned our art into engineering, introduced process-models, come up with best-practices, and developed tools to support our daily work and make each test engineer more productive. Some tools target test execution. They aim to automate the repetitive steps that a tester would take to exercise functions through the user interface of a system in order to verify its functionality. I am sure you have all seen tools like Selenium, WebDriver, Eggplant or other proprietary solutions, and that you learned to love them.On the downside, we observe problems when we employ these tools:Scripting your manual tests this way takes far longer than just executing them manually.The UI is one of the least stable interfaces of any system, so we can start automating quite late in the development phase.Maintenance of the tests takes a significant amount of time.Execution is slow, and sometimes cumbersome.Tests become flaky.Tests break for the wrong reasons.Of course, we can argue that none of these problems is particularly bad, and the advantages of automation still outweigh the cost. This might well be true. We learned to accept some of these problems as 'the price of automation', whereas others are met by some common-sense workarounds:It takes long to automate a test\u2014Well, let's automate only tests that are important, and will be executed again and again in regression testing.Execution might be slow, but it is still faster than manual testing.Tests cannot break for the wrong reason\u2014When they break we found a bug.In the rest of this post I'd like to summarize some experiences I had when I tried to overcome these problems, not by working around them, but by eliminating their causes.Most of these problems are rooted in the fact that we are just automating manual tests. By doing so we are not taking into account whether the added computational power, access to different interfaces, and faster execution speed should make us change the way we test systems.Considering the fact that a system exposes different interfaces to the environment\u2014e.g., the user-interface, an interface between front-end and back-end, an interface to a data-store, and interfaces to other systems\u2014it is obvious that we need to look at each and every interface and test it. More than that we should not only take each interface into account but also avoid testing the functionality in too many different places.Let me introduce the example of a store-administration system which allows you to add items to the store, see the current inventory, and remove items. One straightforward manual test case for adding an item would be to go to the 'Add' dialogue, enter a new item with quantity 1, and then go to the 'Display' dialogue to check that it is there. To automate this test case you would instrument exactly all the steps through the user-interface.Probably most of the problems I listed above will apply. One way to avoid them in the first place would have been to figure out how this system looks inside.Is there a database? If so, the verification should probably not be performed against the UI but against the database.Do we need to interface with a supplier? If so, how should this interaction look?Is the same functionality available via an API? If so, it should be tested through the API, and the UI should just be checked to interact with the API correctly.This will probably yield a higher number of tests, some of them being much 'smaller' in their resource requirements and executing far faster than the full end-to-end tests. Applying these simple questions will allow us to:write many more tests through the API, e.g., to cover many boundary conditions,execute multiple threads of tests on the same machine, giving us a chance to spot race-conditions,start earlier with testing the system, as we can test each interface when it becomes 'quasi-stable',makes maintenance of tests and debugging easier, as the tests break closer to the source of the problem,require fewer machine resources, and still execute in reasonable time.I am not advocating the total absence of UI tests here. The user interface is just another interface, and so it deserves attention too. However I do think that we are currently focusing most of our testing-efforts on the UI. The common attitude, that the UI deserves most attention because it is what the user sees, is flawed. Even a perfect UI will not satisfy a user if the underlying functionality is corrupt.Neither should we abandon our end-to-end tests. They are valuable and no system can be considered tested without them. Again, the question we need to ask ourselves is the ratio between full end-to-end tests and smaller integration tests.Unfortunately, there is no free lunch. In order to change the style of test-automation we will also need to change our approach to testing. Successful test-automation needs to:start early in the development cycle,take the internal structure of the system into account,have a feedback loop to developers to influence the system-design.Some of these points require quite a change in the way we approach testing. They are only achievable if we work as a single team with our developers. It is crucial that there is an absolute free flow of information between the different roles in this team.In previous projects we were able to achieve this byremoving any spatial separation between the test engineers and the development engineers. Sitting on the next desk is probably the best way to promote information exchange,using the same tools and methods as the developers,getting involved into daily stand-ups and design-discussions.This helps not only in getting involved really early (there are projects where test development starts at the same time as development), but it is also a great way to give continuous feedback. Some of the items in the list call for very development-oriented test engineers, as it is easier for them to be recognized as a peer by the development teams.To summarize, I figured out that a successful automation project needs:to take the internal details and exposed interface of the system under test into account,to have many fast tests for each interface (including the UI),to verify the functionality at the lowest possible level,to have a set of end-to-end tests,to start at the same time as development,to overcome traditional boundaries between development and testing (spatial, organizational and process boundaries), andto use the same tools as the development team.", "Posted by Marc Kaplan, Test Engineering LeadAt Google, we have infrastructure that is shared between many projects. This infrastructure creates a situation where we have a many dependencies in terms of build requirements, but also in terms of test requirements. We've found that we actually need two approaches to deal with these requirements depending on whether we are looking to run larger system tests or smaller unittests, both of which ultimately need to be executed to improve quality.For unittests, we are typically interested in only the module or function that is under test at the time, and we don't care as much about downstream dependencies, except insofar as they relate to the module under test. So we will typically write test mocks to mock out the downstream components that we aren't interested in actually running that simulate their behaviors and failure modes. Of course, this can only be done after understanding how the downstream module works and interfaces with our module.As an example of mocking out a downstream component in Bigtable, we want to simulate the failure of Chubby , our external lockservice, so we we write a Chubby test mock that simulates the various ways that Chubby can interact with Bigtable. We then use this for the Bigtable unittests so that they a) run faster, b) reduce external dependencies and c) enable us to simulate various failure and retry conditions in the Bigtable Chubby related code.There are also cases where we want to simulate components that are actually upstream to the component under test. In these cases we write what is called a test driver. This is very similar to a mock, except that instead of being called by our module (downstream) it calls our module (upstream). For example, if Bigtable component has some Mapreduce specific handling, we might want to write a test driver to simulate these Mapreduce-specific interfaces so we don't have to run the full Mapreduce framework inside our unittest framework. The benefits are all the same as those of using test mocks. In fact, in many cases it may be desirable to use both drivers and mocks, or perhaps multiple of each.In system tests where we're more interested in the true system behaviors and timings, or in other cases where we can't write a driver or mocks we might turn to fault injection. Typically, this involves either completely failing certain components sporadically in system tests, or injecting particular faults via a fault injection layer that we write. Looking back to Bigtable again, since Bigtable uses GFS when we run system tests, we are running fault injection for GFS by failing actual masters and chunkservers sporadically, and seeing how Bigtable reacts under load to verify that when we deploy new versions of Bigtable that they it will work given the frequent rate of hardware failures. Another approach that we're currently work on is actually simulating the GFS behavior via a fault injection library so we can reduce the need to use private GFS cells which will result in better use of resources.Overall, the use of Test Drivers, Test Mocks, and Fault Injection allows developers and test engineers at Google to test components more accurately, quickly, and above all helps improve quality.", "Posted by Patrick Copeland, Test Engineering DirectorWanted to let you know about a partnership Google Test Engineering is doing with the University of California, Irvine. We've teamed up with Professor Hadar Ziv to sponsor a course that focuses on preparing students for industry (code.google.com and several other companies are also participating). Naturally, our project focuses on testing. George Pirocanac is heading up this work and recently went down to Irvine to talk about how they will test our mash-up editor. Here's the basic project outline if you are curious.Class Project Plan: Testing Google's Mash-up EditorOverall Class Goal: To understand the basic software functional testing concepts through the experience of a case study of testing the Google Mash-up Editor and to provide meaningful feedback to Google about the effectiveness and usability of the tool.Phase I - Gaining Domain expertise and Exploratory Testing (four months)Goals: Be able to explain what a mash-up is and why it is becoming important in today's internet. Be able to code a simple mash-up using a javascript api. Be able to code that same mash-up using Google Mash-up Editor tags. Be able to outline the basic features of the Google Mash-up editor. Be able to identify the essential elements of a functional test plan. Create a functional test plan outline for the Google Mash-up editor.Phase II - Test Plan Execution over time (Keeping in step with development) (three months)Goals: Be able to identify the major challenges in executing a test plan during the life of a software project. Be able to identify testing technologies for dealing with these challenges. Be able to identify the effectiveness of a testing approach. Execute the test plan and provide feedback to Google.Phase III - Usability & Competing Technologies Survey (two months)Goals: Be able to identify the essential elements of a usability study. Apply the topic of usability to programming. Compare and contrast the GME with three other industry mash-up editors.", "Posted by Goranka Bjedov, Senior Test EngineerThis post is my best shot at explaining what I do, why I do it, and why I think it is the right thing to do. Performance testing is a category of testing that seems to evoke strong feelings in people: feelings of fear (Oh, my God, I have no idea what to do because performance testing is so hard!), feelings of inadequacy (We bought this tool that does every aspect of performance testing, we paid so much for it, and we are not getting anything done!), feelings of confusion (So, what the heck am I supposed to be doing again?), and I don't think this is necessary.Think of performance testing as another tool in your testing arsenal - something you will do when you need to. It explores several system qualities, that can be simplified to: Speed - does the system respond quickly enough Capacity - is the infrastructure sized adequately Scalability - can the system grow to handle future volumes Stability - does the system behave correctly under load So, I do performance testing of a service when risk analysis indicates that failing in any of the above categories would be more costly to the company than performing the tests. (Which, if your name is Google and you care about your brand, happens with any service you launch.) Note that I am talking about services - I work almost exclusively with servers and spend no time worrying about client-side rendering/processing issues. While those are becoming increasingly more important, and have always been more complex than my work, I consider those to be a part of functionality tests, and they are designed, created and executed by functional testing teams.Another interesting thing about performance testing is that you will never be able to be 100% \"right\" or 100% \"done. Accept it, deal with it, and move on. Any system in existence today will depend on thousands of different parameters, and if I spent the time analyzing each one of them, understanding the relationships between each two or each three, graphing their impact curves, trying to non-dimensionalize them, I would still be testing my first service two years later. The thought of doing anything less filled me with horror (They cannot seriously expect me to provide meaningful performance results in less than a year, can they?) but I have since learned that I can provide at least 90% of meaningful information to my customers by applying only 10% of my total effort and time. And, 90% is more than enough for vast majority of problems.So, here is what I really do - I create benchmarks. If I am lucky and have fantastic information about current usage patterns of a particular product (which I usually do), I will make sure this benchmark covers most operations that are top resource hogs (either per single use or cumulative). I'll run this benchmark with different loads (number of virtual users) against a loosely controlled system (it would be nice to have 100 machines all to myself for every service we have, which I can use once a day or once a week, but that would be expensive and unrealistic) and investigate its behavior. Which transactions are taking the most time? Which transactions seem to get progressively worse with increasing load? Which transactions seem unstable (I cannot explain their behavior)? I call this exploratory performance testing, and I'll repeat my tests until I am convinced I am observing real system behavior. While I am doing this, I make sure I am not getting biased by investigating the code. If I have questions, I ask programmers, but I know they are biased, and I will avoid getting biased myself!Once I have my graphs (think, interesting transaction latencies and throughput vs. load here) I meet with the development team and discuss the findings. Usually, there is one or two things they know and have been working on, and a few more they were unaware of. Sometimes, they look over my benchmark and suggest changes (could you make the ratio 80:20, and not 50:50?) After this meeting, we create our final benchmark, I modify the performance testing scripts, and now this benchmark will run as often as possible, but hopefully at least once a night. And, here is the biggest value of this effort: if there is a code change that has impacted performance in an unacceptable way, you will find out about it the next day. Not a week or a month later (How many of us remember what we did in the last month? So, why expect our developers to do so?)Here is why I think this is the right thing to do: I have seen more bad code developed as a result of premature performance optimizations - before the team even thought they had a problem! Please don't do that. Develop your service in a clean, maintainable and extensible manner. Let me test it, and keep regression testing it. If we find we have a problem in a particular area, we can then address that problem easily - because our code is not obfuscated with performance optimization that have improved code paths that execute once a month by 5%.I can usually do this in two - four weeks depending on the complexity of the project. Occasionally, we will find an issue that cannot be explained or understood with performance tests. At that point in time, we look under the hood. This is where performance profiling and performance modeling come in. And, both of those are considerably more complex than performance testing. Both great tools, but should be used only when the easy tool fails.Tools, tools, tools... So, what do we use? I gave a presentation at Google Test Automation Conference in London on exactly this topic. I use open source tools. I discuss the reasons why in the presentation. In general, even if you have decided to go one of the other two routes (vendor tools or develop your own) check out what is available. You may find out that you will get a lot of information about your service using JMeter and spending some time playing around with it. Sure, you can also spend $500K and get similar information or you can spend two years developing \"the next best performance testing tool ever,\" but before you are certain free is not good enough, why would you want to?Final word: monitor your services during performance tests. If you do not have service related monitoring developed and set up to be used during live operations, you do not need performance testing. If the risks of your service failing are not important enough that you would want to know about it *before* it happens, then you should not be wasting time or money on performance testing. I am incredibly lucky in this area - Google infrastructure is developed by a bunch of people who, if they had a meeting where the topic would be \"How to make Goranka's life easy?\", could not have done better. I love them - they make my job trivial. At a minimum, I monitor CPU, memory and I/O usage. I cannot see a case when you would want to do less, but you may want to do a lot more on occasion.", "Posted by Michael Bachman, Test Engineering ManagerA testing organization's job is not done with the release of a product.  As the software development cycle does not end with the release of the product but has an extension into the post-release diagnostics and evaluation. Learning from post-release metrics like product performance, defects, and behavior after it is in production (or in the field) provides valuable input into how to adjust future testing and development techniques. Measuring product defect trends and performance, and analyzing those results, can identify holes in test coverage, prevent bugs, plug gaps in the release cycle or product life cycle, and determine if the pre-release test environment was adequately representative of key customer scenarios.Here are a few metrics that can help jump-start this effortPre- versus post-production defect ratio: This metric measures the ratio of total number of defects found before production divided by the overall number of defects in the product (including the post-release issues). This lets a team measure how many defects are being caught before release. This effort supplements the age old valuable practice of partnering with Product Support to measure incidents/defect rate. The goal is not and should not be to indulge in the blame game of \"A defect found after release is test's \"fault,\" \" but as a means to find ways to make the product release cycle better. The thing to focus on is to identify what were the causes for the issue -   gaps in the release cycle, communication mishaps between product, development, and testing, inadequate test environments, or the overall testability of a product. There may not be a perfect metric, but obvious ones might be: time to resolutions (how long it takes to react to a broken issue), cost to the customer, or cost to customer support. The main point is to agree on a cross-organizational metric, track it, do the root cause analysis, and make the time to change.Breakdown of defects by component or functional area: In conjunction with monitoring which defects are found in production, categorizing them by functional area and component provides the necessary information to highlight trends and, more specifically, problematic areas. When a problematic component is identified, the test team can fill holes in test coverage, unit test coverage, product usability issues or the life cycle managing that functional area. Also, trending defects by component over time has additional advantages like - this data provides a better sense of the quality of particular components as they age, also provides the information of how effective the changes(if any) that were introduced in the engineering practices resulting in better quality and finally measure of introduction of new functionality caused any de-stabilizing effects to the system. This metric will allow product teams in making informed decisions regarding the product. Potential outcomes are resources allocation changes, feature deprecation or redesign of the features.Performance measurements (CPU usage, memory consumption, disk load, database load, latency, etc.): Without going into the various load and performance (L&P) measurements one can monitor within a product (since that can be a whole separate article in itself :) ) the product teams should ensure they have mechanisms to measure key and product relevant metrics can be collected. In order to gauge the effectiveness of the test environments these measurements need to obtained both in the test labs and production environments. Identifying these mismatches allows test organizations to correct any topology issues early and before any subsequent releases (or similar releases).Furthermore analysis of these could expose multiple causes of why the product behavior was different in production than in test labs. Some examples (like we have found at Google) of these issues are : different machine hardware, load mismatch on the system, localized tests not measuring round-trip latency, the number of concurrent users hitting the product simultaneously (your testing team compared to the potential millions of users in production). It is important to remember that, the most important point here to measure and monitor the performance of the product as well as determine the adequacy of the test environment. When a performance issue is found in production, ask yourself \"could we have caught that in the test environment?\"Are users or your monitoring systems finding the defects? Having reliable monitoring and debugging systems, logs, and notifications are key in reacting immediately to large defects in production, as well as potentially finding the defect before your users do. Some of the best practices of engineering teams (and those followed at Google) are real-time notifications of exceptions, load, and performance, pager alerts when systems are unavailable, as well as robust logs to help developers and testing debug the system state before and after a crash. There are many open source and commercial solutions to these pieces rather than building in-house solutions. The bulk of the effort in setting up reliable monitoring typically lies in development, but it is key for test teams to assist in identifying the need and also ensure they are utilized in their test environments. This not only allows the testing organization to test the functionality of the monitoring tools, but also alerts testing of defects that might have slipped through and are not directly visible on a front-end.So, what does this get you? A solid picture of the product's quality and performance trend over time.  Measuring lets testing tweak their coverage and environments, as well as analyze how the team works together. Reacting to the findings helps open communication channels between development, production, and testing, and lets them join together to debug and reproduce defects and eventually reduce defects. Every part of the larger team can watch defect trends; help prioritize resources and features, and better increase unit test, system test, and performance test coverage. Getting to a robust, real-time defect, performance, monitoring framework takes effort from all teams, but, in the end, everyone can reap the benefit, especially, and most important, your users.", "Posted by John Thomas, Software Engineer  We thought you might be interested in another article from our internal monthly testing newsletter called CODE GREEN... Originally titled: \"Opinion: But it works on my machine!\"We spent so much time hearing about \"make your tests small and      run fast.\" While this is important for quick CL verification,      system level testing is important, too, and doesn't get enough      air time.   You write cool features. You write lots and lots of unit      tests to make sure your features work. You make sure the unit      tests run as part of your project's continuous build. Yet when      the QA engineer tries out a few user scenarios, she finds many      defects. She logs them as bugs. You try to reproduce them, but      ... you can't!     Sound familiar? It might to a lot of you who deal with complex      systems that touch many other dependent systems. Want to test a      simple service that just talks to a database? Simple, write a      few unit tests with a mocked-out database. Want to test a      service that connects to authentication to manage user accounts, talks to      a risk engine, a biller, and a database? Now that's a different      story!    So what are system level tests      again?     System level tests to the rescue. They're also referred to as      integration tests, scenario tests, and end-end tests. No matter      what they're called, these tests are a vital part of any test      strategy. They wait for screen responses, they punch in HTML      form fields, they click on buttons and links, they verify text      on the UI (sometimes in different languages and locales). Heck,      sometimes they even poke open inboxes and verify email content!  But I have a gazillion unit      tests and I don't need system level tests!     Sure you do. Unit tests are useful in helping you quickly      decide whether your latest code changes haven't caused your      existing code to regress. They are an invaluable part of the      agile developers' tool kit. But when code is finally packaged      and deployed, it could look and behave very differently. And no      amount of unit tests can help you decide whether that awesome UI      feature you designed works the way it was intended, or that one      of the services your feature depended on is broken or not      behaving as expected. If you think of a \"testing diet,\" system      level tests are like carbohydrates -- they are a crucial part of      your diet, but only in the right amount!     System level tests provide that sense of comfort that      everything works the way it should, when it lands in the      customer's hands. In short, they're the closest thing to      simulating your customers. And that makes them pretty darn      valuable.  Wait a minute -- how stable are      these tests?     Very good question. It should be pretty obvious that if you      test a full blown deployment of any large, complex system you're      going to run into some stability issues. Especially since      large, complex systems consist of components that talk to many      other components, sometimes asynchronously. And real world      systems aren't perfect. Sometimes the database doesn't respond      at all, sometimes the web server responds a few seconds later,      and sometimes a simple confirmation message takes forever to      reach an email inbox!     Automated system level tests are sensitive to such issues, and      sometimes report false failures. The key is utilizing them      effectively, quickly identifying and fixing false failures, and      pairing them up with the right set of small, fast tests.", "Posted by Patrick Copeland, Test Engineering DirectorWe thought you might be interested in some articles from our internal monthly testing newsletter called CODE GREEN...Many projects at Google have started using or are considering using Selenium. We recently interviewed Noogler (new people to Google) Jason Huggins, who is the creator and developer of Selenium, to learn about how it all started and where it's going.   [Side note: Even before his first day at the Googleplex, Jason showed an amazing dedication to Google. After leaving Chicago with the big moving truck, he and his family had to stop after just a few hours because of an ice storm. Cars sliding off the road left and right. Further along, in Kansas, one of his kids caught pneumonia. His family stayed in the local hospital while Jason drove on. Heading west, there was a big snow storm in Colorado, which he wanted to avoid. He drove further south and ended up in a rare (but really real) dust storm over the Border States. He promised us some great video footage of his drive through tumbleweeds. He finally arrived and has settled in and is looking forward to calmer times in the Bay Area. After that trip, he isn't even worried about earthquakes, fires, mud slides, or traffic on the 101.]Couple of GTAC Videos with Jason: SeleniumRC, Selenium vs WebDriver CG: Why did you invent Selenium? What was the motivation? Huggins: Selenium was extracted from a web-based (Python + Plone!) time-and-expense (T&E) application my team and I were building for my previous employer. One of the mandates for the new T&E app was that it needed to be \"fast, fast, fast.\" The legacy application was a client-server Lotus Notes application and wasn't scalable to the current number of offices and employees at the time. To live up to the \"fast, fast, fast\" design goal, we tried to improve and speed up the user experience as much as possible. For example, expense reports can get pretty long for people who travel a lot. No matter how many default rows we put in a blank expense form, people often needed to add more rows of expense items to their reports. So we added an \"Add row\" button to the expense report form. To make this \"fast, fast, fast,\" I decided to use a button that triggered JavaScript to dynamically add one blank row to the form. At the time (Spring 2004), however, JavaScript was considered buggy and evil by most web developers, so I caught a lot of flak for not going with the classic approach of POSTing the form and triggering a complete page refresh with the current form data, plus one blank row. Going down the road of using JavaScript had consequences. For one, we had a really, really difficult time testing that little \"Add row\" button. And sadly, it broke often. One week \"Add row\" would be working in Mozilla (Firefox was pre-1.0), but broken in Internet Explorer. And of course, nothing was working in Safari since few developers were allowed to have Macs. ;-) The following week, we'd open a bug saying \"'Add row' is broken in IE!!\" The developer assigned to the issue would fix and test it in IE, but not test for regressions in Mozilla. So, \"Add row\" would now be broken in Mozilla, and I'd have to open a ticket saying \"'Add row' is now broken in Mozilla!!!\". Unlike most other corporate IT shops, we didn't have the luxury of telling all employees to use a single browser, and developers didn't want to manually test every feature in all supported browsers every time. Also, we had a very tiny budget and commercial testing tools were -- and still are -- ridiculously over-priced on a per-seat basis. The T&E project was done the \"Agile Way\" -- every developer does testing -- so shelling out thousands of dollars per developer for a testing tool wasn't going to happen. Never mind the fact that there were no commercial tools that did what we needed anyway! After many months of trial and error and various code workarounds, I came to the conclusion I needed a testing tool that would let me functional-test JavaScript-enhanced web user interfaces (aka \"DHTML\" or now \"Ajax\"). More specifically, I needed to test the browser UIs: Internet Explorer, Mozilla Firefox, and Safari. There were no commercial apps at the time that could do this, and the only open source option was JsUnit, which was more focused on unit testing pure JavaScript functions, rather than being a higher-level black-box/smoke-test walk through a web app. So we needed to write our own tool. Our first attempt was a Mozilla extension called \"driftwood\" (never released), coded by Andrew McCormick, another co-worker of mine at the time. It did make testing  the \"Add row\" button possible, but since it was Mozilla-only, it wasn't what we needed for testing in all browsers. Paul Gross and I started over, and I started to educate myself on functional testing tools and techniques and stumbled upon Ward Cunningham's Framework for Integrated Testing (FIT). I originally set out to implement \"FIT for JavaScript,\" but quickly realized we were drifting away from the FIT API, so Selenium became its own thing. CG: Why does the world need another test tool?Huggins: At the time I created Selenium, had there been another testing tool that could test JavaScript UI features in all browsers on all platforms, believe me, I would have saved lots of time *not* writing my own tool. CG: What's special about it?Huggins: Well, maybe the right question is \"What's \u201clucky\u201d about it? Selenium was created in a time when JavaScript was considered \"bad\" and generally avoided by most professional web developers. Then Google Maps hit the scene a year later, the term \"Ajax\" was coined, and BOOM! Overnight, JavaScript became \"good.\" Also, Firefox started stealing market share from IE. The combination of needing to test 1) JavaScript features 2) in several browsers (not just IE) was a \"right place, right time\" moment for Selenium. CG: When did you realize that Selenium was a big deal? What was the tipping point? Huggins: When I started being asked to give presentations or write about Selenium by people I didn't know. The tipping point for Selenium technically relied on two things: 1) the Selenium IDE for Firefox, written by Shinya Kasatani, which made installation and the first-hour experience tons better for new users. And 2), Selenium Remote Control (RC) created by Paul Hammant, and extended by Dan Fabulich, Nelson Sproul, and Patrick Lightbody, which let developers write their tests in Java, C#, Perl, Python, or Ruby, and not have to write all their tests in the original FIT-like HTML tables. Socially, if Google Maps or Gmail never existed and thus the whole Ajax gold rush, I wonder if JavaScript would still be considered \"bad,\" with a similar \"why bother?\" attitude to testing it.  CG: Have you discovered any interesting teams using Selenium in ways you'd never intended? Huggins: At my previous company, I did see some developers write Selenium scripts to create their time and expense reports for them from YAML or XLS files. Since we hadn't exposed a back-end API, automating the browser for data entry was the next best thing. It was never designed for this purpose, but I started (ab)using it as coded bug reports. Asking users for steps on how to reproduce a bug naturally lends itself to looking like a Selenium test for that bug. Also, I've used the Selenium IDE Firefox plug-in to enter NBC's \"Deal or No Deal\" contest on their website from home, but I stopped doing that when I read in the fine print that the use of automation tools to enter their contest was grounds for disqualification.  CG: What advice do you have to offer Google groups interested in Selenium?Huggins: Well, one of the biggest drawbacks with user interface testing tools is that they're slow for various reasons. One way to bring the test run times down is to run them in parallel on a grid of servers, instead of sequentially. Of course, that isn't news to your average Googler. Engineers would be more likely to run automated browser UI tests if they could run 1000 tests in 1 minute total time on 1000 machines instead of 1000 tests in 1000 minutes on 1 machine. Sadly, though, most projects allocate only one machine, maybe two, to browser testing. I'm really excited to come to Google with the resources, the corporate interest, and the internal client base to make a large scale Selenium UI test farm possible. Eventually, I\u2019d like to take Selenium in some new directions that we\u2019ll talk about in later blog posts. But I'm getting ahead of myself. I have to survive Noogler training first.", "Posted by George Pirocanac, Test Engineering ManagerEarlier Blog entries described the strategy and methodology for testing the functionality of various kinds of applications. The basic approach is to isolate the logic of the application from the external API calls that the application makes through the use of various constructs called mocks, fakes, dummy routines, etc. Depending on how the application is designed and written, this can lead to smaller, simpler tests that cover more, execute more quickly, and lead to quicker diagnosis of problems than the larger end-to-end or system tests. On the other hand, they are not a complete replacement for end-to-end testing. By their very nature, the small tests don't test the assumptions and interactions between the application and the APIs that it calls. As a result, a diversified application testing strategy includes small, medium, and large tests. (See Copeland\u2019s GTAC Video, fast forward about 5 minutes in to hear a brief description of developer testing and small, medium, large)   What about testing the APIs themselves? What if anything is different? The first approach mirrors the small test approach. Each of the API calls is exercised with a variety of inputs and the outputs that are verified according to the specification. For isolated, stateless APIs (math library functions come to mind), this can be very effective by itself. However, many APIs are not isolated or stateless, and their results can vary according to the *combinations* of calls that were made.  One way to deal with this is to analyze the dependencies between the calls and create mini-applications to exercise and verify these combinations of calls. Often, this falls into the so-called typical usage patterns or user scenarios. While good, this first approach only gives us limited confidence. We also need to test what happens when not-so-typical sets of calls are made as well. Often application writers introduce usage patterns that the spec didn't anticipate.  Another approach is to capture the API calls made by real applications under controlled situations and then replay only the calls under the same controlled situations. These types of tests fall into the medium category. Again, the idea is to test series and combinations of calls, but the difficulty can lie in recreating the exact environment. In addition, this approach is vulnerable to building tests that traverse the same paths in the code. Adding fuzz to the parameters and call patterns can help, but not eliminate, this problem.  The third approach is to pull out the big hammer. Does it make sense to test the APIs with large applications? After all, if something goes wrong, you have to have specific knowledge about the application to triage the problem. You also have to be familiar with the techniques in testing the application. Testing a map-based application can be quite different from a calendar-based one, even if they share a common subset of APIs. The strongest case for testing APIs with large applications is compatibility testing. APIs not only have to return correct results, but they have to do it in the same manner from revision to revision. It's a sort of contract between the API writer and the application writer. When the API is private, then only a relative small number of parties have to agree on a change to the contract, but when it is public, even a small change can break a lot of applications.   So when it comes to API testing, it seems we are back to small, medium, and large approaches after all. Just as in application testing where you can't completely divorce the API from the application, we cannot completely divorce the application from API testing.", "Posted by Patrick Copeland, Test Engineering DirectorI visited the University  of Arizona last night, along with several Googlers, as part of a series of Tech Talks being given at selected schools. The talk was about GFS (Google File System). The auditorium was standing room only, with a turn out of over 150 computer science and computer engineering students (probably enticed with the free pizza and t-shirts :^). We really appreciated the turn out and the enthusiasm for Google. The questions following the talk were great and we probably could have gone on for several hours. I had a chance to talk to a few folks afterwards about their projects ranging from security research, to traffic simulation. I also met one of the professors and discussed the potential of doing some joint research. During the trip I also visited my grandmother who lives in Tucson. I was showing her a picture of my son on a BlackBerry. She was fascinated and asked me how it worked. I started to think about how to explain it and in that moment, it humbled me to think about the number of complex systems employed to do such a simple thing. Displaying a photo from a web page includes device side operating systems, run time languages, cell technology, network stacks, cell receivers, routers, serving front and back-ends,\u2026and more. An interesting side note: the bits for my jpg file ultimately get stored at Google in GFS, the topic of my talk that night. Obviously, each part of that chain is complex and important to get my simple scenario to work. I started to explain it in plain language and she quickly stopped me and said that when she was a child her family had a crank operated \u201cparty-line\u201d phone, where multiple families shared a single line. What hit me was that even though technology has gotten more complex, the types of scenarios we are enabling are still very basic and human: communicating, sharing, and connecting people. Even with all of the automated testing that we do, deep testing done from the perspective of customers is still absolutely critical. Again, thanks to the students at the University of Arizona. Bear Down! We\u2019re looking forward to visiting all of the schools on our list this season.", "Posted by Patrick Copeland, Test Engineering Director Here's some unsolicited feedback from Kurt Kluever who ended up on his school's web page (Rochester Institute of Technology). While he was at Google this summer he tested JotSpot. Kurt's project included enhancing a Selenium Framework for testing JotSpot. He add automated tests to an existing suite and refactored code to allow the framework to be more extensible. In addition, he fixed several issues with Selenium RC that allows for better concurrent execution of tests within a browser\u2019s object model. The scope of the project was focused on one application, but he looked through code from various other projects and applied the design patterns to the JotSpot framework.His experience in his own words\u2026link to RIT site pdf.", "Posted by Allen Hutchison, Engineering ManagerWe finished up the second annual Google Test Automation Conference on Saturday with a boat cruise around Manhattan. There's still a lot of work for us to do to wrap up all the loose ends, but one that we have gotten to right away is posting the videos on YouTube.We hope that you enjoy the videos, and we'd like you to join everyone already conversing on our Google Group. Several people have posted their reviews of the conference; you can find them in the comments on the GTAC Community Thread, or through a blog search for GTAC. Finally, our team posted several conference photos on Picasa Web Albums.", "Posted by Allen Hutchison, Engineering ManagerWe're moments away from the beginning of the Google Test Automation Conference. If you are attending in person, then I'll see you shortly. If you plan to watch the videos on YouTube, then keep an eye on this space. We'll be posting links to the talk videos throughout the next few days. The conference isn't just about the talks, though: this is a community conference. So use the comment space on this post to point people to your GTAC Live Blog, your pictures from the conference, and your thoughts about the conference.We also have a Google Group set aside for discussion about the conference and test automation. There are several people there from past conferences as well as this one.", "Posted by Patrick Copeland, Test Engineering DirectorWith intern season coming to a close, we wanted to share a testimonial from one of the 90 interns who joined us in Test Engineering this summer. This one is from a Software Engineer in Test who worked in the NYC office on the \"blog search\" project...Interning as a Software Engineering Tester for Google was certainly an experience I won't forget. It started three weeks before the end of my summer break, when I received an email from a recruiter in Google's New York City office requesting an interview for a Fall internship position. I  had not planned for this, but I figured I would give it a shot! Three weeks and a few interviews later, I was in New York City, apartment-shopping for my internship, which started the following week! Having never worked in test engineering before, I wasn't quite sure what to expect. All I knew was Google works fast! I hoped I could keep up.The first couple of weeks at Google required the most adjustment. While diving into my project's code and \"Noogler\" (new Googler) training courses, I came to the realization that this was going to be one of the most challenging experiences of my life. In the past, I've always associated challenging experiences with stress. But this was beginning to feel quite the opposite. Everyone around me was incredibly helpful, and the facilities were so comfortable that I actually found myself enjoying the challenge to the point of not wanting to leave work. The opportunity to learn at Google is literally endless, and I couldn't get enough.The great thing about test engineering is the ability to pick the challenges you want to attack. You get to find problems and attack code from angles that you never thought of before. The more you work and the more you test, the more you find out the RIGHT way of doing things, and just when you think you've got it perfect, you find a new way of optimizing your project's code. The product is as good as your testing proves it to be, and in a sense you become the gatekeeper for a product that is being used by millions worldwide. Having that responsibility is an amazing feeling. There were many projects available for me to work on at Google. From a test engineer's standpoint, there is always room for improvement in any product. I started my internship working on the back-end crawling and indexing components of Blog Search. After a few weeks of studying RSS technology and looking through Blog Search's parsing and indexing approach, the testing team and I were able to formulate a formal test plan that addressed potential security vulnerabilities and potential avenues for failure in many special case scenarios. Our code analysis and test cases will be constantly revisited and extended at every code change and will be the basis of many design decisions through the life of Blog Search. As my internship continued, I had the opportunity to work on several other projects, from small, new projects to larger, more established ones. I personally preferred the newer projects. I enjoy being involved from the beginning and being able to sit side-by-side with project managers and technical leads and give feedback about issues in the latest development. What's really fun is that every week, the entire NYC test engineering team gets together to sync up, share ideas, communicate issues, and share the latest news. This is where some of the great ideas get drawn up, and when something good hits the table, we pick it up and run with it! In my last few weeks at Google, we were able to draw a correlation between some common issues that we were all facing and identify the underlying challenges that needed to be addressed. Immediately, we started work on an automated testing framework that would solve most of our problems. When finished, it will be used not only by us, but possibly by every project team within Google.Overall, working at Google was a great experience. I had my share of fun, and I learned a great deal and am very grateful for the experience.", "Posted by Harry Robinson, Software Engineer in TestLydia Ash and I, both Seattle Googlers, recently gave presentations at the second annual Conference of the Association for Software Testing (CAST) held in Bellevue, Washington, on July 9-11.CAST is an informative and challenging conference run entirely by volunteers from the testing community. We felt palpable enthusiasm for software testing throughout the event. In fact, if you had stopped by Quardev the evening before the conference, you would have seen a dozen testers from around the country preparing attendee packets, burning CDs, and having a great time arguing about boundary values.Lydia's presentation (Data Set Analysis: Approaches to Testing when the Build is the Data) featured innovative heuristics used by the Google Maps team to detect subtle anomalies in large data sets.My talk on The Bionic Tester showed how agile test automation can extend an exploratory tester's reach into complex features like Google Talk's Multi-User Chat.On the second evening, we sponsored the first-ever Tester Exhibition in which several CAST presenters were asked to tell how they would use their expertise to test the CAST 2007 Registration page. The experts came at the problem from every direction and raised enough issues that the CAST organizers decided to disable the page prior to the Exhibition. :-) Slides and follow-up material from all CAST presentations are available through the CAST 2007 wiki.", "Posted by Zuri Kemp, Test Engineering Lead, Google New YorkGoogle New York is hosting a forum so that those of you in the local test engineering community can get to know one another while discussing topics significant to test engineers. This is a new series of events we're calling the Google New York Test Engineering Forum. So if you're a Test Engineer in the New York City tri-state area, please consider this your invitation to the kickoff forum on Wednesday, July 25th.For this first event, our panel will ask \"Is there a career path in test engineering?\" and \"What is the value of a Test Engineer?\". This is an informal session, and we welcome lots of audience participation.Register and learn more here.", "Posted by Harry Robinson, Software Engineer in TestThank you to everyone who applied to attend the Google Test Automation Conference in New York on August 23rd-24th. We received lots of applications for a very limited number of seats, so the choices were not easy.The sign-up website is now closed, and this week we'll be sending out acceptances and wait list notices. If you get an acceptance, congratulations -- we look forward to seeing you in New York. If you don't receive one, you'll be on the wait list, and we'll let you know if spaces become available. Either way, we'll let you know. If you don't hear from us in the next week, drop us a note.All the presentations from the conference will be posted on YouTube Google Channel within a New York minute after the speaker leaves the podium (well, maybe 72 New York minutes). We look forward to seeing you at the conference.", "When a method is long and complex, it is harder to test. You can make it easier by extracting methods: finding pieces of code in existing, complex methods (or functions) that can be replaced with method calls (or function calls). Consider the following complicated method: def GetTestResults(self):   # Check if results have been cached.   results = cache.get('test_results', None)   if results is None:     # No results in the cache, so check the database.     results = db.FetchResults(SQL_SELECT_TEST_RESULTS)   # Count passing and failing tests.   num_passing = len([r for r in results if r['outcome'] == 'pass'])   num_failing = len(results) - num_passing   return num_passing, num_failingThis method is difficult to test because it not only relies on a database, but also on a cache. In addition, it performs some post processing of the retrieved results. The first hint that this method could use refactoring is the abundance of comments. Extracting sections of code into well-named methods reduces the original method's complexity. When complexity is reduced, comments often become unnecessary. For example, consider the following: def GetTestResults(self):   results = self._GetTestResultsFromCache()   if results is None:     results = self._GetTestResultsFromDatabase()   return self._CountPassFail(results) def _GetTestResultsFromCache(self):   return cache.get('test_results', None) def _GetTestResultsFromDatabase(self):   return db.FetchResults(SQL_SELECT_TEST_RESULTS) def _CountPassFail(self, results):   num_passing = len([r for r in results if r['outcome'] == 'pass'])   num_failing = len(results) - num_passing   return num_passing, num_failing Now, tests can focus on each individual piece of the original method by testing each extracted method. This has the added benefit of making the code more readable and easier to maintain.(Note: Method extraction can be done for you automatically in Python by the open-source refactoring browser BicycleRepairMan, and in Java by several IDEs, including IntelliJ IDEA and Eclipse.)Remember to download this episode of Testing on the Toilet and post it in your office.", "posted by Allen Hutchison, Engineering Manager A few months ago we announced that we would be holding a Google Test Automation Conference in New York City on August 23-24. Today, I'm happy to tell you that we've finalized the speaker list and have opened the attendee registration process.The conference is completely free; however, because we have room for only 150 people, everyone who wishes to attend must apply and be accepted. You can apply to attend via this link, and we'll let you know by June 15 if your application has been accepted.     At last year's conference, we found that the informal hallway discussions were tremendously valuable to everyone who came, and we would like to continue that custom. Therefore, the main point in the application process is to tell us how you can contribute to the conversation and what you would like to learn from the conference.       To get you started, here is what our speakers will be talking about:      Apple Chow and Santiago Etchebehere -- Building a flexible and extensible automation framework around Selenium        Ali-Akber Saifee -- muvee Framework for Autonomous Testing        Olivier Warin -- Spirent's WorkSuite Manager        Douglas Sellers -- CustomInk Domain Specific Language for automating an AJAX based application        Julian Harty -- Mobile Wireless Test Automation        Matt Heusser and Sean McMillan -- Examining Interaction-Based testing        Risto Kumpulainen -- Automated testing for F-Secure's Linux/UNIX Anti-Virus products        Sergio Pinon -- User Interface Functional Testing with AFTER (Automated Functional Testing Engine in Ruby)        Vivek Prahlad -- Functional Testing Swing applications with Frankenstein   Simon Stewart -- Web Driver        Adam Porter and Atif Memon -- Skoll distributed continuous quality assurance system        Cedric Beust -- TestNG        Hadar Ziv -- Specification-based Testing         We look forward to your application and to a great conference.", "Posted by Allen Hutchison, Engineering ManagerEarlier this week a colleague sent me a link to this post over on doodlebyte. In the post, Michael Schidlowsky talks about a recent experience watching an 8-year-old, play with Logo for the first time. The child was able to crash Logo in just a few commands. After that, Michael wanted to try to reproduce the bug:I had played with UCBLogo for two weeks and hadn\u2019t made it crash once. Brian brought the whole thing down in three commands. The most telling part is that when I tried to reproduce the defect a week later I couldn\u2019t. I issued rt with a ton of 9s and just couldn\u2019t get it to break. As it turns, it only crashes when you omit the space, which of course I didn\u2019t think of doing. It took me more time to reproduce the defect than it took Brian to discover it.This story vividly demonstrates two points about exploratory testing. First is that you have to be very careful to record everything you do during a testing session. In this case, Michael overlooked the fact that the 8-year-old had issued an \"rt999...\" instead of an \"rt 999...\". That made a huge difference to his testing and resulted in his taking a lot of time to reproduce the error. Some testers use screen recorders or key loggers to help with this issue.The other point is that we who use computers every day tend to adopt certain assumptions in our interaction with them. Michael didn't try \"rt999...\" because he knew that he should always put a space between a command and a parameter. When testing an application, we have to remember that our customers don't always hold the same assumptions that we do, whether they are 8 or 80 years old. Approach your testing with an awareness of your assumptions, and try to find out what happens when you break them. Often the results will surprise you.", "With a good set of tests in place, refactoring code is much easier, as you can quickly gain a lot of confidence by running the tests again and making sure the code still passes.As suites of tests grow, it's common to see duplication emerge. Like any code, tests should ideally be kept in a state that's easy to understand and maintain. So, you'll want to refactor your tests, too.However, refactoring tests can be hard because you don't have tests for the tests.How do you know that your refactoring of the tests was safe and you didn't accidentally remove one of the assertions?If you intentionally break the code under test, the failing test can show you that your assertions are still working. For example, if you were refactoring methods in CombineHarvesterTest, you would alter CombineHarvester, making it return the wrong results.Check that the reason the tests are failing is because the assertions are failing as you'd expect them to. You can then (carefully) refactor the failing tests. If at any step they start passing, it immediately lets you know that the test is broken \u2013 undo! When you're done, remember to fix the code under test and make sure the tests pass again.(revert is your friend, but don't revert the tests!)Let's repeat that important point:When you're done...remember to fix the code under test!SummaryRefactor production code with the tests passing. This helps you determine that the production code still does what it is meant to.Refactor test code with the tests failing. This helps you determine that the test code still does what it is meant to.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Phil Rollet, Senior Software QA EngineerZurich, Switzerland is the location of one of Google's largest engineering offices in Europe. We like to say it is no longer necessary to live in Silicon Valley to develop great software for Google, and the Zurich office has over 100 software engineers working on Google products and infrastructure. As a result the Zurich Test Engineering team is kept very busy.  Producing great software is the driving motivation for each member of the Test Engineering team, which is relatively small by industry standards. But we're growing quickly and are passionate about software development, software quality, and testing.  We work on several projects: some are customer-facing and some are infrastructure. We currently work on Google Maps, which has testers and developers in several of our offices around the world. Our team builds tools to check the consistency of the data feeds that support the local search features in Maps. Another project mainly developed in Zurich is Google Transit, which provides public transportation itineraries and schedule information for commuters who take trains, buses, and trams. On this project, we build tools to verify the proper alignment of the transportation layer of the map with the actual location coordinates of transit stops. We also focus on many projects related to Google\u2019s infrastructure. For example, with Google Base API, we work with the Software Engineering team to measure response time and to track bottlenecks during large-scale bulk updates.  Our aim is to assign local test engineers to most projects developed in this office, so hiring for this team is always a priority. Candidates are from all over the world, and many different nationalities are represented in our office. Adapting to Zurich is quite easy, because it is already an international place: many companies have their headquarters here, and Zurich has been named the best city in the world for its quality of life in 2005, 2006, and 2007.", "Posted by Allen Hutchison, Engineering ManagerThe Test Engineering group at Google is truly a global organization. Our team is made up of test engineers in every corner of the world, working on all kinds of interesting projects. Today we are starting a series of blog posts to introduce you to our teams in different offices. Each post will give you some information on the testing projects we have in that particular office, and a little bit of the flavor of that region of the world. The global effort that makes up our test engineering team, and the diversity of ideas that are brought to bear on the challenges that we face, is one of our greatest strengths as a team.In the coming weeks and months, join me in learning more about Google Testing from a global perspective. Perhaps you'll find that we have a team in your part of the world.", "Posted by Allen Hutchison, Engineering ManagerThank you to everyone who submitted a proposal to speak at the Google Test Automation Conference. We'll be reading each proposal closely and selecting those that we think are best for the conference. If you submitted a proposal, then we'll be getting in touch with you directly to let you know if it was accepted or not.If you didn't submit a proposal, but are interested in attending, then check back on the Google Testing Blog on May 7 to see the conference schedule and instructions for how to apply to attend.", "Posted by Zuri Kemp, Lead Software Quality Assurance EngineerThinking about becoming a Google test engineering intern? Want to help us organize the world's information and make it universally accessible and useful -- but you're not sure what it's really like here? Over the next several months we'll be featuring articles by former Test Engineering interns. Until then, here's a top-10 list of adjectives to describe software engineers at Google:Global - Google has engineering offices (and, of course, customers) all over the world. So not only do we have engineers from everywhere, every engineer gets the chance to make software that will be as great in Singapore as it is in Finland.Comfortable - Engineers need to be comfortable to be effective...and Google gives us the equipment to be comfortable. And if our muscles get tight from sitting, we can get an on-site professional massage.Flexible -  Getting work done is more important than what time we work.Unhampered - We're a company designed by engineers for engineers, so functional merit tends to outweigh other considerations at decision-making time.Entertained - We have lots of fun activities. Every winter, there's a company-wide ski trip. Every summer, there's an all-engineer picnic on the Santa Cruz beach boardwalk. Google has bought out theaters for opening days of films like Star Wars and Lord of the Rings.Listened to - Google has a very open environment and truly values each employee's opinions. Every Friday, there's a company-wide meeting where Larry and Sergey (our founders) report on significant events and have an open-to-all Q&A session.Stuffed - In most offices, Google provides free gourmet-quality breakfast, lunch, and dinner. If you come in too late to get breakfast (or you get hungry between meals), you can always head to the nearby mini-kitchen and grab some fruit, cereal, and tea (or crisps, chocolate, and espresso for the less healthily inclined).    Cutting-edge - We have the audacious goal of organizing the world's information -- and to meet it, we've built the largest distributed computer system in the world and written some of the world's most widely-used software.    High-impact - People in every country and every language use our products.    Dependable - We try to keep a high bar for hiring, so you can depend on the ability of your teammates to handle most any engineering problem.If all of that (and this) wasn't enough to convince you that Google is a fantastic environment for a software engineer in test, consider being able to \"make and break\" cool Google products. That is exactly what test engineers at Google do. We not only manually find flaws in software engineers' code, we also build smart, home-grown tools that effectively break and test the limits of web software.While the deadline for Test Engineering internships for this summer is past, we want to help you make a decision for your internship next year. Stay tuned to our blog, we'll be posting a regular series on Test Engineering internships at Google.", "Michael Feathers defines the qualities of a good unit test as: \u201cthey run fast, they help us localize problems.\u201d This can be hard to accomplish when your code accesses a database, hits another server, is time-dependent, etc.By substituting custom objects for some of your module's dependencies, you can thoroughly test your code, increase your coverage, and still run in less than a second. You can even simulate rare scenarios like database failures and test your error handling code.A variety of different terms are used to refer to these \u201ccustom objects\u201d. In an effort to clarify the vocabulary, Gerard Meszaros provides the following definitions:Test Double is a generic term for any test object that replaces a production object.Dummy objects are passed around but not actually used. They are usually fillers for parameter lists.Fakes have working implementations, but take some shortcut (e.g., InMemoryDatabase).Stubs provide canned answers to calls made during a test.Mocks have expectations which form a specification of the calls they do and do not receive.For example, to test a simple method like getIdPrefix() in the IdGetter class:public class IdGetter {  // Constructor omitted.  public String getIdPrefix() {    try {      String s = db.selectString(\"select id from foo\");      return s.substring(0, 5);    } catch (SQLException e) { return \"\"; }  }}You could write:  db.execute(\"create table foo (id varchar(40))\");  // db created in setUp().  db.execute(\"insert into foo (id) values ('hello world!')\");  IdGetter getter = new IdGetter(db);  assertEquals(\"hello\", getter.getIdPrefix());The test above works but takes a relatively long time to run (network access), can be unreliable (db machine might be down), and makes it hard to test for errors. You can avoid these pitfalls by using stubs:  public class StubDbThatReturnsId extends Database {    public String selectString(String query) { return \"hello world\"; }  }  public class StubDbThatFails extends Database {    public String selectString(String query) throws SQLException {      throw new SQLException(\"Fake DB failure\");    }  }  public void testReturnsFirstFiveCharsOfId() throws Exception {    IdGetter getter = new IdGetter(new StubDbThatReturnsId());    assertEquals(\"hello\", getter.getIdPrefix());  }  public void testReturnsEmptyStringIfIdNotFound() throws Exception {    IdGetter getter = new IdGetter(new StubDbThatFails());    assertEquals(\"\", getter.getIdPrefix());  }Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Allen Hutchison, Engineering ManagerIf you recall, we announced the Google Test Automation Conference here about a month ago. To everyone who has submitted a proposal to speak at the conference, thank you. To those of you who haven't yet submitted your proposal, I want to remind you that the deadline for submissions is this Friday, April 6.We're looking for speakers with exciting ideas and new approaches to test automation. If you have a subject you'd like to talk about, please send an email to gtac-submission@google.com and include a description of your 45 minute session in 500 words or less (no attachments, please). Deadline for submissions is April 6.", "Sometimes you need to test client-side JavaScript code that uses setTimeout() to do some work in the future. jsUnit contains the Clock.tick() method, which simulates time passing without causing the test to sleep. For example, this function will set up some callbacks to update a status message over the course of four seconds:function showProgress(status) {  status.message = \"Loading\";  for (var time = 1000; time     // Append a '.' to the message every second for 3 secs.    setTimeout(function() {      status.message += \".\";    }, time);  }  setTimeout(function() {    // Special case for the 4th second.    status.message = \"Done\";  }, 4000);}The jsUnit test for this function would look like this:function testUpdatesStatusMessageOverFourSeconds() {  Clock.reset(); // Clear any existing timeout functions on the event queue.  var  status = {};  showProgress(status); // Call our function.  assertEquals(\"Loading\", status.message);  Clock.tick(2000); // Call any functions on the event queue that have                    // been scheduled for the first two seconds.  assertEquals(\"Loading..\",  status.message);  Clock.tick(2000); // Same thing again, for the next two seconds.  assertEquals(\"Done\", status.message);}This test will run very quickly - it does not require four seconds to run.Clock supports the functions setTimeout(),setInterval(), clearTimeout(), andclearInterval(). The Clock object is defined injsUnitMockTimeout.js, which is in the same directory asjsUnitCore.js.Remember to download this episode of Testing on the Toilet and post it in your office.", "Posted by Harry Robinson, Software Engineer in TestSoftware testing is tough. It can be exhausting, and there is rarely enough time to find all the important bugs. Wouldn't it be nice to have a staff of tireless servants working day and night to make you look good? Well, those days are here. On Thursday, March 22, I'll give a lunchtime presentation titled \"How to Build Your Own Robot Army\" for the Quality Assurance SIG of the Software Association of Oregon.    Two decades ago, machine time was expensive, so test suites had to run as quickly and efficiently as possible. Today, CPUs are cheap, so it becomes reasonable to move test creation to the shoulders of a test machine army. But we're not talking about the run-of-the-mill automated scripts that only do what you explicitly told them. We're talking about programs that create and execute tests you never thought of to find bugs you never dreamed of. From Orcs to Zergs to Droids to Cyborgs, this presentation will show how to create a robot test army using tools lying around on the Web. Most importantly, it will cover how to take appropriate credit for your army's work!", "Posted by Harry Robinson, Software Engineer in Test     The first-ever industry Developer-Tester/Tester-Developer Summit was held at the Mountain View Googleplex on Saturday, February 24th. Hosted by Elisabeth Hendrickson and Chris McMahon,  the all-day workshop consisted of experience reports and lightning talks including:       Al Snow - Form Letter Generator Technique     Chris McMahon \u2013 Emulating User Actions in Random and Deterministic Modes     Dave Liebreich \u2013 Test Mozilla     David Martinez \u2013 Tk-Acceptance     Dave W. Smith \u2013 System Effects of Slow Tests    Harry Robinson \u2013 Exploratory Automation     Jason Reid \u2013 Not Trusting Your Developers      Jeff Brown \u2013 MBUnit     Jeff Fry \u2013 Generating Methods on the Fly     Keith Ray \u2013 ckr_spec     Kurman Karabukaev \u2013 Whitebox testing using Watir     Mark Striebeck \u2013 How to Get Developers and Tester to Work Closer Together     Sergio Pinon \u2013 UI testing + Cruise Control        There were also brainstorming exercises and discussions on the benefits that DT/TDs can bring to organizations and the challenges they face. Several participants have blogged about the Summit. The discussions continue at http://groups.google.com/group/td-dt-discuss.          If you spend your days coding and testing, try this opening exercise from the Summit. Imagine that:        T \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 D  is a spectrum that has \"Tester\" at one end and \"Developer\" at the other. Where would you put yourself, and why?", "Posted by Allen Hutchison, Engineering Manager  Some of the most difficult challenges in creating great software are guaranteeing it works every time, for every customer, ensuring that it will scale well, and making it accessible to all users. Over the years, languages have become easier to work with, frameworks have become extensible for the creation of several products, and integrated development environments have made the software developer faster and more productive. But automation techniques, extensible testing frameworks, and easy-to-use test tools have lagged behind. While there are many good solutions for automated testing, there is plenty of room for innovation.I'm happy to announce that Google will be hosting our 2nd Annual Google Test Automation Conference (GTAC) in our New York office on August 23 and 24, 2007. Our goal is to create a collegial atmosphere where participants can discuss challenges facing people on the cutting edge of test automation, evaluate solutions for meeting those challenges, and have a little fun.Call for ProposalsWe're looking for speakers with exciting ideas and new approaches to test automation. If you have a subject you'd like to talk about, please send an email to gtac-submission@google.com and include a description of your 45 minute session in 500 words or less (no attachments, please). Deadline for submissions is April 6.We're planning to have 10 people give presentations at the conference, followed by adequate time for discussion. If you'd like to attend as a non-speaker, please check back to this page on May 7, when we'll post our slate of speakers and information about how to attend.", "Posted by Allen Hutchison, Engineering Manager and Jay Han, Software Engineer in TestThe testing world has a lot of terms for the activity that we undertake every day. You'll often hear the words QA, QC, and Test Engineering used interchangeably. While it is usually enough to get your point across with a developer, it is helpful to think about these terms and how they apply to the world of software testing. In the classic definition QC is short for Quality Control, a process of verifying predefined requirements for quality. In the terms of an assembly-line this might involve pulling manufactured units off at the end of the process and verifying different parts of the assembly process. For software the QC function may involve checking the software against a set of requirements and verifying that the software meets the predefined requirements.Quality Assurance, on the other hand, is much more about providing the continuous and consistent improvement and maintenance of process that enables the QC job. We use the QC process to verify a product does what we think it does, and we use the QA process to give us confidence that the product will meet the needs of customers. To that end the QA process can be considered a meta process that includes aspects of the QC process. It also goes beyond that to influence usability and design, to verify that functionality is not only correct, but useful.Here at Google, we tend to take a third approach that we call Test Engineering. We look at this as a bridge between the meta world of QA and the concrete world of QC. Our approach allows us to ensure that we get the opportunity to think about customers and their needs, while we still provide results that are needed on day to day engineering projects.Our teams certainly work with Software Engineers in QA and QC roles, but we also work with teams to ensure that a product is testable, that it is adequately unit tested, and that it can be automated even further in our teams. We often review design documents and ask for more test hooks in a project, and we implement mock objects and servers to help developers with their unit testing and to allow our teams to test components individually.We put an emphasis on building automated tests so that we can let people do what people are good at, and have computers do what computers are good at. That doesn't mean that we never do manual testing, but instead that we do the \"right\" amount of manual testing with more human-oriented focus (e.g. exploratory testing), and we try to ensure that we never do repetitive manual testing.", "Posted by Mark Striebeck, Engineering Project ManagerYesterday we held a global internal test conference that we called \"Testapalooza\".The idea for Testapalooza came out of discussions about how to build a vibrant testing community here at Google. Many diverse groups work daily on quality-related activities, but each group uses different tools and has different ideas for testing an application, so it can be difficult to find out what others are doing. So we decided to put on a conference!We asked engineers from Testing, Development, User Experience, and other groups to submit conference sessions: tool presentations, tutorials, workshops, panels, and experience reports. We hoped to get 30-40 submissions from which we could select about 20. In typical conference mode, the day before the submission deadline, we had 12 submissions. The day after the deadline, we had more than 130! It was very impressive and fun to read what our engineers submitted. We had some of our most involved engineers on the reviewing committee, and even they were surprised about the breadth and depth of the proposed sessions. It was extremely hard to pick just 41 of these proposals, but we couldn't fit any more into a one-day conference.We ran 11 tracks: Agile, Automation, Developer, Internationalization, Perf, QA, Release Engineering, Security, Reliability, SysOps, and User Experience. Registrations for the event filled up quickly and proved that there is indeed a great desire for more cross-specialty collaboration: software developers signed up to attend QA sessions, operations engineers learned more about unit testing, and QA engineers were everywhere.The conference was a great success. We had sessions going the whole day, and people were discussing testing in the hallways. New ideas were generated and debated in every corner of the Googleplex. People appreciated the variety of topics from agile testing to micro-level unit testing to testing tools to usability testing. We also had a poster session, where internal groups could show other Googlers what they were doing, our equivalent of the conference expo.Of course, this wouldn't be a true Google event without some great food, and we were fortunate to have enthusiastic participation from our chefs: Taste-a-Palooza!We finished the day with an hour of lightning talks. At the end of the day everybody was exhausted, but with new and interesting ideas to think about.All Testapalooza sessions were video recorded (many were videoconferenced to other offices). We want to publish as many of these videos as possible, and will review them over the coming weeks to publish sessions which did not contain any confidential information. Watch this space for more information on the videos.", "Posted by Allen Hutchison, Engineering ManagerRegardless of the amount of testing you do for an application, if the application doesn't scale, there is a good chance that no one will ever see it. As you can imagine, we at Google care a lot about scalability. In fact, it's rare to talk to another Googler about a new idea without the question, \"How does it scale?\" coming into the discussion. On June 23, our Seattle office will host a conference on scalable systems.The team is currently accepting proposals for 45-minute talks. You can find out more from the Google Research Blog.", "Posted by Allen Hutchison, Engineering ManagerGooglers in our Test Engineering group often speak at, and write for, forums on testing all over the world. This week our own Julian Harty has published an interesting article titled \"Improving the Accuracy of Tests by Weighing the Results\". In this piece, Julian says: In software as in life there are things we notice that help confirm whether something is satisfactory or unsatisfactory. Let's call these things that affect our judgment of the results \"factors.\" Some factors provide stronger indications than others. When using these factors to rate results, we will assign higher scores (or \"weightings\") to the stronger indicators. By assigning higher weightings to the stronger indicators, we enable these to have a stronger influence on the overall outcome. You can read the rest of the article on Stickyminds.com.", "Posted by Harry Robinson, Software Engineer in Test Several readers have commented that our current blog slogan, \"Life is too short for manual testing,\" implies that we don't value manual and exploratory testing. In fact, we are big fans of exploratory testing, and we intended the message to be liberating, not insulting. Manual testing can find bugs quickly and with little overhead in the short run. But it can be expensive and exhausting in a long project. And manual testing is gated by how fast and long humans can work. Running millions of test sequences and combinations by hand would take longer than most people's lifetimes - life is literally too short for manual testing to reach all the bugs worth reaching.  We originally featured the \"Life is too short ...\" slogan on T-shirts at the Google London Test Automation Conference. One theme of that conference was that it makes sense to get machines to do some of the heavy lifting in software testing, leaving human testers free to do the kinds of testing that people do well. If you'd like to find out more about computer-assisted testing, check out the LTAC videos as well as Cem Kaner's excellent STAR 2004 presentation on High Volume Test Automation . And if you can wait a bit, I will be doing a talk on \"The Bionic Exploratory Tester\" at CAST 2007 in Bellevue, Washington, in July.I asked Jon Bach 's opinion on the slogan, and he suggested that what we are really trying to say is:       Life's too short to only use an approach for testing that relies solely on a human's ability to execute a series of mouse clicks and keystrokes when the processing power that makes computers so useful can be leveraged to execute these tests, freeing testers from especially mundane or repetitive testing so that their brains can be used for higher order tests that computers can't do yet.        I agree, but it would've been a heck of a T-shirt. :-)              Future slogans:    Testing is about being willing to try different approaches and entertain different perspectives, so a single slogan can't do it justice. We are planning to feature different slogans on a regular basis, and already have a few of our favorites lined up. If you've got a slogan to share, we'd love to hear it. Post it in the comments below or email us.", "For a class, try having a corresponding set of test methods, where each one describes a responsibility of the object, with the first word implicitly the name of the class under test. For example, in Java:  class HtmlLinkRewriterTest ... {    void testAppendsAdditionalParameterToUrlsInHrefAttributes(){?}    void testDoesNotRewriteImageOrJavascriptLinks(){?}    void testThrowsExceptionIfHrefContainsSessionId(){?}    void testEncodesParameterValue(){?}  }This can be read as:    HtmlLinkRewriter appends additional param to URLs in href attrs.   HtmlLinkRewriter does not rewrite image or JavaScript links.   HtmlLinkRewriter throws exception if href contains session ID.   HtmlLinkRewriter encodes parameter value.   BenefitsThe tests emphasizes the object's responsibilities (or features) rather than public methods and inputs/output. This makes it easier for future engineers who want to know what it does without having to delve into the code.These naming conventions can help point out smells. For example, when it's hard to construct a sentence where the first word is the class under test, it suggests the test may be in the wrong place. And classes that are hard to describe in general often need to be broken down into smaller classes with clearer responsibilities.Additionally, tools can be used to help understand code quicker:(This example shows a class in IntelliJ with the TestDox plugin giving an overview of the test.)Remember to download this episode of Testing on the Toilet, print it, and flyer your office.", "Posted by Harry Robinson, Software Engineer in Test    On Saturday, February 24, the Mountain View Googleplex will offer hospitality, support and free munchies for the first-ever \"Developer-Tester/Tester-Developer Summit\" -- a peer-driven gathering of developer-testers and tester-developers from around the industry to share knowledge and code.              The Summit is being organized by Elisabeth Hendrickson of Quality Tree Software and Chris McMahon of Lyris Technologies, who describe the day this way:   Our emphasis will be on good coding practices for testing, and good testing practices for automation. That might include topics like: test code and patterns; refactoring test code; creating abstract layers; programmatically analyzing/verifying large amounts of data; achieving repeatability with random tests; OO model-based tests; creating domain specific languages; writing test fixtures or harnesses; and/or automatically generating large amounts of test data.                                Check out this post at Test Obsessed to learn more about the inspiration behind DT/TD and how you can participate.", "So you've learned all about method stubs, mock objects, and fakes. You might be tempted to stub out slow or I/O-dependent built-ins. For example:  def Foo(path):   if os.path.exists(path):     return DoSomething()   else:     return DoSomethingElse() def testFoo(self):         # Somewhere in your unit test class   old_exists = os.path.exists   try:     os.path.exists = lambda x: True     self.assertEqual(Foo('bar'), something)     os.path.exists = lambda x: False     self.assertEqual(Foo('bar'), something_else)   finally:     # Remember to clean-up after yourself!     os.path.exists = old_existsCongratulations, you just achieved 100% coverage! Unfortunately, you might find that this test fails in strange ways. For example, given the following DoSomethingElse which checks the existence of a different file:  def DoSomethingElse():   assert os.path.exists(some_other_file)   return some_other_fileFoo will now throw an exception in its second invocation because os.path.exists returns False so the assertion fails.You could avoid this problem by stubbing or mocking out DoSomethingElse, but the task might be daunting in a real-life situation.  Instead, it is safer and faster to parameterize the built-in: def Foo(path, path_checker=os.path.exists):   if path_checker(path):     return DoSomething()   else:     return DoSomethingElse() def testFoo(self):   self.assertEqual(Foo('bar', lambda x: True), something)   self.assertEqual(Foo('bar', lambda x: False), something_else)Remember to download this episode of Testing on the Toilet, print it, and flyer your office.", "Posted by Allen Hutchison, Engineering ManagerI work in the Test Engineering team, and have always been passionate about sharing my test experiences with others. Last year we organized the first Google Test Automation Conference, where about 150 people from the testing community got together at our London office for two days to talk about testing. One of the great bits of feedback that we heard was that people were very interested in our experiences with testing at Google. We decided to start this blog to share our experiences with the rest of the developer community, and so that we can learn about some of your smart solutions to the same sorts of problems.From unit testing to performance testing and beyond, the testing umbrella is vast, and hopefully this blog will be the start of a great conversation about these important issues. Whether you're already deeply immersed in the world of testing or just starting to get your feet wet, we hope that this blog will help spread our enthusiasm about testing and help you to write better code. We don't have all the answers, but we have gained a lot of experience and we are anxious to share it with you! So please keep reading, and do send us your comments.", "We want you to write more tests. Yes, you. You've already been told that tests are the safety net that protects you when you need to refactor your code, or when another developer adds features. You even know that tests can help with the design of your code. But, although you've read the books and heard the lectures, maybe you need a little more inspiration, tips, and prodding.  And you need it to be in a place where when you see it, you can't ignore it.That's where we can help.  We're the \"Google Testing Grouplet,\" a small band of volunteers who are passionate about software testing.We're unveiling the public release of \"Testing on the Toilet\": one of Google's little secrets that has helped us to inspire our developers to write well-tested code. We write flyers about everything from dependency injection to code coverage, and then regularly plaster the bathrooms all over Google with each episode, almost 500 stalls worldwide.  We've received a lot of feedback about it.  Some favorable (\"This is great because I'm always forgetting to bring my copy of Linux Nerd 2000 to the bathroom!\") and some not (\"I'm trying to use the bathroom, can you folks please just LEAVE ME ALONE?\").  Even the Washington Post noticed.We've decided to share this secret weapon with the rest of the world to help spread our passion for testing, and to provide a fun and easy way for you to educate yourself and the rest of your company about these important tricks and techniques.We'll be putting episodes on this blog on a regular basis and providing PDFs so you can print them out and put them up in your own bathrooms, hallways, kitchens, moon bases, secret underground fortresses, billionaire founders' Priuses, wherever. Send your photos and stories to  TotT@google.com and let us know how Testing on the Toilet is received at your company.And meanwhile, keep writing those tests."], "urls": ["http://googletesting.blogspot.com/2016/02/earlgrey-ios-functional-ui-testing.html", "http://googletesting.blogspot.com/2015/12/gtac-2015-wrap-up.html", "http://googletesting.blogspot.com/2015/11/gtac-2015-is-next-week.html", "http://googletesting.blogspot.com/2015/10/announcing-gtac-2015-agenda.html", "http://googletesting.blogspot.com/2015/10/audio-testing-automatic-gain-control.html", "http://googletesting.blogspot.com/2015/08/the-deadline-to-apply-for-gtac-2015-is.html", "http://googletesting.blogspot.com/2015/06/gtac-2015-call-for-proposals-attendance.html", "http://googletesting.blogspot.com/2015/05/gtac-2015-coming-to-cambridge-greater.html", "http://googletesting.blogspot.com/2015/05/multi-repository-development.html", "http://googletesting.blogspot.com/2015/04/just-say-no-to-more-end-to-end-tests.html", "http://googletesting.blogspot.com/2015/04/quantum-quality.html", "http://googletesting.blogspot.com/2015/03/android-ui-automated-testing.html", "http://googletesting.blogspot.com/2015/02/the-first-annual-testing-on-toilet.html", "http://googletesting.blogspot.com/2015/01/testing-on-toilet-change-detector-tests.html", "http://googletesting.blogspot.com/2015/01/testing-on-toilet-prefer-testing-public.html", "http://googletesting.blogspot.com/2014/12/testing-on-toilet-truth-fluent.html", "http://googletesting.blogspot.com/2014/12/gtac-2014-wrap-up.html", "http://googletesting.blogspot.com/2014/11/protractor-angular-testing-made-easy.html", "http://googletesting.blogspot.com/2014/10/gtac-2014-is-this-week.html", "http://googletesting.blogspot.com/2014/10/testing-on-toilet-writing-descriptive.html", "http://googletesting.blogspot.com/2014/09/announcing-gtac-2014-agenda.html", "http://googletesting.blogspot.com/2014/09/chrome-firefox-webrtc-interop-test-pt-2.html", "http://googletesting.blogspot.com/2014/08/chrome-firefox-webrtc-interop-test-pt-1.html", "http://googletesting.blogspot.com/2014/08/testing-on-toilet-web-testing-made.html", "http://googletesting.blogspot.com/2014/07/testing-on-toilet-dont-put-logic-in.html", "http://googletesting.blogspot.com/2014/07/the-deadline-to-sign-up-for-gtac-2014.html", "http://googletesting.blogspot.com/2014/07/measuring-coverage-at-google.html", "http://googletesting.blogspot.com/2014/06/threadsanitizer-slaughtering-data-races.html", "http://googletesting.blogspot.com/2014/06/gtac-2014-call-for-proposals-attendance.html", "http://googletesting.blogspot.com/2014/06/gtac-2014-coming-to-seattlekirkland-in.html", "http://googletesting.blogspot.com/2014/05/testing-on-toilet-risk-driven-testing.html", "http://googletesting.blogspot.com/2014/05/testing-on-toilet-effective-testing.html", "http://googletesting.blogspot.com/2014/04/testing-on-toilet-test-behaviors-not.html", "http://googletesting.blogspot.com/2014/04/the-real-test-driven-development.html", "http://googletesting.blogspot.com/2014/03/testing-on-toilet-what-makes-good-test.html", "http://googletesting.blogspot.com/2014/03/whenhow-to-use-mockito-answer.html", "http://googletesting.blogspot.com/2014/02/minimizing-unreproducible-bugs.html", "http://googletesting.blogspot.com/2014/01/the-google-test-and-development_21.html", "http://googletesting.blogspot.com/2014/01/the-google-test-and-development.html", "http://googletesting.blogspot.com/2013/12/the-google-test-and-development.html", "http://googletesting.blogspot.com/2013/11/webrtc-audio-quality-testing.html", "http://googletesting.blogspot.com/2013/10/espresso-for-android-is-here.html", "http://googletesting.blogspot.com/2013/08/how-google-team-tests-mobile-apps.html", "http://googletesting.blogspot.com/2013/08/testing-on-toilet-test-behavior-not.html", "http://googletesting.blogspot.com/2013/07/testing-on-toilet-know-your-test-doubles.html", "http://googletesting.blogspot.com/2013/06/testing-on-toilet-fake-your-way-to.html", "http://googletesting.blogspot.com/2013/06/optimal-logging.html", "http://googletesting.blogspot.com/2013/05/testing-on-toilet-dont-overuse-mocks.html", "http://googletesting.blogspot.com/2013/05/gtac-2013-wrap-up.html", "http://googletesting.blogspot.com/2013/04/gtac-is-almost-here.html", "http://googletesting.blogspot.com/2013/04/two-new-videos-about-testing-at-google.html", "http://googletesting.blogspot.com/2013/03/testing-on-toilet-testing-state-vs.html", "http://googletesting.blogspot.com/2013/03/announcing-gtac-2013-agenda.html", "http://googletesting.blogspot.com/2013/01/the-deadline-to-signup-for-gtac-is-jan.html", "http://googletesting.blogspot.com/2013/01/test-engineers-google.html", "http://googletesting.blogspot.com/2012/12/gtac-call-for-proposals-attendance.html", "http://googletesting.blogspot.com/2012/11/testacular-spectacular-test-runner-for.html", "http://googletesting.blogspot.com/2012/11/a-virtual-battlefield-with-bugs-ship.html", "http://googletesting.blogspot.com/2012/10/gtac-coming-to-new-york-in-spring.html", "http://googletesting.blogspot.com/2012/10/why-are-there-so-many-c-testing.html", "http://googletesting.blogspot.com/2012/10/hermetic-servers.html", "http://googletesting.blogspot.com/2012/09/conversation-with-test-engineer.html", "http://googletesting.blogspot.com/2012/08/testing-20.html", "http://googletesting.blogspot.com/2012/08/testing-googles-new-api-infrastructure.html", "http://googletesting.blogspot.com/2012/08/covering-all-your-codebases.html", "http://googletesting.blogspot.com/2012/08/welcome-to-next-generation-of-google.html", "http://googletesting.blogspot.com/2011/11/rpf-googles-record-playback-framework.html", "http://googletesting.blogspot.com/2011/11/gtac-videos-now-available.html", "http://googletesting.blogspot.com/2011/10/scriptcover-makes-javascript-coverage.html", "http://googletesting.blogspot.com/2011/10/google-test-analytics-now-in-open.html", "http://googletesting.blogspot.com/2011/10/google-js-test-now-in-open-source.html", "http://googletesting.blogspot.com/2011/10/take-bite-out-of-bugs-and-redundant.html", "http://googletesting.blogspot.com/2011/10/unleash-qualitybots.html", "http://googletesting.blogspot.com/2011/09/announcing-final-gtac-agenda.html", "http://googletesting.blogspot.com/2011/09/10-minute-test-plan.html", "http://googletesting.blogspot.com/2011/08/google-developer-day-2011.html", "http://googletesting.blogspot.com/2011/08/gtac-speakers-and-attendees-finalized.html", "http://googletesting.blogspot.com/2011/08/pretotyping-different-type-of-testing.html", "http://googletesting.blogspot.com/2011/08/keynote-lineup-for-gtac-2011.html", "http://googletesting.blogspot.com/2011/07/how-we-tested-google-instant-pages.html", "http://googletesting.blogspot.com/2011/07/gtac-call-for-team-attendance.html", "http://googletesting.blogspot.com/2011/06/google-at-star-west-2011.html", "http://googletesting.blogspot.com/2011/06/lessons-in-21st-century-tech-career.html", "http://googletesting.blogspot.com/2011/06/introducing-dom-snitch-our-passive-in.html", "http://googletesting.blogspot.com/2011/06/gtac-2011-keynotes.html", "http://googletesting.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html", "http://googletesting.blogspot.com/2011/05/how-google-tests-software-part-seven.html", "http://googletesting.blogspot.com/2011/05/gtac-2011-open-for-submission.html", "http://googletesting.blogspot.com/2011/05/how-google-tests-software-break-for-q.html", "http://googletesting.blogspot.com/2011/05/how-google-tests-software-part-six.html", "http://googletesting.blogspot.com/2011/04/gtac-2011-cloudy-with-chance-of-tests.html", "http://googletesting.blogspot.com/2011/04/entrepreneurial-innovation-at-google.html", "http://googletesting.blogspot.com/2011/04/set-career-path.html", "http://googletesting.blogspot.com/2011/03/were-back-live-on-twitter_31.html", "http://googletesting.blogspot.com/2011/03/how-google-tests-software-part-five.html", "http://googletesting.blogspot.com/2011/03/innovation-at-google.html", "http://googletesting.blogspot.com/2011/03/how-google-tests-software-part-four.html", "http://googletesting.blogspot.com/2011/02/this-code-is-crap.html", "http://googletesting.blogspot.com/2011/02/how-google-tests-software-brief.html", "http://googletesting.blogspot.com/2011/02/who-reads-this-blog.html", "http://googletesting.blogspot.com/2011/02/how-google-tests-software-part-three.html", "http://googletesting.blogspot.com/2011/02/how-google-tests-software-part-two.html", "http://googletesting.blogspot.com/2011/01/how-google-tests-software.html", "http://googletesting.blogspot.com/2011/01/google-innovation-and-pretotyping.html", "http://googletesting.blogspot.com/2011/01/new-years-resolutions.html", "http://googletesting.blogspot.com/2010/12/gtac-5-videos-slides-abstracts.html", "http://googletesting.blogspot.com/2010/12/test-sizes.html", "http://googletesting.blogspot.com/2010/12/chrome-os-pilot-program-announced.html", "http://googletesting.blogspot.com/2010/11/ingredients-list-for-testing-part-seven.html", "http://googletesting.blogspot.com/2010/11/ingredients-list-for-testing-part-six.html", "http://googletesting.blogspot.com/2010/11/test-your-app-from-right-to-left.html", "http://googletesting.blogspot.com/2010/10/india-withdrawals.html", "http://googletesting.blogspot.com/2010/10/keynote-at-qcon-sfo.html", "http://googletesting.blogspot.com/2010/10/gtac-starts-tomorrow.html", "http://googletesting.blogspot.com/2010/10/ingredients-list-for-testing-part-five.html", "http://googletesting.blogspot.com/2010/09/update-speakers-talks-for-gtac-2010-we.html", "http://googletesting.blogspot.com/2010/09/aftermath-of-google-ny-event.html", "http://googletesting.blogspot.com/2010/09/ingredients-list-for-testing-part-four.html", "http://googletesting.blogspot.com/2010/09/our-culture.html", "http://googletesting.blogspot.com/2010/09/our-impact.html", "http://googletesting.blogspot.com/2010/09/its-not-qa.html", "http://googletesting.blogspot.com/2010/09/test-open-house-in-new-york.html", "http://googletesting.blogspot.com/2010/09/ingredients-list-for-testing-part-three.html", "http://googletesting.blogspot.com/2010/08/ingredients-list-for-testing-part-two.html", "http://googletesting.blogspot.com/2010/08/ingredients-list-for-testing-part-one.html", "http://googletesting.blogspot.com/2010/08/test-driven-code-review.html", "http://googletesting.blogspot.com/2010/07/code-coverage-goal-80-and-no-less.html", "http://googletesting.blogspot.com/2010/07/there-but-for-grace-of-testing-go-i.html", "http://googletesting.blogspot.com/2010/06/testivus-testability-and-dr-jekill-and.html", "http://googletesting.blogspot.com/2010/06/update-gtac-2010-keynote-speakers.html", "http://googletesting.blogspot.com/2010/06/test-driven-integration.html", "http://googletesting.blogspot.com/2010/05/gtac-call-for-attendance-proposals.html", "http://googletesting.blogspot.com/2010/05/do-know-evil.html", "http://googletesting.blogspot.com/2010/04/gtac.html", "http://googletesting.blogspot.com/2010/04/google-test-automation-conference-2010.html", "http://googletesting.blogspot.com/2010/04/googles-innovation-factory-and-how.html", "http://googletesting.blogspot.com/2010/03/google-is-hiring-sets.html", "http://googletesting.blogspot.com/2010/03/google-icst-2010.html", "http://googletesting.blogspot.com/2010/03/still-stuck-in-90s.html", "http://googletesting.blogspot.com/2010/02/interview-with-copeland.html", "http://googletesting.blogspot.com/2010/02/testing-in-data-center-manufacturing-no.html", "http://googletesting.blogspot.com/2010/01/interviewing-insights-and-test.html", "http://googletesting.blogspot.com/2009/12/httptwittercomgoogletesting.html", "http://googletesting.blogspot.com/2009/12/if-you-were-brand-new-qa-manager-cont.html", "http://googletesting.blogspot.com/2009/12/if-you-were-brand-new-qa-manager.html", "http://googletesting.blogspot.com/2009/11/testing-chrome-os.html", "http://googletesting.blogspot.com/2009/11/how-to-get-started-with-tdd.html", "http://googletesting.blogspot.com/2009/10/fedex-tour.html", "http://googletesting.blogspot.com/2009/10/tott-making-perfect-matcher.html", "http://googletesting.blogspot.com/2009/10/cost-of-testing.html", "http://googletesting.blogspot.com/2009/09/tott-literate-testing-with-matchers.html", "http://googletesting.blogspot.com/2009/09/checked-exceptions-i-love-you-but-you.html", "http://googletesting.blogspot.com/2009/09/plague-of-entropy.html", "http://googletesting.blogspot.com/2009/09/it-is-not-about-writing-tests-its-about.html", "http://googletesting.blogspot.com/2009/09/7th-plague-and-beyond.html", "http://googletesting.blogspot.com/2009/08/update-speakers-talks-for-gtac.html", "http://googletesting.blogspot.com/2009/08/super-fast-js-testing.html", "http://googletesting.blogspot.com/2009/08/7th-plague.html", "http://googletesting.blogspot.com/2009/08/tott-testing-gwt-without-gwttest.html", "http://googletesting.blogspot.com/2009/07/how-to-think-about-oo.html", "http://googletesting.blogspot.com/2009/07/call-for-attendance-google-test.html", "http://googletesting.blogspot.com/2009/07/plague-of-blindness.html", "http://googletesting.blogspot.com/2009/07/update-gtac-keynote-speakers-niklaus.html", "http://googletesting.blogspot.com/2009/07/plague-of-homelessness.html", "http://googletesting.blogspot.com/2009/07/blog-stats.html", "http://googletesting.blogspot.com/2009/07/plague-of-boredom.html", "http://googletesting.blogspot.com/2009/07/blog-editor-moderating-comments.html", "http://googletesting.blogspot.com/2009/07/by-shyam-seshadri-nowadays-when-i-talk.html", "http://googletesting.blogspot.com/2009/07/software-testing-categorization.html", "http://googletesting.blogspot.com/2009/07/plague-of-amnesia.html", "http://googletesting.blogspot.com/2009/07/update-google-test-automation.html", "http://googletesting.blogspot.com/2009/07/old-habits-die-hard.html", "http://googletesting.blogspot.com/2009/07/why-are-we-embarrassed-to-admit-that-we.html", "http://googletesting.blogspot.com/2009/07/separation-anxiety.html", "http://googletesting.blogspot.com/2009/06/by-james.html", "http://googletesting.blogspot.com/2009/06/7-plagues-of-software-testing.html", "http://googletesting.blogspot.com/2009/06/gtac-call-for-proposals.html", "http://googletesting.blogspot.com/2009/06/google-test-automation-conference-2009.html", "http://googletesting.blogspot.com/2009/06/burning-test-questions-at-google.html", "http://googletesting.blogspot.com/2009/06/im-googler-now.html", "http://googletesting.blogspot.com/2009/06/james-whittaker-joins-google.html", "http://googletesting.blogspot.com/2009/06/my-selenium-tests-arent-stable.html", "http://googletesting.blogspot.com/2009/05/yet-another-javascript-testing.html", "http://googletesting.blogspot.com/2009/05/web-app-acceptance-test-survival.html", "http://googletesting.blogspot.com/2009/05/survival-techniques-for-web-app.html", "http://googletesting.blogspot.com/2009/04/survival-techniques-for-acceptance.html", "http://googletesting.blogspot.com/2009/04/beyond-testing-becoming-part-of.html", "http://googletesting.blogspot.com/2009/02/fast-exploratory-tests-with-iframes.html", "http://googletesting.blogspot.com/2009/02/tott-partial-mocks-using-forwarding_19.html", "http://googletesting.blogspot.com/2009/02/constructor-injection-vs-setter.html", "http://googletesting.blogspot.com/2009/02/to-assert-or-not-to-assert.html", "http://googletesting.blogspot.com/2009/02/with-all-sport-drug-scandals-of-late.html", "http://googletesting.blogspot.com/2009/01/tott-keep-your-fakes-simple.html", "http://googletesting.blogspot.com/2009/01/when-to-use-dependency-injection.html", "http://googletesting.blogspot.com/2009/01/tott-use-easymock.html", "http://googletesting.blogspot.com/2009/01/interfacing-with-hard-to-test-third.html", "http://googletesting.blogspot.com/2008/12/static-methods-are-death-to-testability.html", "http://googletesting.blogspot.com/2008/12/posted-by-lydia-ash-gtac-conference.html", "http://googletesting.blogspot.com/2008/12/tott-mockers-of-c-world-delight.html", "http://googletesting.blogspot.com/2008/12/announcing-google-c-mocking-framework.html", "http://googletesting.blogspot.com/2008/12/mockers-of-c-world-delight.html", "http://googletesting.blogspot.com/2008/12/by-miko-hevery-google-tech-talks.html", "http://googletesting.blogspot.com/2008/11/guide-to-writing-testable-code.html", "http://googletesting.blogspot.com/2008/11/posted-by-alex-icev-test-engineering.html", "http://googletesting.blogspot.com/2008/11/clean-code-talks-global-state-and.html", "http://googletesting.blogspot.com/2008/11/my-unified-theory-of-bugs.html", "http://googletesting.blogspot.com/2008/11/tott-contain-your-environment.html", "http://googletesting.blogspot.com/2008/11/clean-code-talks-dependency-injection.html", "http://googletesting.blogspot.com/2008/11/clean-code-talks-unit-testing.html", "http://googletesting.blogspot.com/2008/11/partial-automation-keeping-humans-in.html", "http://googletesting.blogspot.com/2008/10/tott-contain-your-environment.html", "http://googletesting.blogspot.com/2008/10/gui-testing-dont-sleep-without.html", "http://googletesting.blogspot.com/2008/10/testability-explorer-measuring.html", "http://googletesting.blogspot.com/2008/10/dependency-injection-myth-reference.html", "http://googletesting.blogspot.com/2008/10/test-engineering-at-google.html", "http://googletesting.blogspot.com/2008/10/tott-floating-point-comparison.html", "http://googletesting.blogspot.com/2008/10/to-new-or-not-to-new.html", "http://googletesting.blogspot.com/2008/10/tott-simulating-time-in-jsunit-tests.html", "http://googletesting.blogspot.com/2008/10/six-hats-of-software-testing.html", "http://googletesting.blogspot.com/2008/09/by-miko-hevery-we-talked-about-how-it.html", "http://googletesting.blogspot.com/2008/09/tott-mockin-ur-objectz.html", "http://googletesting.blogspot.com/2008/09/presubmit-and-performance.html", "http://googletesting.blogspot.com/2008/09/google-maps-api-open-sources-their.html", "http://googletesting.blogspot.com/2008/09/where-have-all-new-operators-gone.html", "http://googletesting.blogspot.com/2008/09/test-first-is-fun_08.html", "http://googletesting.blogspot.com/2008/09/my-main-method-is-better-than-yours.html", "http://googletesting.blogspot.com/2008/09/tott-data-driven-traps.html", "http://googletesting.blogspot.com/2008/08/taming-beast.html", "http://googletesting.blogspot.com/2008/08/root-cause-of-singletons.html", "http://googletesting.blogspot.com/2008/08/tott-sleeping-synchronization.html", "http://googletesting.blogspot.com/2008/08/where-have-all-singletons-gone.html", "http://googletesting.blogspot.com/2008/08/by-miko-hevery-so-you-join-new-project.html", "http://googletesting.blogspot.com/2008/08/tott-100-and-counting.html", "http://googletesting.blogspot.com/2008/08/progressive-developer-knows-that-in.html", "http://googletesting.blogspot.com/2008/08/by-miko-hevery-so-you-decided-to.html", "http://googletesting.blogspot.com/2008/08/gtac-attendance-application-deadline.html", "http://googletesting.blogspot.com/2008/07/circular-dependency-in-constructors-and.html", "http://googletesting.blogspot.com/2008/07/tott-testing-against-interfaces.html", "http://googletesting.blogspot.com/2008/07/how-to-write-3v1l-untestable-code.html", "http://googletesting.blogspot.com/2008/07/breaking-law-of-demeter-is-like-looking.html", "http://googletesting.blogspot.com/2008/07/yui-and-gwt-how-do-you-test.html", "http://googletesting.blogspot.com/2008/07/call-for-attendance-gtac-2008.html", "http://googletesting.blogspot.com/2008/07/how-to-think-about-new-operator-with.html", "http://googletesting.blogspot.com/2008/07/tott-expect-vs-assert.html", "http://googletesting.blogspot.com/2008/07/announcing-new-google-c-testing.html", "http://googletesting.blogspot.com/2008/06/defeat-static-cling.html", "http://googletesting.blogspot.com/2008/06/test-engineering-class-project-at-uc.html", "http://googletesting.blogspot.com/2008/06/productivity-games-using-games-to.html", "http://googletesting.blogspot.com/2008/06/taming-beast-aka-how-to-test-ajax.html", "http://googletesting.blogspot.com/2008/06/tott-friends-you-can-depend-on.html", "http://googletesting.blogspot.com/2008/06/3-days-left-for-gtac-proposals.html", "http://googletesting.blogspot.com/2008/05/performance-testing-of-distributed-file.html", "http://googletesting.blogspot.com/2008/05/tott-invisible-branch.html", "http://googletesting.blogspot.com/2008/05/intro-to-ad-quality-test-challenges.html", "http://googletesting.blogspot.com/2008/05/exploratory-testing-on-chat.html", "http://googletesting.blogspot.com/2008/05/tott-using-dependancy-injection-to.html", "http://googletesting.blogspot.com/2008/05/tott-testable-contracts-make.html", "http://googletesting.blogspot.com/2008/04/gtac-call-for-proposals.html", "http://googletesting.blogspot.com/2008/04/gtac-2008-in-seattle.html", "http://googletesting.blogspot.com/2008/04/tott-avoiding-flakey-tests.html", "http://googletesting.blogspot.com/2008/04/tott-time-is-random.html", "http://googletesting.blogspot.com/2008/03/tott-testng-on-toilet.html", "http://googletesting.blogspot.com/2008/03/watching-movies-to-find-localization.html", "http://googletesting.blogspot.com/2008/03/cost-benefit-analysis-of-test.html", "http://googletesting.blogspot.com/2008/03/tott-understanding-your-coverage-data.html", "http://googletesting.blogspot.com/2008/02/ieee-article.html", "http://googletesting.blogspot.com/2008/02/in-movie-amadeus-austrian-emperor.html", "http://googletesting.blogspot.com/2008/02/tott-stroop-effect.html", "http://googletesting.blogspot.com/2008/02/announcing-google-testing-blog-german.html", "http://googletesting.blogspot.com/2008/01/testing-systems-with-large-and-complex.html", "http://googletesting.blogspot.com/2008/01/primer-in-testing-mobile-phone.html", "http://googletesting.blogspot.com/2007/10/tott-avoiding-friend-twister-in-c.html", "http://googletesting.blogspot.com/2007/10/automating-tests-vs-test-automation.html", "http://googletesting.blogspot.com/2007/10/overview-of-infrastructure-testing.html", "http://googletesting.blogspot.com/2007/10/testing-google-mashup-editor-class.html", "http://googletesting.blogspot.com/2007/10/performance-testing.html", "http://googletesting.blogspot.com/2007/10/post-release-closing-loop_02.html", "http://googletesting.blogspot.com/2007/09/but-it-works-on-my-machine.html", "http://googletesting.blogspot.com/2007/09/seleniums-inventor.html", "http://googletesting.blogspot.com/2007/09/testing-applications-and-apis.html", "http://googletesting.blogspot.com/2007/09/university-of-arizona-tech-talk.html", "http://googletesting.blogspot.com/2007/09/more-feedback-from-google-interns.html", "http://googletesting.blogspot.com/2007/08/gtac-videos-now-online.html", "http://googletesting.blogspot.com/2007/08/gtac-community-thread.html", "http://googletesting.blogspot.com/2007/08/abduls-summer-intern-testimonial.html", "http://googletesting.blogspot.com/2007/07/contributing-to-all-star-cast.html", "http://googletesting.blogspot.com/2007/07/google-new-york-test-engineering-forum.html", "http://googletesting.blogspot.com/2007/06/gtac-registration-closed.html", "http://googletesting.blogspot.com/2007/06/tott-extracting-methods-to-simplify.html", "http://googletesting.blogspot.com/2007/05/sign-up-to-attend-our-test-automation.html", "http://googletesting.blogspot.com/2007/05/8-year-old-exploratory-testing.html", "http://googletesting.blogspot.com/2007/04/tott-refactoring-tests-in-red.html", "http://googletesting.blogspot.com/2007/04/test-engineering-in-zurich.html", "http://googletesting.blogspot.com/2007/04/test-engineering-around-world.html", "http://googletesting.blogspot.com/2007/04/gtac-speaker-submission-window-closed.html", "http://googletesting.blogspot.com/2007/04/test-engineering-internships.html", "http://googletesting.blogspot.com/2007/04/tott-stubs-speed-up-your-unit-tests.html", "http://googletesting.blogspot.com/2007/04/google-test-automation-conference.html", "http://googletesting.blogspot.com/2007/03/javascript-simulating-time-in-jsunit.html", "http://googletesting.blogspot.com/2007/03/robot-testers-invade-portland_15.html", "http://googletesting.blogspot.com/2007/03/developer-testertester-developer-summit.html", "http://googletesting.blogspot.com/2007/03/2nd-annual-google-test-automation.html", "http://googletesting.blogspot.com/2007/03/difference-between-qa-qc-and-test.html", "http://googletesting.blogspot.com/2007/02/testapalooza.html", "http://googletesting.blogspot.com/2007/02/seattle-conference-on-scalability.html", "http://googletesting.blogspot.com/2007/02/improving-accuracy-of-tests-by-weighing.html", "http://googletesting.blogspot.com/2007/02/lifes-too-short-to-worry-about-slogans.html", "http://googletesting.blogspot.com/2007/02/tott-naming-unit-tests-responsibly.html", "http://googletesting.blogspot.com/2007/01/where-are-developer-testers-and-tester.html", "http://googletesting.blogspot.com/2007/01/better-stubbing-in-python.html", "http://googletesting.blogspot.com/2007/01/welcome-to-google-testing-blog.html", "http://googletesting.blogspot.com/2007/01/introducing-testing-on-toilet.html"]}}